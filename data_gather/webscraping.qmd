---
title: "Webscraping"
---

```{r}
#| label: setup
#| echo: false
#| warning: false
#| message: false
library(rvest)
library(stringr)
library(dplyr)

```


<iframe src="https://uio.cloud.panopto.eu/Panopto/Pages/Embed.aspx?id=a4fa888f-25db-49e1-af44-b02f00def2ac&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>

The internet is a fantastic resource for data and give very efficient ways of 
collecting data for research. One way of gathering data from the internet is
to copy and paste from any given web-page into a sheet or document. This is,
however, often inefficient. In this section, we will cover how to automate this
process by utilizing various scraping utilities in R.

# Good manners!

Before we start extracting data from the web, we need to make sure we are taught
good manners when it comes to scraping; there are some important rules and
norms we should follow. 

## Sleep

It is always good practice to not overload the webpages we are scraping from. 
Most sites have a limit on how many times an IP can ping their page within a 
given time frame. If you exceed this, you will be timed out for a while before 
you can resume scraping. 

Worst case, you can overload the server hosting the website and bring it down.
That will probably make you unpopular with the server maintainers.

:::callout-tip
## Remember to add sleep between calls

In R, the best way to make sure you have delay between calls is to use the
`Sys.sleep()` command. For instance, `Sys.sleep(2)` will force R to pause for
2 seconds.

You can also add random sleep by using `rnorm`:
```{r}
#| label: sleep_ex
#| eval: false
Sys.sleep(2 + abs(rnorm(1)))
```
R will then sleep for 2 seconds plus the absolute value of `rnorm`.
:::

## Save locally

In the same vein, we should also (if possible) save the scraped webpages locally
on our computer. By doing this, we do not need to scrape everything each time
we want to e.g run an analysis.

But, we also want to save locally because the internet is not static; webpages
can change over time, which can result in replicability issues for researchers.




:::callout-important
## Recommended workflow

1. Make separate scripts for scraping, preprocessing, and analyses
2. Test your scraping script on a couple of pages before running on everything
3. Set up iterative code with sleep that downloads the page to your drive
4. Make use of control flow to check whether the files are already on disk.

:::

## `robots.txt`

Most webpages have a `robots.txt` that instructs web robots (e.g search engine 
bots) what data they can and can not fetch from the website. The `robots.txt`
is located in the root URL of the website: "https://<url>/robots.txt".

For instance, [New York Times](https://nytimes.com) has a `robots.txt` at
<https://www.nytimes.com/robots.txt>.

When scraping a website, one should always look at the `robots.txt` file and
build the scraper accordingly. If there are disallowed pages for "User Agent",
you should consider not scraping these.

# Formats

As shown in the 
[reading data section](../basics_refresh/R_refresher.qmd#sec-loading-data),
there are a lot of different file formats we can encounter when working with 
data in R. In this section, we will focus on scraping `.html`, but also showcase
how we can download a set of `.csv` files, and tips on how we can scrape sites 
that are parsed with javascript. We will also cover `.xml` and `.json` in the 
[API chapter](apis.qmd).

## `.html`

Most webpages are written in Hypertext Mark-up Language (HTML). HTML is a markup
language that gives your web-browser information on how a webpage should look 
for its users. At the core, HTML is build by a hierarchical structure of nodes
encased in `<>` and closed by `</>`. For instance, `<p>` opens a paragraph node,
which is closed by `</p>`. We will explore this further below.

As long as the webpage is openly accessible, the underlying HTML code is also 
available for users to read. You can access the entire source of any webpage you
are browsing by simply pressing `ctrl/cmd + u`, or you can inspect specific 
elements of the page by right clicking on the page and then pressing "Inspect" 
(or pressing `ctrl/cmd + i`):

![](../pics/inspect1.png)

You will then get a tree of HTML nodes in a panel on the right side of your 
screen, where the node you right clicked on will be highlighted:

![](../pics/inspect2.png)

As we usually want to extract specific data from a webpage, excluding ads,
menus, and other irrelevant data, these **element nodes** are very useful for
extracting the data we want.

<!-- fixme: make screen cap of how to inspect -->

:::callout-tip
## Selector Gadget

A helpful tool in finding element nodes in webpages is the 
[Selector Gadget](https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb)
plugin for Google Chrome (or similar for your preferred browser). You can find
a guide for using this plugin on the 
[`rvest` webpage](https://rvest.tidyverse.org/articles/selectorgadget.html).
:::

The most common structure of the HTML hierarchy is that you have the `<html>` or
`<!DOCTYPE html>` node at the top that encompass the entire page, 
a `<head>` node of meta data (language, dates, etc), and a `<body>` node which
contains the content of the webpage. The nodes are often referred to as 
*parents* and *child*. For instance:

```
<div>
  <p>Hello World!</p>
</div>
```

Here, `<div>` is the parent of `<p>` and `<p>` is the child of `<div>`. 

:::callout-tip
## HTML structure

If you have a `.html` file loaded in i R with the `rvest` package, you can
inspect the entire structure of the html tree with `xml2::html_structure()`.
The output might, however, be somewhat verbose.
:::

There are a lot of nodes in HTML markup, but here is a list of some common nodes
you will encounter:

| HTML code   | Node description                                                |
|:------------|:----------------------------------------------------------------|
| `<div>`     | Part of the document                                            |
| `<section>` | Section of the document                                         |
| `<table>`   | A table                                                         |
| `<p>`       | A paragraph                                                     |
| `<h2>`      | Heading in size 2                                               |
| `<h6>`      | Heading in size 6                                               |
| `<a>`       | Anchor combined with the `href` attribute for making hyperlinks |
| `<img>`     | An image                                                        |
| `<br>`      | Vertical line break                                             |
 
 
### `.html` example

In this section, we will use the 
[British Political Speech Archive](http://www.britishpoliticalspeech.org) as an 
example of scraping `.html` files.

First, as this is a short example, we want to extract only the speeches by 
Labour between 1990 and 2000. You can follow the instructions in the video 
below for how to obtain this link. Following the good manners section above,
we download the page one time:

<!-- fixme: make short screencap of link making -->

```{r}
#| eval: false
#| echo: true
download.file("http://www.britishpoliticalspeech.org/speech-archive.htm?q=&speaker=&party=4&searchRangeFrom=1990&searchRangeTo=2000",
              destfile = "../data/scrape/bps/lab_90-00.html")

```

<iframe src="https://uio.cloud.panopto.eu/Panopto/Pages/Embed.aspx?id=a302f521-1046-4777-a500-b05700de5e43&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>


We can now read this `.html` file with the `rvest` package and extract the
table of speeches:

```{r}
#| label: bps_dl
#| eval: true
#| echo: true
bps_front <- read_html("../data/scrape/bps/lab_90-00.html")             # <1>

lab_speak <- bps_front %>%                                              # <2>
  html_nodes("table[class='results-table']") %>%                        # <2>
  html_table() %>%                                                      # <2>
  bind_rows()                                                           # <2>


```

1. Reading the `.html` file
2. Extracting the table node and search result class attribute, then converting 
   to a table and binding the rows (unlisting)

<iframe src="https://uio.cloud.panopto.eu/Panopto/Pages/Embed.aspx?id=a9ca2091-ad37-41a3-9b91-b05800801d01&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>

```{r}
#| label: bps_speech_links
#| eval: true
#| echo: true

lab_txt_links <- bps_front %>%                                          # <1>
  html_nodes("table[class='results-table'] > tbody > tr > td > a") %>%  # <1>
  html_attr("href") %>%                                                 # <1>
  str_c("http://www.britishpoliticalspeech.org/", .)                    # <1>

lab_txt_ids <- str_extract(lab_txt_links, "[0-9]+$")                    # <2>

lab_speak$id <- lab_txt_ids                                             # <2>
```

1. Extracting the hyperlinks for all speeches (used below) and pasting with
   the root URL of the webpage
2. Extracting the numerical id of each speech and inserting it to the data frame
   (also used below)

<iframe src="https://uio.cloud.panopto.eu/Panopto/Pages/Embed.aspx?id=85c4c704-25cd-49ff-89b4-b058008bb6a6&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>

In the next step, we want to download the actual speeches. This involves
extracting the links for each item in the list of our front page, download the 
page from that link, and extract the data we want from that page.

```{r}
#| label: bps_txt_dl
#| eval: false

for(i in 1:length(lab_txt_links)) {                                  # <1>
  download.file(lab_txt_links[i],                                    # <2>
                destfile = str_c("../data/scrape/bps/txt/",          # <3>
                                 lab_txt_ids[i],                     # <3>
                                 ".html"))                           # <3>
  
  Sys.sleep(3)                                                       # <4>
}
```

1. For each number in 1 to the length of `lab_txt_links`
2. ... download link `i`
3. ... and store it in the folder `data/scrape/bps/txt/`, names as its
   id and the `.html` file extension.

<iframe src="https://uio.cloud.panopto.eu/Panopto/Pages/Embed.aspx?id=7d17cafd-6cee-4c06-9e5c-b05800936936&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>

Finally, we can iterate over the downloaded speeches and extract the text we
want from these pages. Turns out, by inspecting one of the speech pages, that
there is a `div` node with the class attribute and "speech" value. This makes 
the extraction straightforward:


```{r}
#| label: struct_bps_speeches
bps_files <- list.files("../data/scrape/bps/txt", full.names = TRUE)      # <1>

lab_speak$text <- NA                                                      # <1>

for(i in bps_files) {                                                     # <2>
  
  tmp <- read_html(i) %>%                                                 # <3>
    html_nodes("div[class='speech']") %>%                                 # <4>
    html_text() %>%                                                       # <5>
    str_replace_all("\\s+", " ")                                          # <6>
  
  lab_speak$text[which(lab_speak$id == str_extract(i, "[0-9]+"))] <- tmp  # <7>
  
}

lab_speak[1:3, c(1:3, 6)]                                                 # <8>

```

1. Listing all files containing the speeches and making a holder variable for 
   the texts in the data frame `lab_speak`
2. For each of the files, we want to...
3. ... read the html-file
4. ... extract the div node with the speech class
5. ... convert the speech to text
6. ... and, replace all connected whitespace with one space
7. Inserts the text to the row in `lab_speech` with corresponding id
8. Printing a subset of rows and columns to show how the data looks.

<iframe src="https://uio.cloud.panopto.eu/Panopto/Pages/Embed.aspx?id=680a5976-933e-4bce-a594-b05800a0799f&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>

Now, we can use the text for whatever we need the text for. Say I want to look
at speech length (number of words) comparisons between Gordon Brown and Tony 
Blair:^[Obviously, this figure does not really show anything interesting. But
take a look at the 
[text data section](../text_data/extracting_data_from_texts.qmd)]

```{r}
#| label: blair_brown
#| warning: false
library(ggplot2)

lab_speak %>% 
  filter(str_detect(Speaker, "Blair|Brown")) %>% 
  mutate(nwords = quanteda::ntoken(text),
         Date = as.Date(Date, "%d/%m/%Y")) %>% 
  ggplot(aes(x = Date, y = nwords, color = Speaker)) +
  geom_point() +
  geom_line() +
  ggthemes::theme_wsj() 
  
  
  
```


## `.csv`

The data we want to use in our research might, of course, already be available
in structured formats. A common way to share data is in the `.csv` format. See
the [section on reading data](../basics_refresh/R_refresher.qmd#sec-loading-data) 
for more information in the `.csv` format.

```{r}
#| label: csv_setup
#| eval: false
#| echo: true
all_csvs <- read_html("https://github.com/MainakRepositor/Datasets")  # <1>

all_links <- all_csvs %>% html_nodes("a") %>% html_attr("href")       # <2>

csvs <- all_links[which(str_detect(all_links, "\\.csv"))] %>%         # <3>
  str_remove("/MainakRepositor/Datasets/blob/master/")                # <4>
 
csvs[1:3]                                                             # <5>
```

1. Reading the [folder of datasets](https://github.com/MainakRepositor/Datasets)
2. Extracting all links from the page
3. Subsetting only links ending with `.csv`
4. Removing "/MainakRepositor/Datasets/blob/master/" from the links; now the only
   thing that remains is the file name of the `.csv` files.
5. Printing the first three `.csv` files

```{r}
#| label: csv_setup_hidden
#| eval: true
#| echo: false

# lapply(1:5, \(x){
#   
#   download.file(raw_links[x], destfile = str_c("data/scrape/csvs/", csvs[x]))
#   Sys.sleep(4)
# })

csv_files <- list.files("../data/scrape/csvs", full.names = TRUE)

csvs <- csv_files %>% str_remove("../data/scrape/csvs/")

csvs[1:3]

```


```{r}
#| label: csv_raw_links_read
#| echo: true
#| eval: false


raw_links <- str_c(                                                     # <1>
  "https://raw.githubusercontent.com/MainakRepositor/Datasets/master/", # <1>
  csvs                                                                  # <1>
  )                                                                     # <1>

for(i in 1:3) {                                                         # <2>
  readr::read_csv(raw_links[i], show_col_types = FALSE) %>%             # <2>
    head() %>%                                                          # <2>
    print()                                                             # <2>
}                                                                       # <2>

```

1. Pasting the name of the `.csv` with the standardized address of raw data on
   [Github](https://github.com)
2. Looping over 1 to 3, where the first three links from `raw_links` is read
   from the repository, and then the first few rows printed to the console


```{r}
#| eval: true
#| echo: false

for(i in 1:3) {
  readr::read_csv(csv_files[i], show_col_types = FALSE) %>% 
    .[1:3, 1:3] %>% 
    print()
  cat("\n")
}


```

## Javascript parsed pages

Although we can get far with scraping `.html` sites, some webpages are 
procedurally generated with JavaScript^[JavaScript is a scripting language used 
to add interactivity and dynamic behavior to webpages.]. A common way of 
scraping these types of pages is to use [Selenium](https://www.selenium.dev/)
which automates browser behavior. If you are interested in learning more about
using selenium in R, 
[check out this guide](https://cran.r-project.org/web/packages/RSelenium/vignettes/basics.html).



