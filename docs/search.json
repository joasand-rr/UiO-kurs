[
  {
    "objectID": "basics_refresh/R_refresher.html",
    "href": "basics_refresh/R_refresher.html",
    "title": "R refresher",
    "section": "",
    "text": "This is a refresher on materials we expect you to be familiar with from your previous methods training.\nIf you feel comfortable working in R, you may skip this content (or parts of it that you find too easy) and move on to the more advanced topics.\nIf your R skills are feeling a little rusty or you just want to make sure you are up to speed before progressing to the new topics, this is definitely the place to start!"
  },
  {
    "objectID": "basics_refresh/R_refresher.html#projects",
    "href": "basics_refresh/R_refresher.html#projects",
    "title": "R refresher",
    "section": "Projects",
    "text": "Projects\nBefore we start working in R (e.g writing R scripts, saving output data, figures, tables, etc.), it is typically a good idea to start a new RStudio project. For instance, you may create a project called “STV4030A”, “DigitalData” or something similar if you would like to work in the same project throughout this course:\n\n\nAlternatively, you could choose to create different projects for the five different assignments you will complete during this course. Once you are finished with this course and proceed to work on something else, e.g. your MA thesis (!), you would want to do so in a new project.\nIn either case, working in “RStudio projects” will allow you to keep track of the different files that belong to the project you are working on and to have a specific directory on the computer where those files will be saved. It will also enable you to easily switch between different projects that you may be working on simultaneously."
  },
  {
    "objectID": "basics_refresh/R_refresher.html#directories-and-paths",
    "href": "basics_refresh/R_refresher.html#directories-and-paths",
    "title": "R refresher",
    "section": "Directories and paths",
    "text": "Directories and paths\nFiles on your computer are saved in different directories. You can find out which directory you are currently working in by running the following line of R code:\n\ngetwd()\n\nThe above line will print the path to the location you are currently working in. If you created a project and are working in that project, it should be the path to the directory where your project lives.\nWe may want to store different types of files in different subdirectories. For instance, you may create a new folder called “Data” where you will save your datasets and another folder called “Figures” where you will save all the figures you create with R. Having such subdirectories is a good idea! However, it means you will need to keep track of the correct path to the folder when you load/save files from/to those folders. If our project working directory has a folder called “Data” which contains a dataset called “example_data.RData”, the code for loading the dataset would be:\n\nload(\"Data/example_data.RData\")\n\nIf, “example_data.RData” had been saved in the working directly (i.e. not in a subdirectory), the code would have been\n\nload(\"example_data.RData\")\n\nIf you are not working in an RStudio project, you will need to set the working directory manually using setwd(). You will need to specify the directory you would like to work in. For instance:\n\nsetwd(\"C:/Users/oyvinsti/OneDrive - Universitetet i Oslo/Documents/STV4030A\")\n\nIf you don’t set the working directory. You will end up working in the default directory. We don’t recommend working in the default directory as it will likely end up being very messy!\nIf you want to load data from or save output to some directory that is not subdirectory of your working directory, you may do so by writing out the full path. For instance, the following line will load “example_data.RData” from the specified directory no matter what working directory you are currently working in (assuming that the specified directory and file exist on your computer):\n\nload(\"C:/Users/oyvinsti/OneDrive - Universitetet i Oslo/Documents/STV4030A/Data/example_data.RData\")\n\nAvoiding such long (and computer-specific) paths is yet another reason to do all your work in RStudio projects!"
  },
  {
    "objectID": "basics_refresh/R_refresher.html#working-in-r-scriptsquarto-documents",
    "href": "basics_refresh/R_refresher.html#working-in-r-scriptsquarto-documents",
    "title": "R refresher",
    "section": "Working in R-scripts/Quarto documents",
    "text": "Working in R-scripts/Quarto documents\nYou should be writing your code in R-scripts (or in Quarto-documents (which we will introduce later)). You can open a new script by clicking “New file” and then selecting “R script”:\n\n\nWriting R scripts (or Quarto documents) helps making your work organized and reproducable. To this end, make sure that the code is written in the right sequence: a line of code may depend on code that is above it in the script (but not on code that comes below it).\nThink of your script as the recipe for re-creating your analysis. Just as when cooking, the order in which you do things may matter.\nSave your R script with an informative name and .R at the end of the file name. The informative file name will help you remember what the script does and the .R will help your computer know that your file is an R script.\nWhen working on your script, you may execute an individual line of code by placing the cursor anywhere on that line and pressing Ctrl+Enter on a PC or Cmd+Enter on a Mac. Executing lines of code line-by-line is great for working interactively in RStudio.\nYou may also run the entire script or some part of it by highlighting everything/what you would like to run, before pressing Ctrl+Enter/Cmd+Enter. You can also run the entire script by pressing Ctrl+Alt+Enter/Cmd+Alt+Enter.\nAlternatively if you have the script saved as a .R-file on your computer, you may run it using the source() function in R:\n\nsource(\"hello_world.R\")\n\nYour complete scripts should run without errors when you open them in a fresh RStudio session and press Ctrl+Alt+Enter/Cmd+Alt+Enter. For your scripts to run without error, you will need to make sure that the code is written in the correct sequence, that all the packages you use are installed on your computer and loaded in the script, that you load data from the right directories, and that any code that doesn’t run is edited/commented out (we will explain what commenting out something means in a moment)."
  },
  {
    "objectID": "basics_refresh/R_refresher.html#the-console",
    "href": "basics_refresh/R_refresher.html#the-console",
    "title": "R refresher",
    "section": "The console",
    "text": "The console\nWhen you run/execute the code, the code and the results will be printed in the “console” (by default located in the lower left corner in the RStudio window). It is also possible to write the code directly in the console (this may be a good idea for instance for installing packages), but you shouldn’t write the code for anything you later would want to reproduce in the console. Write your code in R-scripts or Quarto documents, so that you can reproduce your work later!"
  },
  {
    "objectID": "basics_refresh/R_refresher.html#your-global-environment",
    "href": "basics_refresh/R_refresher.html#your-global-environment",
    "title": "R refresher",
    "section": "Your (global) environment",
    "text": "Your (global) environment\nThe pane called “Environment” (by default located in the upper-right corner of RStudio), lists all the objects currently stored in your global environment. We will talk about such objects later (they might be datasets, individual vectors, regression models, or whatever). You can store something in the environment by using the assignment operator. For instance, we can store the text string “hello world!” in an object called message:\n\nmessage &lt;- \"hello world\"\n\nIn the console we will then only see an echo of this code (it doesn’t print any output) and an object called message will appear in the environment pane.\n\nThe object is listed under values as it is a character vector. We can also see a preview of what the vector contains. Since our vector only has a single value (“hello world!”), the preview just displays this value.\nYou can print out a list of everything stored in your global environment using the ls() function:\n\nls()\n\n[1] \"has_annotations\" \"message\"        \n\n\nStoring “hello world!” in the object message has the advantage that we can now refer to the object name in our subsequent code (rather than retyping the value(s) each time we want to do something with out message), so we can:\n\nprint(message)\n\n[1] \"hello world\"\n\n\nor perhaps we would rather:\n\nprint(toupper(message))\n\n[1] \"HELLO WORLD\"\n\n\nAll the objects in your global environment should be created from your script and in the sequence in which you will be needing them (so don’t create new objects from the console!). This ensures the reproducability of your work. Whenever you close and reopen RStudio, you can just rerun your script to reproduce your objects.\nIt is possible to save everything in the current environment like this:\n\nsave.image(file = \"environment_containing_hello_world_message.RData\")\n\nWhen exiting RStudio, you will be asked whether you want to save what is in your environment (unless you turn this feature off in the RStudio settings).\nIt is usually better to not save what is in your environment and instead re-run your script which should reproduce the all the objects to the environment when you close and reopen RStudio.\nTo make sure that their scripts are not affected by whatever is already in the global environment, some people like to start their R scripts with the following line, which will remove everything from the global environment:\n\nrm(list = ls())\n\nWhile the above line removes all objects from the global environment. The code you run may, however, still be affected by what you have previously done while working in the same R session, for instance, by packages you have loaded. It is therefore usually better to close and reopen RStudio (without saving what is in the environment) whenever you want to rerun your code in a fresh session.\n\n\n\n\n\n\nGlobal vs. local environment\n\n\n\nSo far, we have only dealt with the global environment. This might make you wonder if there are also local environments. There are, but these are not so important just yet. We will get back to local environments, when we start defining our own functions."
  },
  {
    "objectID": "basics_refresh/R_refresher.html#commenting-your-code-and-organizing-your-script",
    "href": "basics_refresh/R_refresher.html#commenting-your-code-and-organizing-your-script",
    "title": "R refresher",
    "section": "Commenting your code and organizing your script",
    "text": "Commenting your code and organizing your script\nIt is useful to add comments in your script to explain your code. Such comments make it easier both for others and for your future self to understand what your code does.\nYou can add comments using the hashtag (#) symbol. Everything that follows the hashtag will be ignored when the code is executed.\n\nmessage &lt;- \"hello world\" # This line assigns the string \"hello world\" to an object called \"message\"\n\nprint(message) # This line prints out whatever is stored in the object called \"message\"\n\n[1] \"hello world\"\n\n\nAs your scripts grow long, it may be a good idea to organize them in different sections with headers. Comments are useful also for this purpose. You make a header using four hashtags on each side of the text of your header:\n\n#### This code just prints out the hello world message ####\nmessage &lt;- \"hello world\" # This line assigns the string \"hello world\" to an object called message\n\nprint(message) # This line prints out whatever is stored in the object called message\n\n[1] \"hello world\"\n\n\nAlternatively, you can include a single hashtag, followed by a label and some dashes:\n\n#This is another way of making a header--------\nmessage &lt;- \"hello world\" # This line assigns the string \"hello world\" to an object called message\n\nprint(message) # This line prints out whatever is stored in the object called message\n\n[1] \"hello world\"\n\n\nFinally, comments can be useful when you have code that doesn’t run (and therefore produces errors when you run your script) but that you don’t want to delete just yet (you are still working on it! Or maybe you just find it hard to let go…). In such cases, commenting out the code is useful:\n\nnew_message &lt;- \"you can also print other messages\"\n\n#print(new_massage) this code doesn't run, but I don't know why. Must be a typo or something... I will have another look at it tomorrow"
  },
  {
    "objectID": "basics_refresh/R_refresher.html#r-as-a-calculator",
    "href": "basics_refresh/R_refresher.html#r-as-a-calculator",
    "title": "R refresher",
    "section": "R as a calculator",
    "text": "R as a calculator\nAn easy way to get started in R is to try using it as a calculator. Try doing some simple calculations such as:\n\n14+65\n\n[1] 79\n\n78*5\n\n[1] 390\n\n15/(6-1)\n\n[1] 3\n\n2^-10\n\n[1] 0.0009765625\n\n\nWhen executing the code above, the result of each calculation will be printed in the console, but not stored anywhere. If you want to store the results, you will need to assign them to objects."
  },
  {
    "objectID": "basics_refresh/R_refresher.html#relational-operators-and-logical-conditions",
    "href": "basics_refresh/R_refresher.html#relational-operators-and-logical-conditions",
    "title": "R refresher",
    "section": "Relational operators and logical conditions",
    "text": "Relational operators and logical conditions\nWe can use relational operators to make R evaluate logical conditions (i.e. statements that are either TRUE or FALSE). For instance, we can ask if 14 is greater than 5:\n\n14 &gt; 5\n\n[1] TRUE\n\n\nBecause 14 is in fact greater than 5, R will return the value TRUE.\nIf we ask whether 14 is equal to 5, R will return FALSE:\n\n14 == 5\n\n[1] FALSE\n\n\nHowever, if we ask if whether 14 is greater or equal to 5, the result will be TRUE:\n\n14 &gt;= 5\n\n[1] TRUE\n\n\nIf we ask if 14 is not equal to 5, the result will be TRUE:\n\n14 != 5\n\n[1] TRUE\n\n\nIf we ask if 14 is smaller than 5, the result will be FALSE:\n\n14 &lt; 5\n\n[1] FALSE\n\n\nWe can ask whether 5 is the sequence of integers from 1 to 14, which it is so the result will be TRUE\n\n5 %in% 1:14\n\n[1] TRUE\n\n\nUsing such logical conditions will be incredibly useful for instance when we want to recode variables, subset our dataset to only contain observations that satisfy specific conditions, etc."
  },
  {
    "objectID": "basics_refresh/R_refresher.html#logical-operators",
    "href": "basics_refresh/R_refresher.html#logical-operators",
    "title": "R refresher",
    "section": "Logical operators",
    "text": "Logical operators\nThe logical operators AND (&) and OR (|) are often useful, in particular to write more complex logical conditions. We may for instance ask if 14 is greater than 5 AND 5 is smaller than 2:\n\n14 &gt; 5 & 5 &lt; 2\n\n[1] FALSE\n\n\nThe above code evaluates to FALSE because only one of the two statements are true.\nIf we instead ask whether 14 is greater than 5 OR 5 is smaller than 2, we get that this is true because one of the statements is true:\n\n14 &gt; 5 | 5 &lt; 2\n\n[1] TRUE"
  },
  {
    "objectID": "basics_refresh/R_refresher.html#sec-assigning",
    "href": "basics_refresh/R_refresher.html#sec-assigning",
    "title": "R refresher",
    "section": "Assigning values to objects",
    "text": "Assigning values to objects\nAs we have already seen, we may store values in objects. To assign something to an object, we use the assignment operator, &lt;-:\n\na &lt;- 15/(6-1)\n\nRunning the above line, will print an echo of the code in the console, but not the result of the calculation. Instead, an object called a containing the result will be saved in our environment. You may print out the value using print() or use it in new calculations\n\nprint(a) #printing whatever is stored in a to the console. \n\n[1] 3\n\na+5 #adding 5 to whatever is stored in a. \n\n[1] 8\n\n\nThe object “a” is a vector containing only a single element. If we want to create longer vectors with multiple elements we can use the c() function:\n\nresults &lt;- c(14+65, 78*5, 15/(6-1), 2^-10) # storing the results of various calculations in the vector called \"results\"\n\n\n\n\n\n\n\nObject names\n\n\n\nObject names in R must start with a letter and may only include letters, numbers, underscores (_) and periods (.). Letters may be upper and lower case (and R distinguishes between lower and upper case, so if you really want to, you may have multiple objects called “results”, “Results”, “RESULTS”, etc. We don’t recommend this as it may be hard for you to remember the differences between objects called very similar things).\nIt is useful to have informative object names that describe what is contained in the object. We recommend the snake_case convention for combining multiple words in a object name. This simply means using lowercase letters and separating different words with _. For instance, you you may name a regression model something like linear_regression_with_all_controls.\nThere, however, also other conventions, such as camelCase in which you you capitalize the first letter of each word. If you prefer this style, you could name your regression model something like linearRegressionWithAllControls."
  },
  {
    "objectID": "basics_refresh/R_refresher.html#do-operations-on-each-element-of-a-vector",
    "href": "basics_refresh/R_refresher.html#do-operations-on-each-element-of-a-vector",
    "title": "R refresher",
    "section": "Do operations on each element of a vector:",
    "text": "Do operations on each element of a vector:\nWe can do operations on all elements of a vector at the same time. For instance, the following line of code, will add 1 to all elements in the vector results:\n\nresults + 1\n\n[1]  80.000000 391.000000   4.000000   1.000977\n\n\nNotice that because we didn’t assign the output of the above code to an object, the results are just printed out in the console. If we instead run the code\n\nresults &lt;- results + 1\n\nwe will overwrite the object “results” with the new vector in which 1 is added to each element."
  },
  {
    "objectID": "basics_refresh/R_refresher.html#functions",
    "href": "basics_refresh/R_refresher.html#functions",
    "title": "R refresher",
    "section": "Functions",
    "text": "Functions\nWhen we are working in R, we are constantly using different functions. For instance, we have already used the function print() to print values to the console. Functions take some input (given in the form of arguments that the function accepts), perform some operation(s), and then returns some output.\nFunctions have a name followed by a parenthesis. Inside the parenthesis, we can specify one or more arguments. Based on the values on these arguments, the function will produce some result. For instance the function mean() calculates the arithmetic mean (or the average) value of a vector. Let’s try it out:\n\nsome_numbers &lt;- c(15, 3, 66, 8, 800) # just creating a vector containing some numbers\nmean(x = some_numbers) # taking the mean of the vector some_numbers\n\n[1] 178.4\n\n\nWe have used the function mean(), which takes the argument x and calculates the mean of x. When calling the function we specified that we want the argument x to take the value some_numbers. Since x is the first argument of mean() we could skip the x = part of the function call. The following code will produce exactly the same result:\n\nmean(some_numbers)\n\n[1] 178.4\n\n\nThe function mean() also takes two other arguments. The first of these is trim, which specifies “the fraction (0 to 0.5) of observations to be trimmed from each end of x before the mean is computed.” trim defaults to 0, meaning that we take the mean of the entire vector x. We can however, set it to something else, such as 0.2:\n\nmean(some_numbers, trim = 0.2)\n\n[1] 29.66667\n\n\nWe now removed 20 per cent of the values on each end of the some_numbers before calculating the mean. We will very rarely have use of the trim argument (in fact, Øyvind used this argument for the first time in his life when writing this page!).\nThe second additional argument of the function mean() will, however, often prove useful! na.rm takes a “logical evaluating to TRUE or FALSE indicating whether NA values should be stripped before the computation proceeds”. In other words, this argument specifies whether missing values should be removed before we calculate the mean. If the vector we calculate the mean of contains missing values and we don’t set na.rm = TRUE our output will just be NA:\n\nsome_numbers_with_missing &lt;- c(15, 3,NA, 66, 8, 800) ## just creating a vector containing some numbers\nmean(some_numbers_with_missing) ## this will result in NA as na.rm defaults to FALSE\n\n[1] NA\n\nmean(some_numbers_with_missing, na.rm = TRUE)\n\n[1] 178.4\n\n\nWe are using functions in R all the time and not just to do mathematical operations such as taking the mean. We use functions to load datasets, fit regression models, export outputs, make graphs, or scrape data from the web. Understanding functions is thus key to mastering R!\n\n\n\n\n\n\nRead the documentation!\n\n\n\nIf you wonder exactly what a function does, what arguments it takes, and what it produces, it is very useful to look at the documentation. You can find the documentation either by running ?function_name or by typing help(topic = function_name). For instance, if we want to look up the documentation for the function mean(), we would run:\n\n?mean\n\nor alternatively:\n\nhelp(topic = mean)\n\nThe help page containing the documentation for the function should then show up in the “help” pane in RStudio (by default locating in the lower-right corner)."
  },
  {
    "objectID": "basics_refresh/R_refresher.html#installing-and-loading-packages",
    "href": "basics_refresh/R_refresher.html#installing-and-loading-packages",
    "title": "R refresher",
    "section": "Installing and Loading Packages",
    "text": "Installing and Loading Packages\nWhen you open a new R session, you immediately have access to some functions, such as mean() or print(), which we have already used. These functions included in “base R” are quite powerful and may suffice for many purposes. However, we can vastly extend the capabilities of R by making use of the thousands of packages, which contains additional functions (and sometimes data).\nSome of these packages are distributed with R, meaning that they are automatically installed when you install R (but are still not automatically loaded when you start a new R session). One example is the survival package, which contains functions for something called “event history” or “survival” analysis (which we will not really get into in this course). Before we can use the functions included in such packages, we will need to load them. We load packages using the library() function:\n\nlibrary(survival) # this loads the survival package. \n\nOther packages are not distributed with R and need to be downloaded from the internet and installed on your computer before we can use them. One example is the haven package (which we will be using shortly). We can download and install (most) packages using the install.packages() function:\n\ninstall.packages(\"haven\") # this will install the haven package\n\n\n\n\n\n\n\nDon’t reinstall packages every time you run your code!\n\n\n\nYou only need to install a package once (that is, at least until you update R and may need to also update your packages). You therefore don’t want to include the calls to install.packages() in your scripts! Including these calls in your scripts will make R reinstall the packages every time you run the script, both slowing down the script and likely causing problems that occur when you try to reinstall packages that are already loaded. It is good practice to install the packages directly in the console, delete the lines installing packages once the packages are installed, or commenting out the lines that install packages if you insist on keeping the code for later use:\n\n#install.packages(\"haven\") # I commented out this line, because haven is already installed\n\n\n\nInstalling the package is not sufficient, we also need to load it. Once the package is installed on our computer, we can load it using the library() function:\n\nlibrary(haven) # this loads the haven package\n\nWhile we only need to install the packages once, we need to load them every time we want to use them in a new R session. It is useful to load all the packages you are using at the beginning of your scripts. This allows you to discover problems (for instance that a package is not installed) already when you run the first lines of the script later. You may then have a first section of the script that looks something like this:\n\n#### Loading packages that I will be using ####\nlibrary(haven) \nlibrary(survival)\n\n\n\n\n\n\n\nFunctions from different packages that share the same name!\n\n\n\nAnyone can develop and publish R packages with new functions. While this is generally a great thing, it also means that there will be many functions from different packages that share the same names. Some function names, such as select(), may be very tempting for developers to use and there will therefore be many functions with names such as select(). If you load different packages that contain functions named select(), you may accidentally be using a different function than you intended to!\nThere are (at least) two ways of dealing with this problem. First, you can write package_name::function_name to specify which package the function is from. So, if you want to use the function select() from the package dplyr, you write dplyr::select(). This approach is fine if you are only invoking this function once, but it can get a bit tedious if you are repeatedly using select() from dplyr.\nA second approach is to use the package conflicted to specify how conflicts between different packages should be resolved. This code will tell R that it should always use select() from dplyr:\n\nlibrary(conflicted)\nconflict_prefer(name = \"select\", winner = \"dplyr\")\n\n[conflicted] Will prefer dplyr::select over any other package.\n\n\n\n\n\n\n\n\n\n\nDifferent package repositories\n\n\n\nMost of the packages you will be installing from the internet are available from a package repository called “The Comprehensive R Archive Network”( CRAN). These packages can simply be installed without specifying any other arguments than the package name when running install.packages() as the default behavior of this function is to install packages from CRAN.\nOccasionally, packages may be available only through other repositories. In such cases, we will need to specify the “repos” argument. One example is the package countreg which is only available through a repository called R-Forge. We therefore need to install this package like this:\n\ninstall.packages(\"countreg\", repos = \"http://R-Forge.R-project.org\") \n\nOther packages will only be available as development versions, for instance through GitHub. One example is the vdemdata package which contains the most recent version of the Varieties of Democracy dataset. To install packages available from GitHub you will need to first install the devtools package (which is available from CRAN!). Note that to install devtools on Windows, you will also need something called RTools installed on your system To install devtools on Mac, you will similarly need something called Xcode or XQuartz.\nYou can install the vdemdata package from GitHub like this:\n\n#install.packages(\"devtools\") # if you haven't already installed devtools, you will need to run this line\nlibrary(devtools)\ninstall_github(\"vdeminstitute/vdemdata\")"
  },
  {
    "objectID": "basics_refresh/R_refresher.html#sec-loading-data",
    "href": "basics_refresh/R_refresher.html#sec-loading-data",
    "title": "R refresher",
    "section": "Loading Data",
    "text": "Loading Data\nNow that we know how to use R functions, assign values to objects, and install and load packages, we are ready to start loading datasets into R (and thus also making the political science relevance of what we are up to more obvious!).\nWe will load the WhoGov dataset which provides “bibliographic information, such as gender and party affiliation, on cabinet members in July every year in the period 1966-2021 in all countries with a population of more than 400,000 citizens”. You can read more about this dataset and get some ideas for what such data can be used for here!. We downloaded the data from https://politicscentre.nuffield.ox.ac.uk/whogov-dataset/ and saved local copies, which you may also want to do if you would like to test the code below.\nHow we will load the data depends on the format (the type of file). You know what type of file it is by looking at the end of the file name (whether it ends in .csv, .rds, or something else).\n\n.rds-files\nOne of the formats that the WhoGov dataset is supplied in is .rds. We can load .rds-files using the read_rds() function from the package readr. The read_rds() takes the argument file which we use to specify the location of the file on our computer:\n\nlibrary(readr)\nwho_gov &lt;- read_rds(file = \"../data/WhoGov_within_V2.0.rds\")\n\nBecause file is the first argument, we don’t have to name it in the function call, so we could also run:\n\nwho_gov &lt;- read_rds(\"../data/WhoGov_within_V2.0.rds\")\n\nThe nice WhoGov people have also made the file available as a comma-separated values files (.csv). This is a very nice format to work with. .csv-files are just plain text files in which different columns are separated by commas. You can load .csv-file using the read_delim() function from the readr package (Since we already loaded this packages, we don’t need to reload it):\n\n\n.csv-files\n\nwho_gov &lt;- read_delim(file = \"../data/WhoGov_within_V2.0.csv\", delim = \",\")\n\nRows: 249957 Columns: 32\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (20): country_isocode, country_name, position, name, title, gender, dead...\ndbl (12): ...1, year, id, birthyear, partyfacts_id, core, minister, leader, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIn addition to the file argument, we also specified the delim argument which tells R how the columns are separated in the file. When columns are separated using ,, we could also use read_csv() which works in exactly the same way as read_delim() except that it will assume that the delimiter is a comma:\n\nwho_gov &lt;- read_csv(file = \"../data/WhoGov_within_V2.0.csv\")\n\nRows: 249957 Columns: 32\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (20): country_isocode, country_name, position, name, title, gender, dead...\ndbl (12): ...1, year, id, birthyear, partyfacts_id, core, minister, leader, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe delimiter is, however, not always a comma. Sometimes people use a semicolon or the tabulator to separate the columns. Using read_delim() and specifying the delimiter is useful in such cases. read_delim() also has a number of other arguments you can specify. For instance, you can specify whether white space at the beginning and end of each cell should be removed (trim_ws), whether the first row of the dataset contains variable names (col_names), or whether R should skip some lines in the file when loading it (skip). These arguments have default options that will work well for most dataset, but if you run into trouble loading a .csv-file, it is a good idea to check out these options. You can see the full list options by looking up the documentation:\n\n?read_delim\n\nBy default, read_delim() spits out some information about the variables included in the dataset and whether they are read as numbers or as character strings (we will discuss data types later). If you find this behavior annoying, you can turn it off using the show_col_types argument:\n\nwho_gov &lt;- read_delim(file = \"../data/WhoGov_within_V2.0.csv\", delim = \",\", show_col_types = FALSE)\n\n\n\n.xlsx-files\nFinally, WhoGov is made available in Microsoft Excel (xlsx) format. To load a .xlsx-file, you can use the read_excel() function from the readxl package:\n\nlibrary(readxl)\nwho_gov &lt;- read_excel(file = \"../data/WhoGov_within_V2.0.xlsx\")\n\nSometimes Excel-files will contain data in different sheets. The read_excel() function has a sheet argument which allows you to specify which sheet you are interested in.\n\n\n.dta-files\nYou will frequently encounter datasets saved in the Stata dataset format (.dta). While WhoGov is not published in this format, we have saved it locally as a .dta-file to illustrate how such files can be loaded.\nWe can load .dta-files using the read_dta() function from the haven package:\n\nlibrary(haven)\nwho_gov &lt;- read_dta(\"../data/WhoGov_within_V2.dta\")\n\n\n\n.sav-files\nSimilarly, we can load SPSS (.sav) files using the read_sav() function which is also available from the haven package:\n\nwho_gov &lt;- read_sav(\"../data/WhoGov_within_V2.sav\", encoding = \"latin1\")\n\nTo load the SPSS version of the file, we had to specify correct encoding. The “encoding” is just how variables containing text is stored in the file. If you encounter error messages like this one: “Unable to convert string to the requested encoding (invalid byte sequence)”, the problem is typically that R doesn’t recognize the encoding of the file and that you therefore need to specify it. We discuss encoding in more detail here.\n\n\n.rda and .RData-files\n.rda and .RData are R data formats. You can load .rda- and .RData- files using the function load():\n\nload(file = \"../data/WhoGov_within_V2.rda\")\n\nor\n\nload(file = \"../data/WhoGov_within_V2.RData\")"
  },
  {
    "objectID": "basics_refresh/R_refresher.html#saving-data",
    "href": "basics_refresh/R_refresher.html#saving-data",
    "title": "R refresher",
    "section": "Saving Data",
    "text": "Saving Data\nYou may save your data in any of the formats we encountered above (as well as many others). However, we will recommend saving your data either as plain text .csv-files or as R datasets (.rda or .RData).\nTo save a dataset as a .csv-file, we can use the function write_csv() from the package readr:\n\nwrite_csv(x = who_gov, file = \"../data/who_gov_copy.csv\")\n\nWhen saving datasets, we both need to specify the data frame that we would like to save (using the x argument) and the file name (and directory) we want to save the data to (using the file argument).\nTo save a dataset as an R dataset, we can use the save() function:\n\nsave(who_gov, file = \"../data/who_gov_copy.rda\")\n\nWe can save multiple objects at the same time using save() and these objects don’t necessarily have to be data.frames. We created some other objects above, such as the vectors a and results. We can save them in .rda-file together with the who_gov data.frame if we want to:\n\nsave(who_gov,a, results, file = \"../data/who_gov_copy_and_other_stuff.rda\")\n\nIf we then load() this file, we will load all these three objects to our environment.\n\nload(file = \"../data/who_gov_copy_and_other_stuff.rda\")\n\n\n\n\n\n\n\nSaving data in other formats\n\n\n\nFunctions for saving data to other formats are generally available in the same packages as the functions for loading data. Their function names typically start with write_ followed by the format. To save a dataset as a Stata (.dta) file, you may for instance use write_dta() from haven."
  },
  {
    "objectID": "basics_refresh/R_refresher.html#exploring-data.frames-and-other-r-objects",
    "href": "basics_refresh/R_refresher.html#exploring-data.frames-and-other-r-objects",
    "title": "R refresher",
    "section": "Exploring data.frames and other R objects",
    "text": "Exploring data.frames and other R objects\nThe WhoGov data was loaded to R as a data.frame and we assigned it to the object we called who_gov. You will see that who_gov stored under the header “Data” in the Environment pane.\n\nWe generally want to store our datasets as data.frames (or as tibbles. The tibble format is just a somewhat modernized version of the traditional R data.frame). data.frames should be organized with the observations (our units of analysis) as rows and variables as columns. We can get a quick glimpse of what the dataset looks like by printing out the first six rows in the console using the function head()\n\nhead(who_gov)\n\n  year country_isocode country_name id            position\n1 1966             AFG  Afghanistan  1                King\n2 1966             AFG  Afghanistan  2          Prime Min.\n3 1966             AFG  Afghanistan  3 1st Dep. Prime Min.\n4 1966             AFG  Afghanistan  4 2nd Dep. Prime Min.\n5 1966             AFG  Afghanistan  5 Min. Of Agriculture\n6 1966             AFG  Afghanistan  6    Min. Of Commerce\n                        name title gender birthyear deadyear       party\n1             Mohammed Zahir  &lt;NA&gt;   Male        NA     &lt;NA&gt; independent\n2 Maiwandwal Mohammed Hashim  &lt;NA&gt;   Male        NA     &lt;NA&gt;        pdpa\n3          Nur Ahmed Etemadi  &lt;NA&gt;   Male        NA     &lt;NA&gt; independent\n4       Shalizi Abdus Sattar  &lt;NA&gt;   Male        NA     &lt;NA&gt; independent\n5        Mohammed Akbar Reza  &lt;NA&gt;   Male        NA     &lt;NA&gt;     unknown\n6                    Ali Nur   Dr.   Male        NA     &lt;NA&gt;     unknown\n                                party_english party_otherlanguage\n1                                 independent         independent\n2 Progressive Democratic Party of Afghanistan                &lt;NA&gt;\n3                                 independent         independent\n4                                 independent         independent\n5                                        &lt;NA&gt;                &lt;NA&gt;\n6                                        &lt;NA&gt;                &lt;NA&gt;\n   whogov_partyid partyfacts_id core minister leader        classification\n1 AFG-independent          4908    1        0      0  Member, Royal Family\n2        AFG-pdpa          8700    1        0      1        Prime Minister\n3 AFG-independent          4908    1        0      0 Deputy Prime Minister\n4 AFG-independent          4908    1        0      0 Deputy Prime Minister\n5     AFG-unknown            NA    1        1      0  Minister (Full Rank)\n6     AFG-unknown            NA    1        1      0  Minister (Full Rank)\n                               portfolio_1 prestige_1 portfolio_2 prestige_2\n1                                     &lt;NA&gt;       &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;\n2                                     &lt;NA&gt;       &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;\n3                                     &lt;NA&gt;       &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;\n4                                     &lt;NA&gt;       &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;\n5 Agriculture, Food, Fisheries & Livestock     Medium        &lt;NA&gt;       &lt;NA&gt;\n6                      Industry & Commerce     Medium        &lt;NA&gt;       &lt;NA&gt;\n  portfolio_3 prestige_3 portfolio_4 prestige_4 m_finance m_defense\n1        &lt;NA&gt;       &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;         0         0\n2        &lt;NA&gt;       &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;         0         0\n3        &lt;NA&gt;       &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;         0         0\n4        &lt;NA&gt;       &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;         0         0\n5        &lt;NA&gt;       &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;         0         0\n6        &lt;NA&gt;       &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;         0         0\n  m_agriculture m_foreignaffairs\n1             0                0\n2             0                0\n3             0                0\n4             0                0\n5             1                0\n6             0                0\n\n\nWhat is the unit-of-analysis in the who_gov dataset? Well, we see that the first six rows all cover the country Afghanistan and the year 1966. So, clearly, we are not dealing with a country-year dataset (if so, we would only have one row per country per year). The different observations from the same country-year appear to be different people holding different cabinet positions, so maybe the unit of analysis is the “country-year-position”.\nSince we are not sure, we should look up the documentation for the article. Nyrup and Bramwell (2020) write that the dataset “contains yearly data on members of cabinets in 177 countries”, so our impression that the unit of analysis is the country-year-position appears to be correct.\nA few other functions are useful for getting a quick overview of how a specific data.frame is structured. First, names() will print out the names of all the variables in the dataset:\n\nnames(who_gov)\n\n [1] \"year\"                \"country_isocode\"     \"country_name\"       \n [4] \"id\"                  \"position\"            \"name\"               \n [7] \"title\"               \"gender\"              \"birthyear\"          \n[10] \"deadyear\"            \"party\"               \"party_english\"      \n[13] \"party_otherlanguage\" \"whogov_partyid\"      \"partyfacts_id\"      \n[16] \"core\"                \"minister\"            \"leader\"             \n[19] \"classification\"      \"portfolio_1\"         \"prestige_1\"         \n[22] \"portfolio_2\"         \"prestige_2\"          \"portfolio_3\"        \n[25] \"prestige_3\"          \"portfolio_4\"         \"prestige_4\"         \n[28] \"m_finance\"           \"m_defense\"           \"m_agriculture\"      \n[31] \"m_foreignaffairs\"   \n\n\nTo know what these different variables are and how they are coded, we have to look up the codebook or other documentation. This information is not generally available in R. The codebook for the WhoGov dataset is available online.\nSecond, we can use dim() to get the dimensions of the data.frame: the number of rows and columns\n\ndim(who_gov)\n\n[1] 249957     31\n\n\nWe can see that there are 249957 rows (the observations) and 31 columns (the variables). If we prefer, we could also get this information separately using the nrow() and ncol() functions:\n\nnrow(who_gov) # the number of rows\n\n[1] 249957\n\nncol(who_gov) # the number of columns\n\n[1] 31\n\n\nThird, we can use summary() to get summary statistics for all the variables in the dataset:\n\nsummary(who_gov)\n\n      year      country_isocode    country_name             id      \n Min.   :1963   Length:249957      Length:249957      Min.   :   1  \n 1st Qu.:1984   Class :character   Class :character   1st Qu.: 354  \n Median :1998   Mode  :character   Mode  :character   Median : 722  \n Mean   :1996                                         Mean   : 817  \n 3rd Qu.:2010                                         3rd Qu.:1167  \n Max.   :2021                                         Max.   :3564  \n                                                                    \n   position             name              title              gender         \n Length:249957      Length:249957      Length:249957      Length:249957     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   birthyear        deadyear            party           party_english     \n Min.   :1882     Length:249957      Length:249957      Length:249957     \n 1st Qu.:1929     Class :character   Class :character   Class :character  \n Median :1944     Mode  :character   Mode  :character   Mode  :character  \n Mean   :1943                                                             \n 3rd Qu.:1956                                                             \n Max.   :1993                                                             \n NA's   :138417                                                           \n party_otherlanguage whogov_partyid     partyfacts_id        core       \n Length:249957       Length:249957      Min.   :   3    Min.   :0.0000  \n Class :character    Class :character   1st Qu.:1760    1st Qu.:1.0000  \n Mode  :character    Mode  :character   Median :3343    Median :1.0000  \n                                        Mean   :3419    Mean   :0.8672  \n                                        3rd Qu.:4998    3rd Qu.:1.0000  \n                                        Max.   :9015    Max.   :1.0000  \n                                        NA's   :34897                   \n    minister          leader       classification     portfolio_1       \n Min.   :0.0000   Min.   :0.0000   Length:249957      Length:249957     \n 1st Qu.:0.0000   1st Qu.:0.0000   Class :character   Class :character  \n Median :1.0000   Median :0.0000   Mode  :character   Mode  :character  \n Mean   :0.7194   Mean   :0.0352                                        \n 3rd Qu.:1.0000   3rd Qu.:0.0000                                        \n Max.   :1.0000   Max.   :1.0000                                        \n NA's   :4                                                              \n  prestige_1        portfolio_2         prestige_2        portfolio_3       \n Length:249957      Length:249957      Length:249957      Length:249957     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  prestige_3        portfolio_4         prestige_4          m_finance      \n Length:249957      Length:249957      Length:249957      Min.   :0.00000  \n Class :character   Class :character   Class :character   1st Qu.:0.00000  \n Mode  :character   Mode  :character   Mode  :character   Median :0.00000  \n                                                          Mean   :0.03546  \n                                                          3rd Qu.:0.00000  \n                                                          Max.   :1.00000  \n                                                                           \n   m_defense       m_agriculture     m_foreignaffairs \n Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :0.00000   Median :0.00000   Median :0.00000  \n Mean   :0.03273   Mean   :0.03201   Mean   :0.03509  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :1.00000   Max.   :1.00000   Max.   :1.00000  \n NA's   :1                                            \n\n\n\nInspecting the variables in a data.frame\nYou can use the $ symbol to refer to specific variables inside a data.frame. Your code will look something like data_frame$variable. For instance, we may continue exploring the dataset by looking at how long the time series is (which years are covered by the data) and how many unique countries are included. Let’s start with the year variable, which available as who_gov$year. We can use range() to learn when the time series starts and ends:\n\nrange(who_gov$year)\n\n[1] 1963 2021\n\n\nOr we could use summary to get summary statistics for this variable:\n\nsummary(who_gov$year)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1963    1984    1998    1996    2010    2021 \n\n\nThe first observation is from 1963 and the last observation is from 2021. Maybe we want to get a sense of how many observations there are from each year in the time series? If so, we could use table() to get a frequency table showing the number of observations per year:\n\ntable(who_gov$year)\n\n\n1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 \n  14   14   14 2767 2743 2809 2879 2961 3086 3217 3290 3314 3495 3534 3640 3687 \n1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 \n3782 3813 3955 3975 4153 4187 4100 4167 4215 4203 4216 4074 4027 4331 4671 4852 \n1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 \n5040 5236 5242 5191 5239 5229 5375 5348 5233 5177 5115 5175 5255 5211 5289 5332 \n2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 \n5272 5353 5366 5329 5271 5247 5242 5280 5243 5249 5233 \n\n\nThe table that R spits out in the console is a little ugly, but it is organized as follows: below each unique value on the variable (here year) it shows the number of observations with that value. So for instance, there are 5239 observations from the year 1999.\nWe notice that the first year is 1963 and that there are only 14 observations each year for 1963, 1964, and 1965. Combined with our sense that there must have been more than 14 cabinet ministers in the world also in these years and the fact that Nyrup and Bramwell (2020) write that their dataset starts in 1966, this information should make us somewhat suspicious! We will investigate this more below.\nFor now, we move on to exploring coverage of different countries. The name of the country of each observation is stored in who_gov$country_name. We first want to know how many unique countries there are. Using unique() we can print out all the unique values on who_gov$country_name:\n\nunique(who_gov$country_name)\n\n  [1] \"Afghanistan\"                          \n  [2] \"Albania\"                              \n  [3] \"Algeria\"                              \n  [4] \"Angola\"                               \n  [5] \"Argentina\"                            \n  [6] \"Armenia\"                              \n  [7] \"Australia\"                            \n  [8] \"Austria\"                              \n  [9] \"Azerbaijan\"                           \n [10] \"Bahrain\"                              \n [11] \"Bangladesh\"                           \n [12] \"Belarus\"                              \n [13] \"Belgium\"                              \n [14] \"Benin\"                                \n [15] \"Bhutan\"                               \n [16] \"Bolivia\"                              \n [17] \"Bosnia & Herzegovina\"                 \n [18] \"Botswana\"                             \n [19] \"Brazil\"                               \n [20] \"Brunei\"                               \n [21] \"Bulgaria\"                             \n [22] \"Burkina Faso\"                         \n [23] \"Burundi\"                              \n [24] \"Cambodia\"                             \n [25] \"Cameroon\"                             \n [26] \"Canada\"                               \n [27] \"Cape Verde\"                           \n [28] \"Central African Republic\"             \n [29] \"Chad\"                                 \n [30] \"Chile\"                                \n [31] \"China\"                                \n [32] \"Colombia\"                             \n [33] \"Comoros\"                              \n [34] \"Costa Rica\"                           \n [35] \"Croatia\"                              \n [36] \"Cuba\"                                 \n [37] \"Cyprus\"                               \n [38] \"Czechoslovakia\"                       \n [39] \"Czechia\"                              \n [40] \"Denmark\"                              \n [41] \"Djibouti\"                             \n [42] \"Dominican Republic\"                   \n [43] \"Congo - Kinshasa\"                     \n [44] \"Ecuador\"                              \n [45] \"Egypt\"                                \n [46] \"El Salvador\"                          \n [47] \"Equatorial Guinea\"                    \n [48] \"Eritrea\"                              \n [49] \"Estonia\"                              \n [50] \"Ethiopia\"                             \n [51] \"Fiji\"                                 \n [52] \"Finland\"                              \n [53] \"France\"                               \n [54] \"Gabon\"                                \n [55] \"Gambia\"                               \n [56] \"East Germany\"                         \n [57] \"Georgia\"                              \n [58] \"Germany\"                              \n [59] \"Ghana\"                                \n [60] \"Greece\"                               \n [61] \"Grenada\"                              \n [62] \"Guatemala\"                            \n [63] \"Guinea\"                               \n [64] \"Guinea-Bissau\"                        \n [65] \"Guyana\"                               \n [66] \"Haiti\"                                \n [67] \"Honduras\"                             \n [68] \"Hungary\"                              \n [69] \"Iceland\"                              \n [70] \"India\"                                \n [71] \"Indonesia\"                            \n [72] \"Iran\"                                 \n [73] \"Iraq\"                                 \n [74] \"Ireland\"                              \n [75] \"Israel\"                               \n [76] \"Italy\"                                \n [77] \"Côte d’Ivoire\"                        \n [78] \"Jamaica\"                              \n [79] \"Japan\"                                \n [80] \"Jordan\"                               \n [81] \"Kazakhstan\"                           \n [82] \"Kenya\"                                \n [83] \"Kuwait\"                               \n [84] \"Kyrgyzstan\"                           \n [85] \"Laos\"                                 \n [86] \"Latvia\"                               \n [87] \"Lebanon\"                              \n [88] \"Lesotho\"                              \n [89] \"Liberia\"                              \n [90] \"Libya\"                                \n [91] \"Lithuania\"                            \n [92] \"Luxembourg\"                           \n [93] \"North Macedonia\"                      \n [94] \"Madagascar\"                           \n [95] \"Malawi\"                               \n [96] \"Malaysia\"                             \n [97] \"Maldives\"                             \n [98] \"Mali\"                                 \n [99] \"Malta\"                                \n[100] \"Mauritania\"                           \n[101] \"Mauritius\"                            \n[102] \"Mexico\"                               \n[103] \"Moldova\"                              \n[104] \"Mongolia\"                             \n[105] \"Montenegro\"                           \n[106] \"Morocco\"                              \n[107] \"Mozambique\"                           \n[108] \"Myanmar (Burma)\"                      \n[109] \"Namibia\"                              \n[110] \"Nepal\"                                \n[111] \"Netherlands\"                          \n[112] \"New Zealand\"                          \n[113] \"Nicaragua\"                            \n[114] \"Niger\"                                \n[115] \"Nigeria\"                              \n[116] \"North Korea\"                          \n[117] \"North Vienam\"                         \n[118] \"Norway\"                               \n[119] \"Oman\"                                 \n[120] \"Pakistan\"                             \n[121] \"Panama\"                               \n[122] \"Papua New Guinea\"                     \n[123] \"Paraguay\"                             \n[124] \"Peru\"                                 \n[125] \"Philippines\"                          \n[126] \"Poland\"                               \n[127] \"Portugal\"                             \n[128] \"Qatar\"                                \n[129] \"Congo - Brazzaville\"                  \n[130] \"Romania\"                              \n[131] \"Russia\"                               \n[132] \"Rwanda\"                               \n[133] \"São Tomé & Príncipe\"                  \n[134] \"Saudi Arabia\"                         \n[135] \"Senegal\"                              \n[136] \"Serbia\"                               \n[137] \"Sierra Leone\"                         \n[138] \"Singapore\"                            \n[139] \"Slovakia\"                             \n[140] \"Slovenia\"                             \n[141] \"Somalia\"                              \n[142] \"South Africa\"                         \n[143] \"South Korea\"                          \n[144] \"South Sudan\"                          \n[145] \"South Vietnam\"                        \n[146] \"People's Democratic Republic of Yemen\"\n[147] \"Spain\"                                \n[148] \"Sri Lanka\"                            \n[149] \"Sudan\"                                \n[150] \"Suriname\"                             \n[151] \"Eswatini\"                             \n[152] \"Sweden\"                               \n[153] \"Switzerland\"                          \n[154] \"Syria\"                                \n[155] \"Taiwan\"                               \n[156] \"Tajikistan\"                           \n[157] \"Tanzania\"                             \n[158] \"Thailand\"                             \n[159] \"Timor-Leste\"                          \n[160] \"Togo\"                                 \n[161] \"Trinidad & Tobago\"                    \n[162] \"Tunisia\"                              \n[163] \"Turkey\"                               \n[164] \"Turkmenistan\"                         \n[165] \"Uganda\"                               \n[166] \"Ukraine\"                              \n[167] \"United Arab Emirates\"                 \n[168] \"United Kingdom\"                       \n[169] \"Uruguay\"                              \n[170] \"United States\"                        \n[171] \"Soviet Union\"                         \n[172] \"Uzbekistan\"                           \n[173] \"Venezuela\"                            \n[174] \"Vietnam\"                              \n[175] \"Yemen\"                                \n[176] \"Yugoslavia\"                           \n[177] \"Zambia\"                               \n[178] \"Zimbabwe\"                             \n\n\nTypically, we are not interested in reading this full list. We just want to know how many different countries there are. We can use length() to find out how long a vector is. Using unique() will create vector with all the unique values and finding the length of this vector will tell us how many unique countries there are in the dataset:\n\nlength(unique(who_gov$country_name))\n\n[1] 178\n\n\nWe learn that there are 178 unique country names in the dataset.1\nIf we want to get a sense of what types of positions the people in the WhoGov dataset held, we can similarly use table() on the who_gov$portfolio_1 which contains a standardized coding of the different types of positions (There was no way for us to know this in advance. We had to look up the WhoGov codebook, which is generally what you should do if you wonder how to use a dataset created by others. )\n\ntable(who_gov$portfolio_1)\n\n\n                        Ageing & Elderly \n                                     125 \nAgriculture, Food, Fisheries & Livestock \n                                   11513 \n     Audit, Oversight & Internal Affairs \n                                    1142 \n                       Children & Family \n                                    1120 \n                           Civil Service \n                                    2109 \n            Communications & Information \n                                   10144 \n             Construction & Public Works \n                                    7417 \n          Correctional Services & Police \n                                     854 \n                      Culture & Heritage \n                                    4670 \n   Defense, Military & National Security \n                                   11931 \n            Education, Training & Skills \n                                   12373 \n                                  Energy \n                                    5750 \n       Enterprises, Companies & Business \n                                    2740 \n                             Environment \n                                    8364 \n       Executive & Legislative Relations \n                                    3227 \n              Finance, Budget & Treasury \n                                   11046 \n              Foreign Economic Relations \n                                    4774 \n                       Foreign Relations \n                                   11699 \n                General Economic Affairs \n                                    3695 \n     Government, Interior & Home Affairs \n                                    9348 \n                 Health & Social Welfare \n                                   10256 \n                                 Housing \n                                    2358 \n                Immigration & Emigration \n                                    1149 \n                     Industry & Commerce \n                                    7471 \n                 Justice & Legal Affairs \n                                    8037 \n     Labor, Employment & Social Security \n                                    6496 \n                        Local Government \n                                    1616 \n                              Minorities \n                                     390 \n                       Natural Resources \n                                    1600 \n                                   Other \n                                   11653 \n                  Planning & Development \n                                    7611 \n                        Political Reform \n                                     727 \n                  Properties & Buildings \n                                     137 \n                                Regional \n                                    2319 \n                                Religion \n                                    1545 \n          Science, Technology & Research \n                                    2171 \n                                  Sports \n                                    2465 \n            Tax, Revenue & Fiscal Policy \n                                     275 \n                                 Tourism \n                                    1684 \n                               Transport \n                                    3470 \n                                Veterans \n                                     630 \n                       Without Portfolio \n                                    2068 \n                                   Women \n                                     835 \n                                   Youth \n                                     453"
  },
  {
    "objectID": "basics_refresh/R_refresher.html#changing-or-adding-variables",
    "href": "basics_refresh/R_refresher.html#changing-or-adding-variables",
    "title": "R refresher",
    "section": "Changing or adding variables",
    "text": "Changing or adding variables\nJust as we can use the assignment operator,&lt;- to create objects, we can use it to create or change variables inside a data.frame. If you run the following line, you will create a new column in the who_gov, which always takes the value of 1.\n\nwho_gov$ones &lt;- 1\n\nThis variable is obviously not super interesting. We would rather have a variable that actually varies. Let’s try to make create a variable called who_gov$female which will take the value 1 if the person is female and 0 otherwise. We will base this variable on the variable who_gov$gender which is already in the dataset and which takes the values “Male”, “Female”, “Other”, and “Unknown”. The function ifelse() is very useful for creating a new variable based on some conditions (for example based on e values on some existing variable):\n\n1who_gov$female &lt;- ifelse(who_gov$gender == \"Female\",\n2                         1,\n3                         0)\n\n\n1\n\ncheck if the value on `who_gov$gender is “Female”\n\n2\n\nGive those cases the value 1.\n\n3\n\nGiv other cases the value 0.\n\n\n\n\nWe now created a new variable called who_gov$female. Since this variable is based on the variable who_gov$gender, we can compare the two variables against each other to verify that the who_gov$female was created correctly. We can use table() to create a cross table:\n\ntable(who_gov$female, who_gov$gender)\n\n   \n    Female   Male  Other Unknown\n  0      0 222937      5     177\n  1  23170      0      0       0\n\n\nWe see that all the observations with “Female” as the value on the who_gov$gender variable has the value 1 on the who_gov$female variable and that all those with other values on the who_gov$gender variable has the value 0 on who_gov$female variable. Woot!\nPerhaps we don’t want observations with “Unknown” on who_gov$gender to be coded 0 on the who_gov$female variable? Perhaps giving these a missing value (NA) would be more accurate. If so, we can overwrite the who_gov$female variable like this:\n\n1who_gov$female &lt;- ifelse(who_gov$gender == \"Unknown\",\n2                         NA,\n3                         who_gov$female)\n\n\n1\n\nCheck if the value on `whogov$gender is “Unknown”\n\n2\n\nGive those cases the value NA\n\n3\n\nGive other cases the value they already have on who_gov$female\n\n\n\n\nLet’s check that this did what we intended it to do:\n\ntable(who_gov$female, who_gov$gender)\n\n   \n    Female   Male  Other Unknown\n  0      0 222937      5       0\n  1  23170      0      0       0\n\n\nThis looks like expected. However, the people with “Unknown” on who_gov$gender are no longer displayed in the table. This has to do with how R treats missing values. To be absolutely sure, we can use is.na() to find out what values observations that now have NA on who_gov$female have on who_gov$gender:\n\ntable(is.na(who_gov$female), who_gov$gender)\n\n       \n        Female   Male  Other Unknown\n  FALSE  23170 222937      5       0\n  TRUE       0      0      0     177\n\n\nThis confirms that the 177 people with “Unknown” who_gov$gender now have NA on who_gov$female (and there are no other observations with NA on who_gov$female).\n\n\n\n\n\n\nifelse()\n\n\n\nifelse() is incredibly useful for creating and recoding variables. ifelse() takes three arguments. The first argument should be a logical condition. For example is the value on who_gov$gender equal to “Female”. The second argument tells R what should happen if the logical condition is true. In our case, we want to return the value 1 if the condition is true. The third argument tells R what should happen if the condition is false. In our case, we want the value 0 if the condition is false:\n\n1who_gov$female &lt;- ifelse(who_gov$gender == \"Female\",\n2                         1,\n3                         0)\n\n\n1\n\nIs who_gov$gender equal to “Female” (This will be evaluated for each row in the dataset)\n\n2\n\nIf so, we want the value 1.\n\n3\n\nIf not, we want the value 0."
  },
  {
    "objectID": "basics_refresh/R_refresher.html#sec-data-types",
    "href": "basics_refresh/R_refresher.html#sec-data-types",
    "title": "R refresher",
    "section": "Different data types",
    "text": "Different data types\nVariables stored in a data.frame are vectors. More specifically, they are so-called “atomic vectors” meaning that all their values need to be of the same type. What do we mean by type?\nIn the who_gov data.frame, there are variables of two different types:\n\nSome of the variables are integers. They are numbers and more specifically “whole numbers” (not fractions/without decimals). We have already seen one example: the variable who_gov$year . Another example is the variable who$leader which takes the value of 1 if the person in question was the country leader in the relevant country year and 0 otherwise. Because integers are numbers, we can do various calculations with them. For instance, we can use mean() to calculate the share of observations that are of country leaders:\n\n\nmean(who_gov$leader)\n\n[1] 0.03520205\n\n\nThe mean of this binary variable is 0.0352021, which means that about 0.04 or 4 per cent of the observations are of leaders.\n\nSome of the variables are character vectors. They are just interpreted by R as text. Often, we will refer to such character vectors as “strings”. We have already seen two examples, the who_gov$portfolio_1 and who_gov$country_name. The values in character vectors surrounded by quotation marks (\"\"), which is how R stores text. We cannot do mathematical operations on character vectors. We can, however, do other stuff such as creating frequency tables, using table(). There are a bunch of functions for manipulating strings in R, such as toupper() which changes all the characters in the vector to UPPER CASE (we use head() to avoid printing out the full vector):\n\n\nhead(toupper(who_gov$country_name))\n\n[1] \"AFGHANISTAN\" \"AFGHANISTAN\" \"AFGHANISTAN\" \"AFGHANISTAN\" \"AFGHANISTAN\"\n[6] \"AFGHANISTAN\"\n\n\nWhile these are two types of variables available in who_gov, they are not the only possible data types. Let’s look at some more types:\n\nNumbers obviously don’t have to be integers. We can also have the data type, “numeric”, which also allows for fractions. For instance, we may calculate the share of office holders within each year that are country leaders and add this variable to the dataset. There are many ways of calculating this variable, but one approach would be:\n\n\n1who_gov$leader_share_in_year &lt;- ave(who_gov$leader,\n2                                    who_gov$year,\n3                                    FUN = mean)\n\n\n1\n\nave() allows you to apply a function to variable within each “group” on another variable. We are applying some function to the variable who_gov$leader.\n\n2\n\nWe are going to group the data by who_gov$year so that we apply the function separately for groups observations that share the same value on who_gov$year.\n\n3\n\nThe function that we apply to who_gov$leader within each group is mean()\n\n\n\n\nDon’t worry too much about the above code yet. Instead look at at the first six rows of the who_gov$leader_share_in_year variable (The first six values are all the same, reflecting that all these observations are from the same year):\n\nhead(who_gov$leader_share_in_year)\n\n[1] 0.04625949 0.04625949 0.04625949 0.04625949 0.04625949 0.04625949\n\n\nIn contrast to the integers we saw before, this variable is a numeric vector, which means that it can consists of fractions (of course, in this instance, it only consists of fractions). Since, these are also numbers, we can do mathematical operations and apply functions that do these, just as we did for integers:\n\nmean(who_gov$leader_share_in_year)\n\n[1] 0.03520205\n\n\n\nLogical vectors only take the values TRUE and FALSE. We can abbreviate TRUE as T and FALSE as F, but our code generally becomes more readable if we write out the full values TRUE and FALSE. For instance, we can create a variable that takes the value TRUE if the person is older than 70 years and FALSE otherwise, using the variables who_gov$birthyear and who_gov$year:\n\n\n1who_gov$older_than_70 &lt;- ifelse(who_gov$year - who_gov$birthyear &gt; 70,\n2                                TRUE,\n3                                FALSE )\n\n\n1\n\nTake year minus birth year to calculate the age and check whether the age is greater than 70\n\n2\n\nIf so, give the value TRUE\n\n3\n\nIf not, give the value FALSE\n\n\n\n\nWe can use table() to see how many of observations that are of people older than 70 in the year they were observed:\n\ntable(who_gov$older_than_70)\n\n\n FALSE   TRUE \n105368   6172 \n\n\nR can treat logical vectors as numeric. If we try to apply arithmetic operations on a logical vector, R will treat TRUE as 1s and FALSE as 0. Thus, we can calculate the share observations older than 70, using mean() (Because we don’t know the birth year of all the observations, we have NA values in the who_gov$older_than_70 variable and we must remember to specify the na.rm = TRUE argument):\n\nmean(who_gov$older_than_70, na.rm = TRUE)\n\n[1] 0.05533441\n\n\nThere are also some other types of variables that we will encounter, such as dates and factors, but these will be based on the four categories discussed above.\n\nChecking which type a variable is\nYou can check the data type of a variable using the or class() function. For instance:\n\nclass(who_gov$country_name)\n\n[1] \"character\"\n\nclass(who_gov$year)\n\n[1] \"integer\"\n\nclass(who_gov$leader_share_in_year)\n\n[1] \"numeric\"\n\nclass(who_gov$older_than_70)\n\n[1] \"logical\"\n\n\nYou can also class() on other types of R objects to find out what kinds of objects they are. For instance:\n\nclass(who_gov)\n\n[1] \"data.frame\"\n\n\nThere are also functions for checking if an object is of a specific type:\n\nis.data.frame(who_gov)\n\n[1] TRUE\n\n\n\n\nChanging the data type\nWe can change the types of variables. Maybe we don’t want who_gov$older_than_70 to be stored as a logical vector, but would prefer it to be stored as an integer vector. If so, we can use as.integer()\n\nwho_gov$older_than_70 &lt;- as.integer(who_gov$older_than_70)\nclass(who_gov$older_than_70)\n\n[1] \"integer\"\n\ntable(who_gov$older_than_70)\n\n\n     0      1 \n105368   6172 \n\n\nTRUE has now been replaced with 1 and FALSE has been replaced with 0 in who_gov$older_than_70 and the class has changed to “integer”.\nWe can also force it to become a character vector:\n\nwho_gov$older_than_70 &lt;- as.character(who_gov$older_than_70)\nclass(who_gov$older_than_70)\n\n[1] \"character\"\n\n\nIf so, 1 will be changed into “1” and 0 will be changed into “0”. For us, this may seem like a small change, but since R is now treating these values as text, we can no longer do mathematical operations. For instance, calculating the mean will just return an NA and a warning message:\n\nmean(who_gov$older_than_70, na.rm = TRUE)\n\nWarning in mean.default(who_gov$older_than_70, na.rm = TRUE): argument is not\nnumeric or logical: returning NA\n\n\n[1] NA\n\n\nWe can force numbers to be text and if numbers are stored as text, we can turn them back to numbers:\n\nwho_gov$older_than_70 &lt;- as.numeric(who_gov$older_than_70)\nmean(who_gov$older_than_70, na.rm = TRUE)\n\n[1] 0.05533441\n\n\nHowever, not all conversions from text to numbers are possible. For instance, R has no idea how to make a numeric variable based on the country names. Attempting this will just return a bunch of missing values:\n\nwho_gov$country_name_numeric &lt;- as.numeric(who_gov$country_name)\n\nWarning: NAs introduced by coercion\n\nhead(who_gov$country_name_numeric)\n\n[1] NA NA NA NA NA NA\n\n\n\n\nMissing values\nMissing values are stored in R as NA. It is not possible to do any computations on missing values and we have to exclude them before we can do most operations. You can check if a vector includes missing values by using is.na(). If you want to see how many NAs there are, you can combine is.na() with table()\n\ntable(is.na(who_gov$older_than_70))\n\n\n FALSE   TRUE \n111540 138417"
  },
  {
    "objectID": "basics_refresh/R_refresher.html#subsetting-data.frames-and-vectors",
    "href": "basics_refresh/R_refresher.html#subsetting-data.frames-and-vectors",
    "title": "R refresher",
    "section": "Subsetting data.frames and vectors",
    "text": "Subsetting data.frames and vectors\nYou can select specific elements of a data.frame or vector in R using square brackets([]). Vectors are one-dimensional, so we just need a number of range of numbers inside the square brackets. For instance, if we want the sixth element of who_gov$country_name we can write:\n\nwho_gov$country_name[6]\n\n[1] \"Afghanistan\"\n\n\nor we could get a the sixth, the 7900th and the 25000th element like this:\n\nwho_gov$country_name[c(6, 7900, 25000)]\n\n[1] \"Afghanistan\" \"Australia\"   \"Brazil\"     \n\n\nIf we want to, we can overwrite specific elements:\n\nwho_gov$country_name[6] &lt;- \"Just to illustrate\"\n\nWe can specify ranges of observations we want, using ::\n\nwho_gov$country_name[4:8]\n\n[1] \"Afghanistan\"        \"Afghanistan\"        \"Just to illustrate\"\n[4] \"Afghanistan\"        \"Afghanistan\"       \n\n\ndata.frames are two-dimensional, so we need to specify whether we want to select rows or columns or both. We use square brackets with a comma that separates between rows (before the comma) and columns (after the comma) [,]. For instance, we can get the 6th row of the 4th column in who_gov like this:\n\nwho_gov[6,4]\n\n[1] 6\n\n\nSince the columns are named, we can also use the column names to subset:\n\nwho_gov[6,\"country_name\"]\n\n[1] \"Just to illustrate\"\n\n\nWe can leave either the rows or the columns empty inside the square brackets to select all rows or columns:\n\nwho_gov[6,]\n\n  year country_isocode       country_name id         position    name title\n6 1966             AFG Just to illustrate  6 Min. Of Commerce Ali Nur   Dr.\n  gender birthyear deadyear   party party_english party_otherlanguage\n6   Male        NA     &lt;NA&gt; unknown          &lt;NA&gt;                &lt;NA&gt;\n  whogov_partyid partyfacts_id core minister leader       classification\n6    AFG-unknown            NA    1        1      0 Minister (Full Rank)\n          portfolio_1 prestige_1 portfolio_2 prestige_2 portfolio_3 prestige_3\n6 Industry & Commerce     Medium        &lt;NA&gt;       &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;\n  portfolio_4 prestige_4 m_finance m_defense m_agriculture m_foreignaffairs\n6        &lt;NA&gt;       &lt;NA&gt;         0         0             0                0\n  ones female leader_share_in_year older_than_70 country_name_numeric\n6    1      0           0.04625949            NA                   NA\n\n\nWe can use logical conditions to select specific rows or columns. For instance, we can select all rows from Norway like this:\n\nNorway &lt;- who_gov[who_gov$country_name == \"Norway\",] \n\nWe now have the tools we need to further investigate the suspicious observations from before 1966 that we discovered above. We can create a new data.frame with only these observations to investigate:\n\nbefore_1966 &lt;- who_gov[who_gov$year &lt; 1966, ]\n\nA quick investigation of the before_1966 reveals that these are all observations from the United States.\n\ntable(before_1966$country_name)\n\n\nUnited States \n           42 \n\n\nPerhaps it doesn’t make sense for our analysis to include a longer time series just for the United States. If so, we may remove these observations from the dataset like this:\n\nwho_gov &lt;- who_gov[who_gov$year &gt;= 1966,]"
  },
  {
    "objectID": "basics_refresh/R_refresher.html#lists",
    "href": "basics_refresh/R_refresher.html#lists",
    "title": "R refresher",
    "section": "Lists",
    "text": "Lists\nWe will often find it useful to store information in lists. Lists are also a type of vector, but unlike the atomic vectors which can only contain elements of the same data type, lists can contain elements of any types. The elements of a list may themselves be vectors. This is a simple example of a list:\n\nexample_list &lt;- list(products = c(\"apple\", \"milk\", \"bread\"), \n                     price = c(5, 18, 25), \n                     store = \"kiwi\")\n\nThe list wouldn’t have to be named, we could just as well have created:\n\nexample_list_2 &lt;- list(c(\"apple\", \"milk\", \"bread\"), \n                    c(5, 18, 25), \n                    \"kiwi\")\n\nIn either case, we could use the double square brackets to extract the different parts of the list:\n\nexample_list_2[[1]] # this will you the first element, which is a character vector\n\n[1] \"apple\" \"milk\"  \"bread\"\n\n\nIf the list is named, we can use the $ to refer to its elements:\n\nexample_list$products\n\n[1] \"apple\" \"milk\"  \"bread\"\n\n\nUsing subsetting, we can find elements within each element of the list:\n\nexample_list_2[[1]][3]\n\n[1] \"bread\"\n\n\nAnd we can use the assignment operator to overwrite any value:\n\nexample_list_2[[1]][3] &lt;- \"white bread\"\nprint(example_list_2)\n\n[[1]]\n[1] \"apple\"       \"milk\"        \"white bread\"\n\n[[2]]\n[1]  5 18 25\n\n[[3]]\n[1] \"kiwi\""
  },
  {
    "objectID": "basics_refresh/R_refresher.html#checking-what-is-stored-inside-any-object",
    "href": "basics_refresh/R_refresher.html#checking-what-is-stored-inside-any-object",
    "title": "R refresher",
    "section": "Checking what is stored inside any object",
    "text": "Checking what is stored inside any object\nWe will encounter many different types of objects in R and sometimes we don’t really know how information is stored inside them. The function str() can show you the structure of any R object:\n\nstr(who_gov)\n\n'data.frame':   249915 obs. of  36 variables:\n $ year                : int  1966 1966 1966 1966 1966 1966 1966 1966 1966 1966 ...\n $ country_isocode     : chr  \"AFG\" \"AFG\" \"AFG\" \"AFG\" ...\n $ country_name        : chr  \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" ...\n $ id                  : int  1 2 3 4 5 6 7 8 9 10 ...\n $ position            : chr  \"King\" \"Prime Min.\" \"1st Dep. Prime Min.\" \"2nd Dep. Prime Min.\" ...\n $ name                : chr  \"Mohammed Zahir\" \"Maiwandwal Mohammed Hashim\" \"Nur Ahmed Etemadi\" \"Shalizi Abdus Sattar\" ...\n $ title               : chr  NA NA NA NA ...\n $ gender              : chr  \"Male\" \"Male\" \"Male\" \"Male\" ...\n $ birthyear           : int  NA NA NA NA NA NA NA NA NA NA ...\n $ deadyear            : chr  NA NA NA NA ...\n $ party               : chr  \"independent\" \"pdpa\" \"independent\" \"independent\" ...\n $ party_english       : chr  \"independent\" \"Progressive Democratic Party of Afghanistan\" \"independent\" \"independent\" ...\n $ party_otherlanguage : chr  \"independent\" NA \"independent\" \"independent\" ...\n $ whogov_partyid      : chr  \"AFG-independent\" \"AFG-pdpa\" \"AFG-independent\" \"AFG-independent\" ...\n $ partyfacts_id       : num  4908 8700 4908 4908 NA ...\n $ core                : int  1 1 1 1 1 1 1 1 1 1 ...\n $ minister            : int  0 0 0 0 1 1 1 1 1 1 ...\n $ leader              : int  0 1 0 0 0 0 0 0 0 0 ...\n $ classification      : chr  \"Member, Royal Family\" \"Prime Minister\" \"Deputy Prime Minister\" \"Deputy Prime Minister\" ...\n $ portfolio_1         : chr  NA NA NA NA ...\n $ prestige_1          : chr  NA NA NA NA ...\n $ portfolio_2         : chr  NA NA NA NA ...\n $ prestige_2          : chr  NA NA NA NA ...\n $ portfolio_3         : chr  NA NA NA NA ...\n $ prestige_3          : chr  NA NA NA NA ...\n $ portfolio_4         : chr  NA NA NA NA ...\n $ prestige_4          : chr  NA NA NA NA ...\n $ m_finance           : int  0 0 0 0 0 0 0 0 1 0 ...\n $ m_defense           : int  0 0 0 0 0 0 0 0 0 0 ...\n $ m_agriculture       : int  0 0 0 0 1 0 0 0 0 0 ...\n $ m_foreignaffairs    : int  0 0 0 0 0 0 0 0 0 1 ...\n $ ones                : num  1 1 1 1 1 1 1 1 1 1 ...\n $ female              : num  0 0 0 0 0 0 0 0 0 0 ...\n $ leader_share_in_year: num  0.0463 0.0463 0.0463 0.0463 0.0463 ...\n $ older_than_70       : num  NA NA NA NA NA NA NA NA NA NA ...\n $ country_name_numeric: num  NA NA NA NA NA NA NA NA NA NA ...\n\nstr(example_list)\n\nList of 3\n $ products: chr [1:3] \"apple\" \"milk\" \"bread\"\n $ price   : num [1:3] 5 18 25\n $ store   : chr \"kiwi\""
  },
  {
    "objectID": "basics_refresh/R_refresher.html#lm",
    "href": "basics_refresh/R_refresher.html#lm",
    "title": "R refresher",
    "section": "lm()",
    "text": "lm()\nGrewal (2023) estimated a linear regression model, which you can do in R using the lm() function. The function first takes a formula argument, which you can think as the regression equation. The basic structure is like this:\n\ndependent_variable ~ independent_variable1 + independent_variable2 + independent_variable3\n\nYou first add your dependent variable followed by a the tilde sign (~). You then add one or more independent variables and you separate each independent variable by a + sign.\nNext you need to specify the data argument. Here you just specify the dataset that you are using.\nlm() also takes a number of other arguments that sometimes may be useful, such as weights where you can specify weights if you are estimated a weighted regression. subset which you can use to subset the dataset and so on. We will not specify any of these additional arguments here.\nUsing lm(), we can thus reestimate Grewal’s first model like this:\n\n1library(readr)\ngrewal_data &lt;- read_delim(\"../data/algeria_military_survey.csv\", delim = \",\")\n\n2grewal_model_1 &lt;- lm(restraint ~\n3                       nonvio2 + prime_frat + prime_future + prime_RU +\n                       prime_UN + oppose92coup2 + civwar + islamist +\n                       supp_sharia + active + conscript + soldier + junoff +\n                       senoff + branch2 + train_west + train_russia +\n                       train_china + young + sex + edu + urban_area +\n                       employed + student + arabic + econ1 + corr1 +\n                       dem + supp_opp_parties + continue + protested +\n                       preboutef + post_exp + mon +\n4                       as.factor(fgov),\n5                     data = grewal_data)\n\n\n1\n\nHis dataset comes in a .csv-format so we will load the readr package and use read_delim() to load the data.\n\n2\n\nWe estimate the regression model using lm() and assign the results to an object grewal_model_1 so that we can do stuff with it later. We start the formula by adding the dependent variable (restraint) followed by a ~.\n\n3\n\nWe add all the independent variables/covariates separated by the + sign.\n\n4\n\nfgov measures which governorate the respondent is from. Grewal included governorate fixed effects which just means one dummy variable for each governorate. You can include these in your model formula using as.factor() around the variable that you want fixed effects on. When we are done specifying the formula, we end this argument using a ,.\n\n5\n\nFinally, we specify the data argument.\n\n\n\n\n\n1summary(grewal_model_1 )\n\n\n1\n\nsummary() prints out a summary of the regression model in the console. This summary is good for us making sense of the the model when we are working in Rstudio, but is not how the model should be reported in your MA thesis or STV4030A assignments. We will show you how to generate publication quality regression tables in your Quarto documents and how to use visualizations to present your regression models. The summary is printed below:\n\n\n\n\n\nCall:\nlm(formula = restraint ~ nonvio2 + prime_frat + prime_future + \n    prime_RU + prime_UN + oppose92coup2 + civwar + islamist + \n    supp_sharia + active + conscript + soldier + junoff + senoff + \n    branch2 + train_west + train_russia + train_china + young + \n    sex + edu + urban_area + employed + student + arabic + econ1 + \n    corr1 + dem + supp_opp_parties + continue + protested + preboutef + \n    post_exp + mon + as.factor(fgov), data = grewal_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1318 -0.3994  0.1661  0.2849  0.7718 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.340296   0.116016   2.933 0.003396 ** \nnonvio2            0.098405   0.027791   3.541 0.000408 ***\nprime_frat        -0.023003   0.035303  -0.652 0.514747    \nprime_future      -0.072684   0.035810  -2.030 0.042524 *  \nprime_RU          -0.066233   0.036766  -1.801 0.071786 .  \nprime_UN          -0.017195   0.035396  -0.486 0.627171    \noppose92coup2      0.041560   0.021020   1.977 0.048172 *  \ncivwar             0.014028   0.025565   0.549 0.583278    \nislamist           0.019400   0.029871   0.649 0.516127    \nsupp_sharia        0.157501   0.036363   4.331 1.56e-05 ***\nactive            -0.002991   0.023729  -0.126 0.899714    \nconscript         -0.015610   0.022980  -0.679 0.497058    \nsoldier            0.013436   0.024245   0.554 0.579536    \njunoff             0.032761   0.030275   1.082 0.279338    \nsenoff             0.110452   0.061325   1.801 0.071847 .  \nbranch2Land        0.018626   0.027530   0.677 0.498754    \ntrain_west        -0.095806   0.057540  -1.665 0.096076 .  \ntrain_russia      -0.019452   0.033929  -0.573 0.566491    \ntrain_china        0.035077   0.104884   0.334 0.738090    \nyoung              0.132958   0.041903   3.173 0.001533 ** \nsex                0.020073   0.053541   0.375 0.707764    \nedu               -0.013767   0.011900  -1.157 0.247464    \nurban_area         0.006059   0.021512   0.282 0.778223    \nemployed           0.060536   0.024347   2.486 0.012991 *  \nstudent           -0.017229   0.045347  -0.380 0.704032    \narabic             0.093961   0.028645   3.280 0.001056 ** \necon1             -0.041342   0.048422  -0.854 0.393340    \ncorr1              0.002089   0.047742   0.044 0.965105    \ndem                0.127105   0.035679   3.562 0.000377 ***\nsupp_opp_parties  -0.055413   0.034429  -1.609 0.107679    \ncontinue          -0.032596   0.022097  -1.475 0.140356    \nprotested          0.090071   0.022265   4.045 5.43e-05 ***\npreboutef         -0.038257   0.049551  -0.772 0.440164    \npost_exp          -0.011467   0.052701  -0.218 0.827774    \nmon               -0.017228   0.006908  -2.494 0.012715 *  \nas.factor(fgov)2   0.116790   0.089020   1.312 0.189697    \nas.factor(fgov)3   0.058483   0.105184   0.556 0.578274    \nas.factor(fgov)4   0.061048   0.083606   0.730 0.465366    \nas.factor(fgov)5   0.130978   0.081296   1.611 0.107322    \nas.factor(fgov)6   0.083368   0.093025   0.896 0.370268    \nas.factor(fgov)7   0.006197   0.085602   0.072 0.942295    \nas.factor(fgov)8   0.211462   0.110190   1.919 0.055127 .  \nas.factor(fgov)9   0.062118   0.084053   0.739 0.459977    \nas.factor(fgov)10  0.076911   0.103818   0.741 0.458894    \nas.factor(fgov)11  0.163302   0.109689   1.489 0.136714    \nas.factor(fgov)12  0.170914   0.084717   2.017 0.043788 *  \nas.factor(fgov)13  0.012839   0.089126   0.144 0.885475    \nas.factor(fgov)14  0.177984   0.096365   1.847 0.064908 .  \nas.factor(fgov)15  0.134210   0.101431   1.323 0.185939    \nas.factor(fgov)16  0.052723   0.073160   0.721 0.471215    \nas.factor(fgov)17  0.106416   0.086357   1.232 0.218000    \nas.factor(fgov)18  0.214414   0.093002   2.305 0.021249 *  \nas.factor(fgov)19  0.114935   0.080674   1.425 0.154417    \nas.factor(fgov)20  0.316394   0.109942   2.878 0.004049 ** \nas.factor(fgov)21  0.028812   0.096043   0.300 0.764216    \nas.factor(fgov)22 -0.116207   0.103973  -1.118 0.263856    \nas.factor(fgov)23 -0.024421   0.099326  -0.246 0.805814    \nas.factor(fgov)24  0.113434   0.104063   1.090 0.275829    \nas.factor(fgov)25 -0.021529   0.087183  -0.247 0.804983    \nas.factor(fgov)26  0.193574   0.104879   1.846 0.065094 .  \nas.factor(fgov)27  0.107213   0.114648   0.935 0.349831    \nas.factor(fgov)28  0.098621   0.085004   1.160 0.246116    \nas.factor(fgov)29  0.131832   0.111982   1.177 0.239237    \nas.factor(fgov)30  0.095761   0.091298   1.049 0.294363    \nas.factor(fgov)31  0.145817   0.088828   1.642 0.100847    \nas.factor(fgov)32  0.107781   0.120921   0.891 0.372861    \nas.factor(fgov)33  0.213301   0.187727   1.136 0.256003    \nas.factor(fgov)34 -0.037991   0.100002  -0.380 0.704058    \nas.factor(fgov)35  0.221270   0.111052   1.992 0.046461 *  \nas.factor(fgov)36  0.200380   0.126085   1.589 0.112171    \nas.factor(fgov)37  0.098960   0.223601   0.443 0.658126    \nas.factor(fgov)38 -0.089886   0.157434  -0.571 0.568106    \nas.factor(fgov)39  0.115744   0.101604   1.139 0.254776    \nas.factor(fgov)40  0.148457   0.096826   1.533 0.125388    \nas.factor(fgov)41  0.137003   0.113494   1.207 0.227532    \nas.factor(fgov)42 -0.008749   0.126798  -0.069 0.944997    \nas.factor(fgov)43  0.194892   0.095958   2.031 0.042394 *  \nas.factor(fgov)44  0.120708   0.101907   1.184 0.236367    \nas.factor(fgov)45  0.311231   0.187292   1.662 0.096731 .  \nas.factor(fgov)46  0.094889   0.144907   0.655 0.512658    \nas.factor(fgov)47  0.126136   0.100125   1.260 0.207901    \nas.factor(fgov)48  0.103402   0.110296   0.937 0.348628    \nas.factor(fgov)49 -0.322913   0.255706  -1.263 0.206807    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4236 on 1886 degrees of freedom\n  (266 observations deleted due to missingness)\nMultiple R-squared:  0.1103,    Adjusted R-squared:  0.07159 \nF-statistic: 2.851 on 82 and 1886 DF,  p-value: 2.936e-15"
  },
  {
    "objectID": "basics_refresh/R_refresher.html#glm",
    "href": "basics_refresh/R_refresher.html#glm",
    "title": "R refresher",
    "section": "glm()",
    "text": "glm()\nlm() only estimates linear regressions. You can use glm() to estimate generalized linear models (such as logistic regressions). With glm(), we need to estimate an additional argument, which tells R what kind of generalized linear model we would like to estimate. If we want a binomial logistic regression the family argument should be set to family = binomial(link = \"logit\"). We can also use glm() to estimate linear regressions by setting family = \"gaussian\". The following code should therefore produce exactly the same model as we estimated using lm() above:\n\n1grewal_model_1_glm &lt;- glm(restraint ~\n                       nonvio2 + prime_frat + prime_future + prime_RU + \n                       prime_UN + oppose92coup2 + civwar + islamist + \n                       supp_sharia + active + conscript + soldier + junoff + \n                       senoff + branch2 + train_west + train_russia + \n                       train_china + young + sex + edu + urban_area + \n                       employed + student + arabic + econ1 + corr1 + \n                       dem + supp_opp_parties + continue + protested + \n                       preboutef + post_exp + mon +\n                       as.factor(fgov), \n                     data = grewal_data, \n2                     family = \"gaussian\")\n\n\n1\n\nwe use glm() instead of lm()\n\n2\n\nWe remember to specify the family argument.\n\n\n\n\nYou can verify that this is the same model by comparing the coefficients to those we estimated above (they should be identical!):\n\nsummary(grewal_model_1_glm)\n\n\nCall:\nglm(formula = restraint ~ nonvio2 + prime_frat + prime_future + \n    prime_RU + prime_UN + oppose92coup2 + civwar + islamist + \n    supp_sharia + active + conscript + soldier + junoff + senoff + \n    branch2 + train_west + train_russia + train_china + young + \n    sex + edu + urban_area + employed + student + arabic + econ1 + \n    corr1 + dem + supp_opp_parties + continue + protested + preboutef + \n    post_exp + mon + as.factor(fgov), family = \"gaussian\", data = grewal_data)\n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.340296   0.116016   2.933 0.003396 ** \nnonvio2            0.098405   0.027791   3.541 0.000408 ***\nprime_frat        -0.023003   0.035303  -0.652 0.514747    \nprime_future      -0.072684   0.035810  -2.030 0.042524 *  \nprime_RU          -0.066233   0.036766  -1.801 0.071786 .  \nprime_UN          -0.017195   0.035396  -0.486 0.627171    \noppose92coup2      0.041560   0.021020   1.977 0.048172 *  \ncivwar             0.014028   0.025565   0.549 0.583278    \nislamist           0.019400   0.029871   0.649 0.516127    \nsupp_sharia        0.157501   0.036363   4.331 1.56e-05 ***\nactive            -0.002991   0.023729  -0.126 0.899714    \nconscript         -0.015610   0.022980  -0.679 0.497058    \nsoldier            0.013436   0.024245   0.554 0.579536    \njunoff             0.032761   0.030275   1.082 0.279338    \nsenoff             0.110452   0.061325   1.801 0.071847 .  \nbranch2Land        0.018626   0.027530   0.677 0.498754    \ntrain_west        -0.095806   0.057540  -1.665 0.096076 .  \ntrain_russia      -0.019452   0.033929  -0.573 0.566491    \ntrain_china        0.035077   0.104884   0.334 0.738090    \nyoung              0.132958   0.041903   3.173 0.001533 ** \nsex                0.020073   0.053541   0.375 0.707764    \nedu               -0.013767   0.011900  -1.157 0.247464    \nurban_area         0.006059   0.021512   0.282 0.778223    \nemployed           0.060536   0.024347   2.486 0.012991 *  \nstudent           -0.017229   0.045347  -0.380 0.704032    \narabic             0.093961   0.028645   3.280 0.001056 ** \necon1             -0.041342   0.048422  -0.854 0.393340    \ncorr1              0.002089   0.047742   0.044 0.965105    \ndem                0.127105   0.035679   3.562 0.000377 ***\nsupp_opp_parties  -0.055413   0.034429  -1.609 0.107679    \ncontinue          -0.032596   0.022097  -1.475 0.140356    \nprotested          0.090071   0.022265   4.045 5.43e-05 ***\npreboutef         -0.038257   0.049551  -0.772 0.440164    \npost_exp          -0.011467   0.052701  -0.218 0.827774    \nmon               -0.017228   0.006908  -2.494 0.012715 *  \nas.factor(fgov)2   0.116790   0.089020   1.312 0.189697    \nas.factor(fgov)3   0.058483   0.105184   0.556 0.578274    \nas.factor(fgov)4   0.061048   0.083606   0.730 0.465366    \nas.factor(fgov)5   0.130978   0.081296   1.611 0.107322    \nas.factor(fgov)6   0.083368   0.093025   0.896 0.370268    \nas.factor(fgov)7   0.006197   0.085602   0.072 0.942295    \nas.factor(fgov)8   0.211462   0.110190   1.919 0.055127 .  \nas.factor(fgov)9   0.062118   0.084053   0.739 0.459977    \nas.factor(fgov)10  0.076911   0.103818   0.741 0.458894    \nas.factor(fgov)11  0.163302   0.109689   1.489 0.136714    \nas.factor(fgov)12  0.170914   0.084717   2.017 0.043788 *  \nas.factor(fgov)13  0.012839   0.089126   0.144 0.885475    \nas.factor(fgov)14  0.177984   0.096365   1.847 0.064908 .  \nas.factor(fgov)15  0.134210   0.101431   1.323 0.185939    \nas.factor(fgov)16  0.052723   0.073160   0.721 0.471215    \nas.factor(fgov)17  0.106416   0.086357   1.232 0.218000    \nas.factor(fgov)18  0.214414   0.093002   2.305 0.021249 *  \nas.factor(fgov)19  0.114935   0.080674   1.425 0.154417    \nas.factor(fgov)20  0.316394   0.109942   2.878 0.004049 ** \nas.factor(fgov)21  0.028812   0.096043   0.300 0.764216    \nas.factor(fgov)22 -0.116207   0.103973  -1.118 0.263856    \nas.factor(fgov)23 -0.024421   0.099326  -0.246 0.805814    \nas.factor(fgov)24  0.113434   0.104063   1.090 0.275829    \nas.factor(fgov)25 -0.021529   0.087183  -0.247 0.804983    \nas.factor(fgov)26  0.193574   0.104879   1.846 0.065094 .  \nas.factor(fgov)27  0.107213   0.114648   0.935 0.349831    \nas.factor(fgov)28  0.098621   0.085004   1.160 0.246116    \nas.factor(fgov)29  0.131832   0.111982   1.177 0.239237    \nas.factor(fgov)30  0.095761   0.091298   1.049 0.294363    \nas.factor(fgov)31  0.145817   0.088828   1.642 0.100847    \nas.factor(fgov)32  0.107781   0.120921   0.891 0.372861    \nas.factor(fgov)33  0.213301   0.187727   1.136 0.256003    \nas.factor(fgov)34 -0.037991   0.100002  -0.380 0.704058    \nas.factor(fgov)35  0.221270   0.111052   1.992 0.046461 *  \nas.factor(fgov)36  0.200380   0.126085   1.589 0.112171    \nas.factor(fgov)37  0.098960   0.223601   0.443 0.658126    \nas.factor(fgov)38 -0.089886   0.157434  -0.571 0.568106    \nas.factor(fgov)39  0.115744   0.101604   1.139 0.254776    \nas.factor(fgov)40  0.148457   0.096826   1.533 0.125388    \nas.factor(fgov)41  0.137003   0.113494   1.207 0.227532    \nas.factor(fgov)42 -0.008749   0.126798  -0.069 0.944997    \nas.factor(fgov)43  0.194892   0.095958   2.031 0.042394 *  \nas.factor(fgov)44  0.120708   0.101907   1.184 0.236367    \nas.factor(fgov)45  0.311231   0.187292   1.662 0.096731 .  \nas.factor(fgov)46  0.094889   0.144907   0.655 0.512658    \nas.factor(fgov)47  0.126136   0.100125   1.260 0.207901    \nas.factor(fgov)48  0.103402   0.110296   0.937 0.348628    \nas.factor(fgov)49 -0.322913   0.255706  -1.263 0.206807    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1794076)\n\n    Null deviance: 380.30  on 1968  degrees of freedom\nResidual deviance: 338.36  on 1886  degrees of freedom\n  (266 observations deleted due to missingness)\nAIC: 2288.1\n\nNumber of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "basics_refresh/R_refresher.html#stan_glm",
    "href": "basics_refresh/R_refresher.html#stan_glm",
    "title": "R refresher",
    "section": "stan_glm()",
    "text": "stan_glm()\nWe can also estimate linear and generalized linear regressions using stan_glm() from the package rstanarm, which is how Gelman, Hill, and Vehtari (2020) introduce regression models in the text book for STV4022. The difference is that while glm() estimates the models using maximum likelihood, stan_glm() uses Bayesian methods.\nWe can re-estimate the model using stan_glm() like this:\n\n1library(rstanarm)\n2grewal_model_1_stan_glm &lt;- stan_glm(restraint ~\n                       nonvio2 + prime_frat + prime_future + prime_RU + \n                       prime_UN + oppose92coup2 + civwar + islamist + \n                       supp_sharia + active + conscript + soldier + junoff + \n                       senoff + branch2 + train_west + train_russia + \n                       train_china + young + sex + edu + urban_area + \n                       employed + student + arabic + econ1 + corr1 + \n                       dem + supp_opp_parties + continue + protested + \n                       preboutef + post_exp + mon +\n                       as.factor(fgov), \n                     data = grewal_data, \n                     family = \"gaussian\") \n\n\n1\n\nWe must remember to load the rstanarm package.\n\n2\n\nWe use stan_glm() instead of lm() or glm()\n\n\n\n\nWe can use print() to print out a summary of the model:\n\nprint(grewal_model_1_stan_glm, digits = 3)\n\nstan_glm\n family:       gaussian [identity]\n formula:      restraint ~ nonvio2 + prime_frat + prime_future + prime_RU + \n       prime_UN + oppose92coup2 + civwar + islamist + supp_sharia + \n       active + conscript + soldier + junoff + senoff + branch2 + \n       train_west + train_russia + train_china + young + sex + edu + \n       urban_area + employed + student + arabic + econ1 + corr1 + \n       dem + supp_opp_parties + continue + protested + preboutef + \n       post_exp + mon + as.factor(fgov)\n observations: 1969\n predictors:   83\n------\n                  Median MAD_SD\n(Intercept)        0.344  0.115\nnonvio2            0.098  0.028\nprime_frat        -0.024  0.036\nprime_future      -0.074  0.037\nprime_RU          -0.067  0.037\nprime_UN          -0.017  0.036\noppose92coup2      0.042  0.021\ncivwar             0.015  0.026\nislamist           0.019  0.029\nsupp_sharia        0.158  0.036\nactive            -0.003  0.023\nconscript         -0.015  0.023\nsoldier            0.012  0.023\njunoff             0.033  0.030\nsenoff             0.113  0.061\nbranch2Land        0.019  0.027\ntrain_west        -0.095  0.059\ntrain_russia      -0.020  0.034\ntrain_china        0.033  0.100\nyoung              0.134  0.041\nsex                0.021  0.055\nedu               -0.014  0.012\nurban_area         0.006  0.021\nemployed           0.061  0.024\nstudent           -0.018  0.045\narabic             0.094  0.028\necon1             -0.040  0.050\ncorr1              0.002  0.048\ndem                0.126  0.037\nsupp_opp_parties  -0.055  0.035\ncontinue          -0.033  0.021\nprotested          0.089  0.022\npreboutef         -0.037  0.049\npost_exp          -0.011  0.054\nmon               -0.017  0.007\nas.factor(fgov)2   0.117  0.091\nas.factor(fgov)3   0.061  0.107\nas.factor(fgov)4   0.060  0.085\nas.factor(fgov)5   0.133  0.085\nas.factor(fgov)6   0.082  0.094\nas.factor(fgov)7   0.007  0.088\nas.factor(fgov)8   0.213  0.113\nas.factor(fgov)9   0.062  0.086\nas.factor(fgov)10  0.078  0.106\nas.factor(fgov)11  0.165  0.109\nas.factor(fgov)12  0.174  0.087\nas.factor(fgov)13  0.014  0.089\nas.factor(fgov)14  0.176  0.094\nas.factor(fgov)15  0.133  0.106\nas.factor(fgov)16  0.054  0.075\nas.factor(fgov)17  0.107  0.089\nas.factor(fgov)18  0.214  0.096\nas.factor(fgov)19  0.117  0.081\nas.factor(fgov)20  0.314  0.114\nas.factor(fgov)21  0.029  0.098\nas.factor(fgov)22 -0.115  0.109\nas.factor(fgov)23 -0.023  0.102\nas.factor(fgov)24  0.117  0.102\nas.factor(fgov)25 -0.022  0.087\nas.factor(fgov)26  0.195  0.111\nas.factor(fgov)27  0.107  0.113\nas.factor(fgov)28  0.100  0.084\nas.factor(fgov)29  0.135  0.109\nas.factor(fgov)30  0.095  0.091\nas.factor(fgov)31  0.146  0.088\nas.factor(fgov)32  0.111  0.124\nas.factor(fgov)33  0.216  0.189\nas.factor(fgov)34 -0.037  0.104\nas.factor(fgov)35  0.222  0.110\nas.factor(fgov)36  0.197  0.128\nas.factor(fgov)37  0.101  0.214\nas.factor(fgov)38 -0.090  0.160\nas.factor(fgov)39  0.116  0.101\nas.factor(fgov)40  0.148  0.099\nas.factor(fgov)41  0.139  0.118\nas.factor(fgov)42 -0.010  0.126\nas.factor(fgov)43  0.194  0.098\nas.factor(fgov)44  0.121  0.104\nas.factor(fgov)45  0.314  0.193\nas.factor(fgov)46  0.093  0.141\nas.factor(fgov)47  0.127  0.101\nas.factor(fgov)48  0.101  0.111\nas.factor(fgov)49 -0.330  0.259\n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.424  0.007 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nWhen comparing the coefficients from grewal_model_1_stan_glm and grewal_model_1_glm, we see some slight differences, but the differences are very small and completely inconsequential!"
  },
  {
    "objectID": "basics_refresh/R_refresher.html#footnotes",
    "href": "basics_refresh/R_refresher.html#footnotes",
    "title": "R refresher",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is actually one more than Nyrup and Bramwell (2020) report, so, again, something we might want to investigate further: it turns out that if we instead look at the variable called who_gov$country_isocode, which is the numeric country code, there are only 177 countries (but one of them appears with two different names. Can you find out which?).↩︎"
  },
  {
    "objectID": "basics_refresh/Beyond_the_basics.html",
    "href": "basics_refresh/Beyond_the_basics.html",
    "title": "Beyond the basics: functions and iteration",
    "section": "",
    "text": "Your R skills from the previous courses (as reviewed here) will take you far. However, as you will now embark on doing more complex data collection and wrangling tasks, two more advanced programming tools will be useful for you to write more efficient code that is easy to edit and debug:\n\nWriting your own R functions: You are already used to employing R functions written by others, but you can also write your own functions, using function():\n\n\nmy_function &lt;- function(x, y, z){\n  ## Add some code that does something with the inputs x, y, and z \n}\n\nIf you find yourself repeatedly doing the same sequence of tasks on different variables (perhaps a across different projects), a good way to make your work more efficient and less error-prone would be to write your own function. After all, we don’t usually calculate the average in by summing up all the values and dividing by the number of observations. Instead we use the existing mean() function. The same logic applies to tasks for which there may not already be a readily available function. The only difference is that in such cases, we will need to define the function ourselves.\n\nIteration: Sometimes we need to make R repeat the same operation or sets of operations many times changing perhaps just one or two arguments to the functions we apply. For instance, maybe we have a many different .dta-files that we need to be load and bind together in a single data.frame. Repeatedly writing read_dta(\"filename.dta\") only changing the argument “filename.dta” is inefficient and error prone. Instead, we would want to use something called iteration to automatize this process.\n\nIn what follows, we will explain both how to write your own functions and how to employ different forms of iteration in R."
  },
  {
    "objectID": "basics_refresh/Functions.html",
    "href": "basics_refresh/Functions.html",
    "title": "Writing your own functions",
    "section": "",
    "text": "Sometimes, there is no function available in R that does exactly what we want to do. For instance, there is no built-in function in that gives you the modal value of a variable. This is not necessarily a big problem. We can find the modal value by combining other functions. Let’s say we want to fine the modal value on the variable called “v2x_regime” (a categorical variable measuring the type of regime in place in a given year on scale from “closed autocracy” to “liberal democracy”) in the Varieties of Democracy country-year dataset, which is available from here.\nWe could use table(), to get the frequency table for the variable:\n\n1load(\"../data/vdem.rda\")\n2table(vdem$v2x_regime)\n\n\n1\n\nloading the data\n\n2\n\nUsing table() to get a frequency table\n\n\n\n\n\n   0    1    2    3 \n9700 4439 2504 2400 \n\n\nFrom the frequency table, we quickly see that 0 (which according to the codebook is the value given to “closed autocracies”) is the most frequent value on this variable. However, this doesn’t give us the modal value as a single number output, which is typically what we want if we want to use the modal value for something else later.\nOne way to retrieve the model value would be to first create a frequency table using table(), then sort this table using sort(), then extract the names of the sorted table using names(), and finally extract the first element of this table (which will now be the most frequent value on our variable) using square brackets [] and the number 1 for the first element.\n\n1freq_table  &lt;- table(vdem$v2x_regime)\n2freq_table_sorted  &lt;- sort(freq_table, decreasing = TRUE)\n3sorted_names &lt;- names(freq_table_sorted)\n4modal_value &lt;- sorted_names[1]\n5print(modal_value)\n\n\n1\n\nFirst, we use table() to create a frequency table.\n\n2\n\nThen we sort() the frequency table so that the most frequent value is in the first row of the table.\n\n3\n\nNext, we extract the different values on the variable in the sorted order (we also get rid of the frequencies which we don’t need anymore).\n\n4\n\nWe extract the first element of the vector of sorted values, which will be the modal value.\n\n5\n\nFinally, we print() out the modal value.\n\n\n\n\n[1] \"0\"\n\n\nIf we only wanted the modal value of one variable, this approach would be fine. But what if also want the mode of many other variables in the dataset? We could copy paste the above code and adapt it to get the modal value for many different variables, but this is not a good approach for a big project such as an MA thesis.\nWhy not?\nFirst, It will be cumbersome to copy and edit multiple lines of code repeatedly.\nSecond, this approach is error prone: as we copy-paste and edit many lines of code, we are very likely to make mistakes and those mistakes will be difficult to spot.\nThird, the copy-pasting and editing approach will make our R-scripts very long and difficult to read.\nA better approach is therefore to define our own function."
  },
  {
    "objectID": "basics_refresh/Functions.html#multiple-arguments",
    "href": "basics_refresh/Functions.html#multiple-arguments",
    "title": "Writing your own functions",
    "section": "Multiple arguments",
    "text": "Multiple arguments\nA first way to extend the idea is to allow the function to have more than one argument. The possibility of having more than argument should come as no surprise given that we have already been using functions written by others that take more two arguments. For instance, mean() takes three arguments, x, trim, and na.rm.\nLet’s make a (relatively silly) function that takes two arguments, x and y, divides x by y, and returns TRUE if the resulting number is an integer and FALSE if the number is a fraction:\n\ndividing_x_by_y_gives_integer &lt;- function(x, y) {\n  ifelse(x/y == as.integer(x/y), TRUE, FALSE)\n}\n\nTo use this function you need to specify both x and y. E.g.\n\ndividing_x_by_y_gives_integer(x = 2, y = 1)\n\n[1] TRUE\n\n\nor\n\ndividing_x_by_y_gives_integer(x = 1, y = 2)\n\n[1] FALSE\n\n\nSpecifying only one of the arguments will produce an error:\n\ndividing_x_by_y_gives_integer(x = 5)\n\nError in dividing_x_by_y_gives_integer(x = 5): argument \"y\" is missing, with no default"
  },
  {
    "objectID": "basics_refresh/Functions.html#conditionals",
    "href": "basics_refresh/Functions.html#conditionals",
    "title": "Writing your own functions",
    "section": "Conditionals",
    "text": "Conditionals\nSometimes we want the behavior of our function to change depending on our arguments or on some other conditions. For instance, in our modal_value function we have not specified what the function should do with missing values (NAs). What happens if we run the function on a vector in which NA is the most frequent value:\n\nnew_numbers &lt;- c(10,3,  6,NA, 5, NA,  6, 10,  8,  3,NA, NA,  6,  4,  4,  4, 10,  2, NA, NA,  1,  2,  5,  5,  6)\n\nmodal_value(new_numbers)\n\n[1] \"6\"\n\n\nThe way we wrote the function, implicitly omits all the NAs. The reason for this is that the NAs are not included in the frequency table created by table():\n\ntable(new_numbers)\n\nnew_numbers\n 1  2  3  4  5  6  8 10 \n 1  2  2  3  3  4  1  3 \n\n\nBut maybe we sometimes (but not always) want to know if NA is the modal value? If so, we need to add an argument to the function that tells the function whether it should also consider the NAs (following the precedence set by other functions, such as mean(), we might call this argument na.rm) and have the behavior of the function change conditional on the value on this argument.\nTo have the behavior of our R functions change depending on some condition(s) we can use if(){} and else{}. Inside the parenthesis following if you specify a logical condition and inside the curly brackets ({ }) you specify what happen if that condition is met. In the curly brackets following else, you specify what should happen if the logical condition is not met. If you want, you may have multiple if() statements before a final else statement. What comes after else will then specify what should happen if the none of the conditions in the previous if() statements were met.\n\n1modal_value &lt;- function(x, na.rm ){\n2  if(na.rm == FALSE){\n    freq_table  &lt;- table(x, useNA = \"ifany\")\n    freq_table_sorted  &lt;- sort(freq_table, decreasing = TRUE)\n    sorted_names &lt;- names(freq_table_sorted)\n    modal_value &lt;- sorted_names[1]\n    return(modal_value)\n  }\n3  else{\n    freq_table  &lt;- table(x)\n    freq_table_sorted  &lt;- sort(freq_table, decreasing = TRUE)\n    sorted_names &lt;- names(freq_table_sorted)\n    modal_value &lt;- sorted_names[1]\n    return(modal_value)\n  }\n}\n\n\n1\n\nOur function now takes two arguments, x and na.rm.\n\n2\n\nHere we tell our function what to do if na.rm is set to FALSE. Inside the curly brackets we specify that in these cases we want to set the useNA argument of table() to \"ifany\", which means that NA will be included as a value in the frequency table.\n\n3\n\nInside the curly brackets following else, we specify what should happen if none of the above if statements (in this case we only have one such statement, but we can have multiple).\n\n\n\n\n\n1modal_value(new_numbers, na.rm = TRUE)\n\n\n1\n\nWhen na.rm = TRUE, our function will behave like before. NAs will not be counted as a value.\n\n\n\n\n[1] \"6\"\n\n\n\n1modal_value(new_numbers, na.rm = FALSE)\n\n\n1\n\nWhen na.rm = FALSE, our function will now include NAs as values and NA will be the modal value for our vector.\n\n\n\n\n[1] NA\n\n\n\n1modal_value(new_numbers)\n\n\n1\n\nIf we forget to supply a value to the na.rm argument, our function will now fail (and throw an error)!\n\n\n\n\nError in modal_value(new_numbers): argument \"na.rm\" is missing, with no default"
  },
  {
    "objectID": "basics_refresh/Functions.html#default-arguments",
    "href": "basics_refresh/Functions.html#default-arguments",
    "title": "Writing your own functions",
    "section": "Default arguments",
    "text": "Default arguments\nHaving the function fail if we forget to supply the na.rm argument seems somewhat suboptimal. We can avoid this problem by giving the argument a default value. We can set default values when we define the function:\n\n1modal_value &lt;- function(x, na.rm = FALSE){\n  if(na.rm == FALSE){\n    freq_table  &lt;- table(x, useNA = \"ifany\")   \n    freq_table_sorted  &lt;- sort(freq_table, decreasing = TRUE) \n    sorted_names &lt;- names(freq_table_sorted) \n    modal_value &lt;- sorted_names[1] \n    return(modal_value) \n  }\n  else{\n    freq_table  &lt;- table(x)  \n    freq_table_sorted  &lt;- sort(freq_table, decreasing = TRUE) \n    sorted_names &lt;- names(freq_table_sorted)\n    modal_value &lt;- sorted_names[1]\n    return(modal_value)\n  }\n}\n\n\n1\n\nHere we set the default value for na.rm to be FALSE.\n\n\n\n\nIf we now run the function with declaring the na.rm argument. The function will treat na.rm as having been declared as FALSE\n\nmodal_value(new_numbers)\n\n[1] NA\n\n\nBut we can override this default behavior by specifying the argument:\n\nmodal_value(new_numbers, na.rm = TRUE)\n\n[1] \"6\""
  },
  {
    "objectID": "Quarto_documents/writing_quarto.html",
    "href": "Quarto_documents/writing_quarto.html",
    "title": "Writing Quarto documents",
    "section": "",
    "text": "To write up your solutions to the weekly challenges (and for most of your writing, really!), you should use the Quarto publishing system.\nWorking in Quarto may be different from how you are used to writing documents, e.g. in Microsoft Word. At first, Quarto may seem more challenging. As you work on larger quantitative projects, this type of publishing system is, however, going to prove very useful (we promise!). This is particularly so when our projects also involves large amounts of R code and figures and tables that may need to be updated continuously as the project develops. In the following video, we give you a very brief overview of Quarto and our main motivation for writing our documents in Quarto:"
  },
  {
    "objectID": "Quarto_documents/writing_quarto.html#what-is-quarto",
    "href": "Quarto_documents/writing_quarto.html#what-is-quarto",
    "title": "Writing Quarto documents",
    "section": "What is Quarto?",
    "text": "What is Quarto?\nQuarto allows us to integrate the three main components of our projects: (1) our prose, (2) our R code, and (3) the figures and tables our R code produces, in .qmd-files. We then “render” the .qmd-files to produce .pdf, .html, or .docx files that we can share with others.\nWe will write our prose in plain text and accompany it with simple code that determines the formatting of the document. We can insert chunks of R code which we can run interactively as we work in RStudio, but which we can also set to rerun every time the document is rendered – ensuring that the included output is always up to date. Tables and figures can be produced from these R chunks to be included in the output document automatically. You can create, edit, and render your Quarto documents directly from RStudio.\nBelow, we illustrate how to create Quarto documents and their main features. Much more extensive documentation of Quarto is available on quarto.org."
  },
  {
    "objectID": "Quarto_documents/writing_quarto.html#creating-your-first-quarto-document",
    "href": "Quarto_documents/writing_quarto.html#creating-your-first-quarto-document",
    "title": "Writing Quarto documents",
    "section": "Creating your first Quarto document",
    "text": "Creating your first Quarto document\nThe best way to learn about Quarto is to start using it. The best way to start using Quarto is to open a blank document and start writing!\nIn this video, we provide a quick example of how you create new document, add some content and render it to produce a PDF:\n\n\nSo, to recap briefly, you should:\n\nClick on “New file” in RStudio and select “Quarto Document…”\nA new window will open where you can add a title and author to your document (what you write here is not so important, you can change it later) and where you will have to select an output format (html, pdf, or Word). We recommend selecting PDF as this is most likely the most suitable format for your MA thesis or similar documents.1 You will also be able to change the document format later if you regret your choice, so don’t worry too much about it.2\nClick “Create Empty Document”. This gives you a blank document (but including the title and author information if you specified those options in the previous step). If you instead click “Create”, you will get a document with some sample content. That’s also fine, you can remove what you don’t need or use it to figure out how various Quarto features work.\nYou are now ready to start adding some content to your document. You may add some text and some chunks with R code. You may also change how the document looks like in the YAML header. We will discuss these things in more detail later. For now, you may try something very simple. For instance, you may write a document that looks like this:\n\n\n\nNow that you have a document with some text in it, you can try to render it. Basically, you will then create a PDF (or your selected output format) based on the content and formatting instructions in your .qmd-file. You can render your document by clicking the “Render” button in RStudio. Rendering the above example will create a PDF file that looks like this:\n\n\n\n\n\n\nAs you can see, the PDF includes the text we wrote, an echo of our R code – nicely formatted as a code chunk – and the output that the code chunk produces (here simply the “hello world” message). We can change whether the R code and outputs are displayed in the PDF or not. For now the key point is that Quarto allows us to work on our text and code in the same documents."
  },
  {
    "objectID": "Quarto_documents/writing_quarto.html#the-yaml-header",
    "href": "Quarto_documents/writing_quarto.html#the-yaml-header",
    "title": "Writing Quarto documents",
    "section": "The YAML header",
    "text": "The YAML header\nThe code in the beginning of the .qmd file is called the the YAML header. The YAML header is where we provide general information about the document and how it should be formatted. For instance, we may want to add an “author” to the document and to have the date on which the document was rendered printed below the title. If so, we can specify author: and date: in the YAML header. Writing date: today will update the date automatically every time the document is rendered.\nSo, if we change the YAML header to what we have in the left hand screenshot below, the (top of the) rendered PDF will look like in the right hand screenshot:\n\n\n\n\n\n\n\nPerhaps, we would prefer a different format for the date, if so we can change the format in which the date is printed by specifying date-format in the YAML header. For instance:\n\n\n\n\n\n\n\nYou will find more options for how to specify and format the date of your documents here.\nWe can specify a bunch of other things in the YAML header to customize our document to look how we would like it to look. We will introduce more options as they become relevant."
  },
  {
    "objectID": "Quarto_documents/writing_quarto.html#visual-vs.-source-editor",
    "href": "Quarto_documents/writing_quarto.html#visual-vs.-source-editor",
    "title": "Writing Quarto documents",
    "section": "Visual vs. source editor",
    "text": "Visual vs. source editor\nIf you look closely, you will see that there are two editors in RStudio in which you edit your Quarto documents, called “Source” and “Visual”. By default, we are using the “Visual” editor and the line editor: visual in the YAML header tells RStudio to open the documents in the “Visual” editor. The difference between the two is that the visual editor contains buttons for different formatting options and tries to mimic how the text will eventually look like when the document is rendered also in the editor (kind of like Microsoft Word). The source editor only displays the code (but editing using code will be much faster once you get the hang of it!):\n\n\nWe prefer the source editor over the visual editor because code is easier to write and edit. In what follows, we will therefore be using the source editor. However, if you prefer the visual editor, that is perfectly fine. It is also possible to switch back-and-forth between the visual and source editor."
  },
  {
    "objectID": "Quarto_documents/writing_quarto.html#formatting-the-text",
    "href": "Quarto_documents/writing_quarto.html#formatting-the-text",
    "title": "Writing Quarto documents",
    "section": "Formatting the text",
    "text": "Formatting the text\nYou use simple code to format the text of your document. The following commands are useful for getting started:\n\nYou can use hashtags (#) to create headers in your text. The number of hashtags will determine the position of the header in the hierarchy of headers in your document:\n\n\n\n\n\n\n\n\nNote that to make the sections numbered, we had to specify that this is what we wanted in the YAML header. We can also turn off the numbering:\n\n\n\n\n\n\n\n\nYou can use asterisks (*) to put some text in italics. For instance:\n\n\n\n\n\n\n\n\n\nYou can similarly use double asterisks (**) to make the text bold. For instance:\n\n\n\n\n\n\n\n\n\nTriple asterisks (***) will put the text in italics and make it bold. Let’s do it:\n\n\n\n\n\n\n\n\n\nIf you want to make a list with bullet points (like the list you are reading right now), you can use a hyphen followed by a line indentation (-) to make each bullet point. For this to work, you will need a blank line before you start the list:\n\n\n\n\n\n\n\n\n\nUsing indentation, we can add different layers of subitems to our lists:\n\n\n\n\n\n\n\n\n\nWe can similarly make numbered lists like this:\n\n\n\n\n\n\n\n\n\nWe can add hyperlinks in our text like this: [text that we want to appear](url). For instance, we may link to Kieran Healy’s The Plain Person’s Guide to Plain Text Social Science in case some of you want to read his take on why we should use tools such as Quarto to write our documents.3\n\n\n\n\n\n\n\n\nThere are, of course, many more formatting options that you want to use for your text. We will not illustrate all of them here. If you are looking for other formatting options, this page has a nice overview. Moreover, we made this website using Quarto, so if you wonder how to do something that we did somewhere on this website, you may consult our code here."
  },
  {
    "objectID": "Quarto_documents/writing_quarto.html#adding-r-chunks",
    "href": "Quarto_documents/writing_quarto.html#adding-r-chunks",
    "title": "Writing Quarto documents",
    "section": "Adding R chunks",
    "text": "Adding R chunks\nYou can add a new R chunk by clicking the green “Insert a new code chunk” button or by pressing Ctrl+Alt+I/Cmd+Alt+I. Your code chunk will start with ```{r} and end with ```. For instance, we may include an R chunk that just prints “hello world”, like this:\n\n```{r}\nprint(\"hello world\")\n```\n\n[1] \"hello world\"\n\n\nInside the code chunk, you write R code that will run when you render the document (unless you tell R to not evaluate the R chunk when rendering) and you can run it interactively line-by-line just as you would when working in an R script."
  },
  {
    "objectID": "Quarto_documents/writing_quarto.html#the-different-code-execution-options",
    "href": "Quarto_documents/writing_quarto.html#the-different-code-execution-options",
    "title": "Writing Quarto documents",
    "section": "The different code execution options",
    "text": "The different code execution options\nWe can set a number of options to our code chunks determining for instance whether the code is evaluated when the code is rendered (eval), whether the echo of the code should be printed in as a code chunk in your output document (echo), whether the output of the code chunk should be included in the document or not (ouput), etc. An overview of the different execution options is available here.\nYou declare these options at the beginning of the chunk using #| followed by the code for the option you want to set.\nLet’s look at some examples (the chunks on the left shows the code as written in our quarto document and the chunks on the left shows the resulting output in our rendered document)\n\nFirst we can use eval to decide whether the code should run when we render the document or not. If we set #| eval: true, the code will run:\n\n\n\n\n```{r}\n#| eval: true\nprint(\"hello world\")\n```\n\n[1] \"hello world\"\n\n\n\n\nprint(\"hello world\")\n\n[1] \"hello world\"\n\n\n\n\nIf we set #| eval: false, it will not run (but an echo of the code will still appear in the document):\n\n\n\n```{r}\n#| eval: false\nprint(\"hello world\")\n```\n\n\n\nprint(\"hello world\")\n\n\n\n\nWe can also prevent the echo from being printed in our rendered document (we typically don’t want to show our R code in the main text of our MA theses or academic articles. For your STV4030A assignments, you should, however, show us your code!):\n\n\n\n\n```{r}\n#| eval: true\n#| echo: false\n\n\nprint(\"hello world\")\n```\n\n[1] \"hello world\"\n\n\n\n\n\n[1] \"hello world\"\n\n\n\n\n\nSometimes, we may want to set both echo and eval to false. Maybe we writing a document where we don’t want to show all our code (at least not in the main text) and we also don’t want to rerun the code every time we render the document (maybe it scrapes a website or makes some slow computations. We just want to do those things once and then save the results somewhere). It may still be useful for our workflow to have this code as one or more R chunks in our Quarto documents, but we want to set both echo and eval to false:\n\n\n\n\n```{r}\n#| eval: false\n\n#| eval: false\n#| echo: false\n\n\nprint(\"hello world\")\n```\n\n\n\n\n\n\nIt may be that we want to evaluate the code, but we don’t want the output to be printed below the code chunk. If so, you can set output to false:\n\n\n\n\n```{r}\n#| eval: true\n#| output: false\n\nprint(\"hello world\")\n```\n\n\n\nprint(\"hello world\")\n\n\n\nSetting output to false may be useful for instance for the code chunk that loads your packages. If we set output to true for the following chunk, it will produce a lot of text that we don’t really need in our documents (not even for an STV4030A assignment!)\n\n```{r}\n#| eval: true\n#| output: true\n\nlibrary(texreg)\n```\n\nVersion:  1.38.6\nDate:     2022-04-06\nAuthor:   Philip Leifeld (University of Essex)\n\nConsider submitting praise using the praise or praise_interactive functions.\nPlease cite the JSS article in your publications -- see citation(\"texreg\").\n\n```{r}\n#| eval: true\n#| output: true\n\nlibrary(rstanarm)\n```\n\nLoading required package: Rcpp\n\n\nThis is rstanarm version 2.21.4\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\n\nIf we instead set output to false, we can load the packages without printing out all their messages:\n\n\n\n```{r}\n#| eval: true\n#| output: false\n\nlibrary(texreg)\nlibrary(rstanarm)\n```\n\n\n\nlibrary(texreg)\nlibrary(rstanarm)\n\n\n\nIt also possible to be more specific about what we would want to include or not. For instance, we may want to include most output but to supress the messages that are produced when packages are loaded or to suppress all the messages that stan_glm() produces as it fits models. If so, we can set message to false (even if output is set to true below, the messages from loaded the packages are not included:\n\n\n\n```{r}\n#| eval: true\n#| message: false\n#| output: true\n\nlibrary(texreg)\nlibrary(rstanarm)\n```\n\n\n\nlibrary(texreg)\nlibrary(rstanarm)\n\n\n\nAgain there are a number of other options and features to consider in the much more extensive documentation on the Quarto website.\n\nGlobal and local execution options\nWhen using #| to declare execution options a the beginning of a code chunk, what we declare will only apply to this individual chunk. However, it will often be the case that we want the same execution options to apply to all the chunks (or to most of them). If so, it will be inconvenient to have to declare exactly the same executions options for each new chunk!\nIf you want execution options to apply to the entire documents, you may specify them in the YAML header! The options are same, but you will use the YAML syntax rather than the #|. Consider the following examples:\nFirst, we updated the my_first_quarto.qmd with some new R chunks at the beginning of the document. We are not specifying any execution options, so when rendered we will just get the default options (i.e. the code will be evaluated and echoed, and the output will be included):\n\n\n\n\n\n\n\nSecond, we decide that we don’t want to include echoes of the R chunks or any output in our document. We declare these options in the YAML header by including execute: followed by new indented lines with each of the options. Our updated YAML header will then look like this:\n\nexecute: \n  eval: true\n  echo: false\n  output: false\n\nIf you compare the .qmd-file and the rendered document, you will see that code and output are no longer included in the document:\n\n\n\n\n\n\n\nThird, we realize that we do want our ASCII cow to be printed (but we still don’t want any other output to be included). We therefore override the ouput option for the chunk that prints the cow. In other words, we keep the YAML header as specified above, but change the execution options for this one chunk to be:\n\n#| output: true\nsay(motivational_message, \"cow\")\n\nWe have now included the cow, but no other output or code echos in our rendered document:"
  },
  {
    "objectID": "Quarto_documents/writing_quarto.html#including-figures",
    "href": "Quarto_documents/writing_quarto.html#including-figures",
    "title": "Writing Quarto documents",
    "section": "Including figures",
    "text": "Including figures\nA major advantage of Quarto is that we can create our figures in R chunks and have them be included in our documents. If we change something to our figure, it will automatically be updated when we re-render the document. We discuss how to wrangle the data into the right shape using dplyr here and how make visualizations in ggplot2 here (so if you wonder about the R code, those places are where you should look)]. Here we focus just on the Quarto features.\nTo illustrate how to include figures, we use data from the Varieties of Democracy’ party-level dataset to plot the the extent to which different Norwegian political parties use “populist rhetoric” over time. The code for creating the figure looks like this:\n\n1Sys.setlocale(\"LC_ALL\", \"nb-NO.UTF-8\")\n2library(dplyr)\nlibrary(ggplot2)\n3load(\"../data/vparty.RData\")\n4norwegian_populism_figure &lt;- vparty %&gt;%\n5  filter(country_name == \"Norway\", year &gt; 1970) %&gt;%\n6  mutate(party_name = case_when(v2paenname == \"Centre [Agrarian] Party\" ~ \"Senterpartiet\",\n                                v2paenname == \"Christian Democratic Party\" ~ \"Kristelig folkeparti\",\n                                v2paenname == \"Conservative Party\" ~ \"Høyre\",\n                                v2paenname == \"Progress Party [Anders Lange’s Party]\" ~ \"Fremskrittspartiet\",\n                                v2paenname == \"Norwegian Labour Party\" ~ \"Arbeiderpartiet\",\n                                v2paenname == \"Liberal Party of Norway\" ~ \"Venstre\",\n                                v2paenname == \"Socialist Left Party\" ~ \"Sosialistisk Venstreparti\")) %&gt;%\n7  ggplot(aes(x = year,\n             y = v2xpa_popul,\n             ymin = v2xpa_popul_codelow,\n             ymax = v2xpa_popul_codehigh,\n             group = party_name,\n             fill = party_name)) +\n8  geom_ribbon(alpha = 0.7) +\n9  geom_line() +\n10  scale_fill_manual(name = \"Political party\",\n                    breaks = c(\"Senterpartiet\", \"Kristelig folkeparti\",\n                               \"Høyre\", \"Fremskrittspartiet\", \"Arbeiderpartiet\", \"Venstre\", \"Sosialistisk Venstreparti\"),\n                    values = c(\"darkgreen\", \"yellow\", \"blue\", \"pink\", \"red\", \"lightblue\", \"darkred\"))+\n11  theme_classic()+\n12  ylab(\"Populist rhetoric\") +\n  xlab(\"Year\")\n\n\n1\n\nSince the code below using Norwegian characters(ÆØÅ), it is useful to specify that we are working in Norwegian “locale” with “UTF-8” encoding. We discussed locales and encoding here.\n\n2\n\nWe are loading dplyr to get functions for wrangling the data and ggplot2 for the functions we use to visualize the data\n\n3\n\nLoading the data, which you can also get from here.\n\n4\n\nWe start with vparty which we will do something with. The output should be assigned to norwegian_populism_figure\n\n5\n\nWe use filter() from dplyr to subset the dataset to observations from Norway after the year 1970\n\n6\n\nUsing mutate() from dplyr we create a new variable party_name. case_when() from dplyr is more elegant than multiple ifelse() when there are many statements in our code for recoding the variable.\n\n7\n\nWe use ggplot()to start a new plot. Since we are continuing from the pipe (%&gt;%) above, we don’t to specify the data. R will automatically use the data created with the piped operations above, but we need to specify the various aesthetic mappings in our plot using aes(). For instance, we want year to be the variable on the x axis.\n\n8\n\ngeom_ribbon()creates the confidence intervals. The upper limit and lower borders are given by ymax and ymin which we declared inside aes() in the line above. The colors of the different confidence intervals are given by fill which we also declared above. alpha determines how transparent the confidence intervals should be.\n\n9\n\ngeom_line()creates the lines using the aesthetic mappings y, x, and group that we already declared.\n\n10\n\nscale_fill_manual() can be used to manually change the colors (as well as changing the title of the legend)\n\n11\n\ntheme_classic() removed unnecessary noise from the plot, most notably the grey background and the grid lines.\n\n12\n\nylab() and xlab() can be used to change labels on the axes.\n\n\n\n\nThe code above assigns the figure to the object norwegian_populism_figure. If we want to include in our Quarto document, we can add an R chunk that prints out the object and allow the output to be included:\n\n```{r}\n#| output: true\n\nnorwegian_populism_figure\n```\n\n\n\n\nWe might also want to give the figure a nice caption. If so we can specify the fig-cap execution option:\n\n```{r}\n#| output: true\n#| fig-cap: The use of populist rhetoric by major Norwegian political parties\n\nnorwegian_populism_figure\n```\n\n\n\n\nThe use of populist rhetoric by major Norwegian political parties\n\n\n\n\nIf we specify the label execution option and start the label with fig-, the caption will start with figure and the figures will be automatically numbered (automatically updating the numbering the figures is incredibly helpful because we are likely to create more figures, remove some figures, and change the sequence in which they appear. We should not be the ones keeping track of the numbering. That is work for our computer! ).\n\n```{r}\n#| output: true\n#| label: fig-populism\n#| fig-cap: The use of populist rhetoric by major Norwegian political parties\n\nnorwegian_populism_figure\n```\n\n\n\n\nFigure 1: The use of populist rhetoric by major Norwegian political parties\n\n\n\n\nWhen figures are labeled we can also cross-reference them in the text using @fig-our-label. So if we want to refer to the figure created below, we would write @fig-populism. For instance, the text in the Quarto document may look like this:\n@fig-populism shows that the Fremskrittspartiet has consistently used the most populist rhetoric. However, both Senterpartiet and Sosialistisk Venstreparti used more populist rhetoric  during the time of the EU referendum in 1994 and Senterpartiet has started using considerably more populist rhetoric in recent years, making it more similar to Fremskrittspartiet (at least as far as populist rhetoric goes). \nThe text in the rendered document will then look like this:\n“Figure 1 shows that the Fremskrittspartiet has consistently used the most populist rhetoric. However, both Senterpartiet and Sosialistisk Venstreparti used more populist rhetoric during the time of the EU referendum in 1994 and Senterpartiet has started using considerably more populist rhetoric in recent years, making it more similar to Fremskrittspartiet (at least as far as populist rhetoric goes)”\nClicking on the label Figure 1 should take you directly to the figure and the numbering will update as needed as you add or remove figures in your document.\nWe can include the figure again with a different label and it will be included as Figure 2. While we are at it, we might also want change the size of the figure using the fig-width and fig-height options:\n\n```{r}\n#| output: true\n#| label: fig-new-label\n#| fig-cap: The use of populist rhetoric by major Norwegian political parties\n#| fig-width: 10\n#| fig-height: 10\n\nnorwegian_populism_figure\n```\n\n\n\n\nFigure 2: The use of populist rhetoric by major Norwegian political parties\n\n\n\n\nAs always, there are many more options that can you specify listed in the Quarto documentation!"
  },
  {
    "objectID": "Quarto_documents/writing_quarto.html#including-tables",
    "href": "Quarto_documents/writing_quarto.html#including-tables",
    "title": "Writing Quarto documents",
    "section": "Including tables",
    "text": "Including tables\nWe can similarly make our tables in R chunks and have them automatically included in our Quarto documents.\nNote that there are many different packages that automatize the process of creating different types of tables from tables containing summary statistics to regression tables (suited for different types of regression models). You should never manually create your table by copy-pasting the output from R into a spreadsheet or something similar. This is work your computer should be doing for you! We illustrate how to make various types of tables here.\nFor now, we will focus on the Quarto syntax for including tables based on R chunks, which we will illustrate by including a small data.frame as a table. Specifically, we will include the populist rheteric score for all Norwegian parties in 1993 (i.e. just prior to the referendum on EU membership). We use the function kable() from the package knitr to create a nice looking table:\n\n1library(knitr)\n2vparty %&gt;%\n  filter(country_name == \"Norway\", year == 1993) %&gt;%\n  select(v2paenname, v2xpa_popul) %&gt;%\n  arrange(v2xpa_popul) %&gt;%\n3  kable(col.names = c(\"Party\", \"Use of populist rhetoric\"))\n\n\n1\n\nHere we use the knitr package to just print out a data.frame as a nice table.\n\n2\n\nWe wrangle the data.frame into the shape of the table we want.\n\n3\n\nWe use the kable() function to create the package. The argument col.names allows us to easily change the column names in the table.\n\n\n\n\n\n\n\nParty\nUse of populist rhetoric\n\n\n\n\nConservative Party\n0.050\n\n\nChristian Democratic Party\n0.128\n\n\nNorwegian Labour Party\n0.222\n\n\nSocialist Left Party\n0.451\n\n\nCentre [Agrarian] Party\n0.561\n\n\nProgress Party [Anders Lange’s Party]\n0.809\n\n\n\n\n\nJust like with did for the figures, we can adjust the execution options for the R chunk to include a caption, a label that we can use for cross-referencing the table, etc.:\n\n```{r}\n#| tbl-cap: Populism score for Norwegian political parties in the year of the 1993 parliamentary election\n#| label: tbl-populism\nlibrary(knitr) #\nvparty %&gt;% \n  filter(country_name == \"Norway\", year == 1993) %&gt;% \n  select(v2paenname, v2xpa_popul) %&gt;% \n  arrange(v2xpa_popul) %&gt;%  \n  kable(col.names = c(\"Party\", \"Use of populist rhetoric\"))\n```\n\n\n\nTable 1: Populism score for Norwegian political parties in the year of the 1993 parliamentary election\n\n\nParty\nUse of populist rhetoric\n\n\n\n\nConservative Party\n0.050\n\n\nChristian Democratic Party\n0.128\n\n\nNorwegian Labour Party\n0.222\n\n\nSocialist Left Party\n0.451\n\n\nCentre [Agrarian] Party\n0.561\n\n\nProgress Party [Anders Lange’s Party]\n0.809\n\n\n\n\n\n\nAdding the label @tbl-populism allows us to cross-reference Table 1.\nWe will show you have to generate publication quality tables that can included directly into your Quarto documents directly from R here."
  },
  {
    "objectID": "Quarto_documents/writing_quarto.html#sec-cross-references",
    "href": "Quarto_documents/writing_quarto.html#sec-cross-references",
    "title": "Writing Quarto documents",
    "section": "Cross-referencing",
    "text": "Cross-referencing\nWe have showed you have to use the @ symbol to reference figures and tables based on the labels we create using the label execution option. It is also useful to reference other things. For instance, we might want to cross-reference the different sections of our documents.\nFor instance, we may cross-reference the section we are currently writing by adding {#sec-cross-references} after the section header, so that the full section header is ## Cross-referencing{#sec-cross-references}. We can then refer back to it using @sec-cross-references, which will direct readers back to Section 10.\nQuarto knows that we are referring to section, because we started the label with sec-. Somewhat awkwardly it uses the number of the section, even if we have turned off displaying the numbers in the section headers (the numbering is, however, likely to be useful in your MA thesis when the sections will probably be numbered. If you want the first level in the hierarchy of headers to be different “chapters” instead of “sections” (which again, you probably want in your MA thesis), you can add the following code to your YAML header:\ncrossref:\n  chapters: true\nYou can cross-reference all sorts of elements in your documents. More documentation on how to do so is available here."
  },
  {
    "objectID": "Quarto_documents/writing_quarto.html#citations-and-bibliographies",
    "href": "Quarto_documents/writing_quarto.html#citations-and-bibliographies",
    "title": "Writing Quarto documents",
    "section": "Citations and bibliographies",
    "text": "Citations and bibliographies\nYou should not make your bibliographies and keep track of what you cite (or what you ended up at the last minute not citing after all) manually. You should use a Quarto to automatically generate the bibliography based on what you cite in your document.\nTo that you will need a bibliographic data source file, such as a .bib, .bibtex. You can export such a file from most citation managers (like Zotero or whatever you are using). But you can also just open a plain text file in R, grab the BibTeX code for the citation from Google Scholar and save the file as a .bib-file:\n\n\nOnce you have a .bib file, you may include it in your YAML header using bibliography: followed by the name of the .bib file. You can then use @ and the label for the citation in the text of your document. The Quarto document in the screen shot to the left will produce the PDF in the screenshot in the right:\n\n\n\n\n\n\n\nTo just include “Shen-Bayh (2018)”, we can just write @shen2018strategies in our document.\nOften, we want to have the reference inside a parenthesis, which can do by wrapping the citation in square brackets: [@shen2018strategies] will produce (Shen-Bayh 2018).\nWe can also add references to specific pages or some leading text in the parenthesis: [see e.g. @shen2018strategies, 334--343] produces (see e.g. Shen-Bayh 2018, 334–43).\nOf course we can add multiple citations inside the same parenthesis, e.g. [@shen2018strategies; @shen2022undue] will yield (Shen-Bayh 2018, 2022).\nIf you consider the list of references at the bottom of this page, you will notice that the two items are formatted differently reflecting that @shen2018strategies refers to an article and @shen2022undue refers to a book. Quarto will know what is what based on the BibTeX code:\n@article{shen2018strategies,\n  title={Strategies of repression: Judicial and extrajudicial methods of autocratic survival},\n  author={Shen-Bayh, Fiona},\n  journal={World Politics},\n  volume={70},\n  number={3},\n  pages={321--357},\n  year={2018},\n  publisher={Cambridge University Press}\n}\n@book{shen2022undue,\n  title={Undue Process: Persecution and Punishment in Autocratic Courts},\n  author={Shen-Bayh, Fiona},\n  year={2022},\n  publisher={Cambridge University Press}\n}\nYou will find our .bib-file here.\nAs always, there is more extensive documentation on how to include citations on the Quarto website."
  },
  {
    "objectID": "Quarto_documents/writing_quarto.html#footnotes",
    "href": "Quarto_documents/writing_quarto.html#footnotes",
    "title": "Writing Quarto documents",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRendering your document as a PDF requires a LaTeX installation on your computer. If you don’t already have a LaTeX installation, you can simply install the “tinytex” R package.↩︎\nhtml is also a great format to use, but we are guessing it is not the format you will use for your MA thesis, so for that reason we will emphasize PDFs. If you struggle rendering the document to a PDF, it would, however, be a good idea to render to html instead so that you can get started on your work.↩︎\nHealy (2019) discusses an older publishing system called RMarkdown instead of Quarto. The two systems are, however, very similar and all his arguments for using RMarkdown also apply to Quarto.↩︎"
  },
  {
    "objectID": "text_data/read_texts.html",
    "href": "text_data/read_texts.html",
    "title": "Reading Texts into R",
    "section": "",
    "text": "In the loading data section, we outlined various ways for reading data into R. In this section, we will delve deeper into one specific type of data and how to load this into R, namely raw text data. We will cover plain text files (.txt), word documents (.docx), and PDF documents (.pdf). Do remember, however, that text data can also be provided in the file formats covered in the loading data section.\nThe data used in this section are gathered from the quanteda package, and includes presidential inaugural speeches from 1789. There are 6 variables in the data:\nWe can load the data into R initially by using the data() function, change the object name (as it was annoyingly long), and convert() the corpus format to a data.frame:\nlibrary(tidyverse)\nlibrary(quanteda)\n\ndata(data_corpus_inaugural, package = \"quanteda\")\n\npres_inaug &lt;- data_corpus_inaugural # just making a shorter name for the object\nrm(data_corpus_inaugural)\n\npres_inaug &lt;- pres_inaug %&gt;% \n  quanteda::convert(., to = \"data.frame\") %&gt;% \n  mutate(text = str_replace_all(text, \"\\\\s{2}\", \" \"))\n\nglimpse(pres_inaug)\n\nRows: 59\nColumns: 6\n$ doc_id    &lt;chr&gt; \"1789-Washington\", \"1793-Washington\", \"1797-Adams\", \"1801-Je…\n$ text      &lt;chr&gt; \"Fellow-Citizens of the Senate and of the House of Represent…\n$ Year      &lt;int&gt; 1789, 1793, 1797, 1801, 1805, 1809, 1813, 1817, 1821, 1825, …\n$ President &lt;chr&gt; \"Washington\", \"Washington\", \"Adams\", \"Jefferson\", \"Jefferson…\n$ FirstName &lt;chr&gt; \"George\", \"George\", \"John\", \"Thomas\", \"Thomas\", \"James\", \"Ja…\n$ Party     &lt;fct&gt; none, none, Federalist, Democratic-Republican, Democratic-Re…"
  },
  {
    "objectID": "text_data/read_texts.html#raw-textfiles-.txt",
    "href": "text_data/read_texts.html#raw-textfiles-.txt",
    "title": "Reading Texts into R",
    "section": "Raw textfiles (.txt)",
    "text": "Raw textfiles (.txt)\nRaw text files is a common format for working with text data. The format does not have any overhead, which makes the files relatively small in size and they are flexible to work with. A common way to structure .txt files is to have each file be a document with a file name that indicates which document we are looking at. For example, we can store the titles in our pres_inaug dataset into text files with their id as file names (here only with the first 10 rows of the data):\n\nlapply(1:10, function(x) writeLines(pres_inaug$text[x], \n                                    paste0(\"../data/inaug_txt/\", \n                                           pres_inaug$doc_id[x],\n                                           \".txt\")))\n\nWe can now check if the files are stored as expected:\n\ninaug_files &lt;- list.files(\"../data/inaug_txt\", \n                             full.names = TRUE, \n                             pattern = \".txt\")\ninaug_files\n\n [1] \"../data/inaug_txt/1789-Washington.txt\"\n [2] \"../data/inaug_txt/1793-Washington.txt\"\n [3] \"../data/inaug_txt/1797-Adams.txt\"     \n [4] \"../data/inaug_txt/1801-Jefferson.txt\" \n [5] \"../data/inaug_txt/1805-Jefferson.txt\" \n [6] \"../data/inaug_txt/1809-Madison.txt\"   \n [7] \"../data/inaug_txt/1813-Madison.txt\"   \n [8] \"../data/inaug_txt/1817-Monroe.txt\"    \n [9] \"../data/inaug_txt/1821-Monroe.txt\"    \n[10] \"../data/inaug_txt/1825-Adams.txt\"     \n\n\nIf we want to read these files back into R, we can use the readLines() function:\n\ninaugs &lt;- lapply(inaug_files, readLines)\n\nclass(inaugs) # lapply always returns a list\n\n[1] \"list\"\n\nstr_sub(inaugs[[1]], 1, 65) # This is the first inaugural speech\n\n[1] \"Fellow-Citizens of the Senate and of the House of Representatives\"\n\n\nWe can now put the texts into a data frame by using unlist() on the inaugs object:\n\ninaug_txt &lt;- data.frame(id = inaug_files, \n                        text = unlist(inaugs))\n\n# Trying to (sort of) reengineer the original data\ninaug_txt %&gt;% \n1  mutate(id = str_extract(inaug_files, \"[0-9]+\\\\-[A-Za-z]+\"),\n         text = str_sub(text, 1, 50)) %&gt;% \n  select(id, text) %&gt;% \n  glimpse()\n\n\n1\n\nSee the section on Regular Expressions\n\n\n\n\nRows: 10\nColumns: 2\n$ id   &lt;chr&gt; \"1789-Washington\", \"1793-Washington\", \"1797-Adams\", \"1801-Jeffers…\n$ text &lt;chr&gt; \"Fellow-Citizens of the Senate and of the House of \", \"Fellow cit…\n\n\nWe can also work directly with the list by giving the elements within the list names:\n\nnames(inaugs) &lt;- str_extract(inaug_files, \"[0-9]+\\\\-[A-Za-z]+\")\nnames(inaugs)\n\n [1] \"1789-Washington\" \"1793-Washington\" \"1797-Adams\"      \"1801-Jefferson\" \n [5] \"1805-Jefferson\"  \"1809-Madison\"    \"1813-Madison\"    \"1817-Monroe\"    \n [9] \"1821-Monroe\"     \"1825-Adams\"     \n\n\n\n\ninaug_txt &lt;- data.frame(text = unlist(inaugs),\n                        id = names(inaugs))\n\nglimpse(inaug_txt)\n\nRows: 10\nColumns: 2\n$ text &lt;chr&gt; \"Fellow-Citizens of the Senate and of the House of Representative…\n$ id   &lt;chr&gt; \"1789-Washington\", \"1793-Washington\", \"1797-Adams\", \"1801-Jeffers…\n\n\nIt will often be a good idea to work with the data, especially in cases of large and many texts, in list format before converting to a data frame. List objects use slightly less resources in terms of RAM, but are also a bit more flexible to work with through parallelization functions such as mclapply().1. In our small example, the difference in object size is small, but still there:\n\nobject.size(inaug_txt) - object.size(inaugs)\n\n840 bytes"
  },
  {
    "objectID": "text_data/read_texts.html#textfiles-with-overhead",
    "href": "text_data/read_texts.html#textfiles-with-overhead",
    "title": "Reading Texts into R",
    "section": "Textfiles with overhead",
    "text": "Textfiles with overhead\nA .txt file is as it is; there are no hidden attributes within the file. That is not necessarily the case with other text file formats. A MS-Word (.docx) file, for instance, is really just a compressed archive of .html and .xml files that give pointers to how this file should appear in programs that can read such files. Let’s look at an example2:\n\nunzip(\"../data/ba_thesis.docx\", exdir = \"../data/wordfiles\")\n\nlist.files(\"../data/wordfiles/\")\n\n[1] \"_rels\"               \"[Content_Types].xml\" \"customXml\"          \n[4] \"docProps\"            \"word\"               \n\n\nThe consequence is that these files are a lot harder to read for R than .txt files, and we get weird outputs when we try to use readLines():\n\nreadLines(\"../data/ba_thesis.docx\", n = 2)\n\n[1] \"PK\\003\\004\\024\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n[2] \"M \\xdbK\\xa4Ğ\\xf7}flǳ\\xbcy4Mv\\017!jgKvU,X\\006V:\\xa5\\xed\\xb6d?\\xd6_\\xf2\\017,\\x8b(\\xac\\022\\x8d\\xb3P\\xb2=Dv\\xb3z\\xfbf\\xb9\\xde{\\x88\\031E\\xdbX\\xb2\\032\\xd1\\177\\xe4&lt;\\xca\\032\\x8c\\x88\\x85\\xf3`i\\xa4r\\xc1\\b\\xa4װ\\xe5^\\xc8\\xdfb\\v\\xfcz\\xb1xϥ\\xb3\\b\\026sL\\032l\\xb5\\xfc\\f\\x95\\xd85\\x98\\xdd&gt;\\xd2\\xe7\\x8e\\xc4\\xdb-\\xcb&gt;u\\xf3\\x92UɴI\\xf1\\xe9;\\037\\x8c\\b\\xd0\\xc4g!\\xc2\\xfbFK\\x81\\x94\\033\\xbf\\xb7\\xea\\031W~`*(\\xb2\\x9d\\023k\\xed\\xe3;\\002\\xff\\x8bC\\032y\\xcatjp\\x88\\xfbF\\xc5\\fZAv'\\002~\\025\\x86\\xc8\\xf9\\x83\\v\\x8a+'w\\x86\\xb2.\\xce\\xcb\\fp\\xba\\xaa\\xd2\\022\\xfa\\xf8\\xa4惓\\020#\\xad\\x92i\\x8a~\\xc4\\bm\\x8f\\xfcC\\034r\\027љ_\\xa6\\xe1\\032\\xc1\\xdc\\005\\xe7\\xe3\\xd5l\\x9c^4\\xe9A@\"\n\n\nIn other words, we need different methods for reading files with overhead. Below are some examples with .docx and .pdf, which are the most used types of files, but there are of course a plethora of other formats you might bump into at some point.\n\n.docx\nFortunately for us, R has a large community of package builders that usually have solved our problem for us. In this case, we use the officer package, which has read functions for MS-office files (word, excel, and powerpoint). We first have to read the document with the read_docx function, and then convert the resulting object to a data frame using the docx_summary() function.\n\nlibrary(officer)\n\nba_docx &lt;- read_docx(\"../data/ba_thesis.docx\")\n\nba_docx &lt;- docx_summary(ba_docx)\n\nba_docx$text[45:48]\n\n[1] \"Three hypotheses are derived from the question:\"                                            \n[2] \"H0: There is no relationship between secrecy jurisdiction status and quality of governance.\"\n[3] \"H1a: Secrecy jurisdictions are jurisdictions with high quality of governance.\"              \n[4] \"H1b: Secrecy jurisdictions are jurisdictions with low quality of governance.\"               \n\n\nIt is, of course, always useful to inspect the data thoroughly before continuing with our analyses. For instance, there could be mistakes in how the data was read because of encoding issues.\n\n\n.pdf\nThe same goes for .pdf files, although the package is now pdftools and the function is pdf_text():\n\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nba_pdf &lt;- pdf_text(\"../data/ba_thesis.pdf\")\n\nba_pdf &lt;- ba_pdf[4] %&gt;%\n  strsplit(\"\\\\n\") %&gt;%\n  unlist() %&gt;% \n  .[which(. != \"\")]\n\nba_pdf[11:14]\n\n[1] \"Three hypotheses are derived from the question:\"                                            \n[2] \"H0: There is no relationship between secrecy jurisdiction status and quality of governance.\"\n[3] \"H1a: Secrecy jurisdictions are jurisdictions with high quality of governance.\"              \n[4] \"H1b: Secrecy jurisdictions are jurisdictions with low quality of governance.\"               \n\n\nAn honorable mention to historical documents that have been scanned in .pdf format: these are often just pictures that needs to be ran through Optical Character Recognition (OCR) before we can utilize the text."
  },
  {
    "objectID": "text_data/read_texts.html#footnotes",
    "href": "text_data/read_texts.html#footnotes",
    "title": "Reading Texts into R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWindows users will not me able to use this function because it relies on the forking method↩︎\nThis is Martin Søyland’s BA-thesis.↩︎"
  },
  {
    "objectID": "Visualization/data_viz_model_predictions.html",
    "href": "Visualization/data_viz_model_predictions.html",
    "title": "Visualizing quantities of interest based on regression models",
    "section": "",
    "text": "We have seen how we can use coefficient plots to present the coefficients and associated confidence intervals from our regression models. Such coefficient plots are useful for presenting models and for comparing coefficients and confidence intervals across specifications. Plotting conditional coefficients is often crucial to understand models with interaction terms.\nAn important limitation is that coefficients from non-linear models (such as logistic and probit regressions) are often hard to interpret and are typically not on the same scale as the quantities of interest that we are actually interested. For instance, when estimating logistic regression models we are typically more interested in how our independent variables correlate with the probability of our outcome than in how our independent variables influence the logit of the outcome. Similar considerations will apply to various other generalized linear models.\nKing, Tomz, and Wittenberg (2000) therefore recommend that we calculate quantities of interest (such as predicted probabilities) based on our models and visualize these quantitative of interest. Typically, we can only calculate these quantities of interest when fixing the values on our covariates, requiring us to create interesting scenarios for illustrating results. Using statistical simulation techniques we can communicate the uncertainty surrounding our results.\nLet’s consider some examples.\nConsider the logistic regression model estimated by Binder (2018) (which we replicated here):\nlibrary(dplyr)\nlibrary(haven)\ndodgingrules &lt;- read_dta(\"../data/dodgingrules.dta\")\n\nbinder_model &lt;- glm(filisave ~ running18 + gop + wings +  leader +\n                      stdrank + servedmin +  running18:gop, \n                    data = dodgingrules, \n                    family = binomial(link = \"logit\"))\nWe have already considered how we can present the coefficients from this model. But how can we how much greater the probability is republicans than ran for reelection in 2018 compared to republicans that didn’t run for reelection in 2018?\nConsider similarly the probit model estimated by Carrubba, Gabel, and Hankla (2008) (which we replicated here):\nload(\"../data/CarrubbaGabelHankla.RData\")\necj_data &lt;- table \necj_probit &lt;- glm(ECJPlAgree ~ \n              normnetwobs *  govislit +\n              percham + \n              CommIsPl + \n              CommIsDef  + \n              CommObsPl + \n              CommObsDef, \n            family = binomial(link = \"probit\"), \n            data = ecj_data )\nHow much does the predicted probability of pro-plaintiff ruling increase with increases in the normnetwobs variable?"
  },
  {
    "objectID": "Visualization/data_viz_model_predictions.html#logistic-regression-with-binary-independent-variable-of-interest",
    "href": "Visualization/data_viz_model_predictions.html#logistic-regression-with-binary-independent-variable-of-interest",
    "title": "Visualizing quantities of interest based on regression models",
    "section": "Logistic regression with binary independent variable of interest",
    "text": "Logistic regression with binary independent variable of interest\nWe will start with our Binder (2018) example. We are interested in fixing the gop variable (identifying Republicans) to 1 and vary the running18 variable. This variable only takes the values 0 and 1, so we will vary it between those values. The interaction term is just gop multiplied with running18 and will accordingly also vary between 0 and 1. We also need to fix the variables for other covariates. Here we will just the median() value for all other covariates but many different choices could be reasonable and the choices you make here will influence your predicted values.\nWe can use cbind() to create our two scenarios:\n\n1coefficients(binder_model)\n2binder_scenarios &lt;- cbind(\n3  1,\n4  c(0,1),\n5  1,\n6  median(dodgingrules$wings, na.rm = TRUE),\n  median(dodgingrules$leader, na.rm = TRUE),\n  median(dodgingrules$servedmin, na.rm = TRUE),\n    median(dodgingrules$stdrank, na.rm = TRUE),\n7  c(0,1))\n\n\n1\n\nThe order will matter, so we print out the coefficients to see the order.\n\n2\n\nUsing cbind() will bind the values together in a matrix\n\n3\n\nThe intercept should be multiplied with 1, so we add we start with a 1\n\n4\n\nrunning18 should vary between 0 and 1, so we add a vector c(0,1).\n\n5\n\nWe want gop fixed at 1, so add a 1 here.\n\n6\n\nWe set wings, leader,servedmin, and stdrank to their median() values.\n\n7\n\nThe interaction term should also vary between 0 and 1.\n\n\n\n\n  (Intercept)     running18           gop         wings        leader \n    0.9956295     1.4104023     0.1539010    -4.8437615    -1.6140983 \n      stdrank     servedmin running18:gop \n    0.2408884     1.2846637    -2.4160361 \n\n\n\n1print(binder_scenarios)\n\n2binder_linear_predictors &lt;- coefficients(binder_model) %*% t(binder_scenarios)\n3print(binder_linear_predictors)\n4inverse_logit &lt;- function(x){exp(x)/(1+exp(x))}\n\n5binder_predicted_probabilities &lt;- inverse_logit(binder_linear_predictors)\nprint(binder_predicted_probabilities)\n\n\n1\n\nWe now produced a matrix which looks like this. Each row is a scenario. To get the linear predictor for each row. we need to multiply the matrix with the vector of coefficients. This involves using matrix multiplication, which you may not be intimately familiar with, but R will take care of all that for you as long as you remember to use %*% as the matrix multiplication operator and t() means taking the transpose of the matrix flipping it on the side so that the rows becomes the columns and vice versa.\n\n2\n\nThe linear predictors are simply the coefficients multiplied by the transpose of our scenarios.\n\n3\n\nPrinting them out we get two linear for signing the letter. The first (1.7087273) is the linear predictor of signing the letter for Republicans that didn’t run for reelection in 2018 keeping all other variables at their median values and the second (0.7030935) is the linear predictor of signing the letter for Republicans that did run for reelection in 2018 keeping all other variables at their median values.\n\n4\n\nWe define function for converting the linear predictors from logistic regressions to predicted probabilities (defining a function is useful as we will do this a lot).\n\n5\n\nPlugging in the linear probabilities to our function we get the predicted probabilities for our two scenarios. They are 0.8466711 for Republicans that didn’t run for reelection in 2018 keeping all other variables at their median values and 0.8466711 for Republicans that did run for reelection in 2018 keeping all other variables at their median values.\n\n\n\n\n     [,1] [,2] [,3]  [,4] [,5] [,6]     [,7] [,8]\n[1,]    1    0    1 0.394    0    1 1.733333    0\n[2,]    1    1    1 0.394    0    1 1.733333    1\n         [,1]      [,2]\n[1,] 1.708727 0.7030935\n          [,1]      [,2]\n[1,] 0.8466711 0.6688733\n\n\nThe above process could be simplified by simply using the predict() function:\n\n1binder_scenarios_df &lt;- as.data.frame(binder_scenarios)\n2colnames(binder_scenarios_df) &lt;- names(coefficients(binder_model))\n3predict(binder_model, newdata = binder_scenarios_df, type = \"response\")\n\n\n1\n\npredict() needs a data.frame rather than a matrix\n\n2\n\npredict() will look for the variables in the data.frame so we need to give the variables names that match the coefficient names\n\n3\n\nSetting the newdata to our scenarios and type to “response”, we get predicted probabilities for our scenarios.\n\n\n\n\n        1         2 \n0.8466711 0.6688733 \n\n\nIf you ever only the predicted probabilities, you should definitely use predict() rather than writing out the code for the calculations. However, writing out the code for the calculations will be necessary going forward as we also incorporate uncertainty using MASS to simulate coefficients."
  },
  {
    "objectID": "Visualization/data_viz_model_predictions.html#probit-regression-with-continuous-independent-variable-of-interest",
    "href": "Visualization/data_viz_model_predictions.html#probit-regression-with-continuous-independent-variable-of-interest",
    "title": "Visualizing quantities of interest based on regression models",
    "section": "Probit regression with continuous independent variable of interest",
    "text": "Probit regression with continuous independent variable of interest\nThe Carrubba, Gabel, and Hankla (2008) example is different from the Binder (2018) in two respects:\n\nWe are interested in a continuous independent variable (normnetwobs), so will need multiple scenarios with different values along the range of this variable.\nWe are dealing with a probit model rather than a logistic regression, so the formula for converting linear predictors into predicted probabilities will different.\n\n\n1ecj_scenarios &lt;- cbind(\n2  1,\n3  seq(min(ecj_data$normnetwobs, na.rm = TRUE),\n      max(ecj_data$normnetwobs, na.rm = TRUE),\n      length.out = 10),\n4  1,\n5  median(ecj_data$percham, na.rm = TRUE),\n  median(ecj_data$CommIsPl, na.rm = TRUE),\n  median(ecj_data$CommIsDef, na.rm = TRUE),\n    median(ecj_data$CommObsPl, na.rm = TRUE),\n      median(ecj_data$CommObsDef, na.rm = TRUE),\n    seq(min(ecj_data$normnetwobs,na.rm = TRUE),\n      max(ecj_data$normnetwobs, na.rm = TRUE),\n6      length.out = 10))\n\n7print(ecj_scenarios)\n\n\n1\n\nAgain, we use cbind() to create a matrix with our scenarios.\n\n2\n\nWe add a 1 for the intercept.\n\n3\n\nUsing seq() we can create a sequence of 10 values for the normnetwobs variable ranging from the min() to the max() value.\n\n4\n\nWe set govlit to 1 (which means that our calculations will reflect a scenario in which a government is a litigant).\n\n5\n\nWe set all the other variables to their median values.\n\n6\n\nAgain we have an interaction term, which will be the value on normnetwobs multiplied by the value on govlit which we fixed to 1. So in other words, just the values on normnetwobs repeated.\n\n7\n\nWe now have 10 different rows in our matrix and the only thing that varies between them is the value on normnetwobs.\n\n\n\n\n      [,1]        [,2] [,3]      [,4] [,5] [,6] [,7] [,8]        [,9]\n [1,]    1 -0.86842108    1 0.3846154    0    0    0    0 -0.86842108\n [2,]    1 -0.71345031    1 0.3846154    0    0    0    0 -0.71345031\n [3,]    1 -0.55847955    1 0.3846154    0    0    0    0 -0.55847955\n [4,]    1 -0.40350878    1 0.3846154    0    0    0    0 -0.40350878\n [5,]    1 -0.24853802    1 0.3846154    0    0    0    0 -0.24853802\n [6,]    1 -0.09356725    1 0.3846154    0    0    0    0 -0.09356725\n [7,]    1  0.06140351    1 0.3846154    0    0    0    0  0.06140351\n [8,]    1  0.21637428    1 0.3846154    0    0    0    0  0.21637428\n [9,]    1  0.37134504    1 0.3846154    0    0    0    0  0.37134504\n[10,]    1  0.52631581    1 0.3846154    0    0    0    0  0.52631581\n\n\n\n1ecj_linear_predictors &lt;- coefficients(ecj_probit) %*% t(ecj_scenarios)\n2ecj_predicted_probabilities &lt;- pnorm(ecj_linear_predictors )\n\n\n1\n\nLike before we multiply the coefficients with the transponse of the matrix with the scenarios.\n\n2\n\nSince we have a probit model, we can convert he linear predictors to probabilities using pnorm().\n\n\n\n\nWe can plot the predicted probabilities against the different values on normnetwobs to see how the predicted probability of a pro-plaintiff ruling varies depending on the net support from the member states.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n1data.frame(probabilities = as.vector(ecj_predicted_probabilities),\n           normnetwobs = as.vector(ecj_scenarios[,2])) %&gt;%\n  ggplot(aes(x = normnetwobs, \n             y = probabilities)) +\n  geom_line()+\n  theme_classic()+\n  labs(y = \"Predicted probability of pro-plaintiff ruling\", \n       x = \"Net weighted observations for plaintiff\",\n       caption = \"A government is the litigant and other variables are kept at their median values\")\n\n\n1\n\nggplot2 needs a data.frame. Both ecj_predicted_probabilities and the second column of ecj_scenarios which contains the sequence of normnetwobs are matrices, so we need to force them to become vectors as we add them to a data.frame\n\n\n\n\n\n\n\nFigure 1: Predicted probabilities of pro-plaintiff ruling at different levels of support for the plaintiff from EU member states.\n\n\n\n\nWhile Figure 1 is nice, it lacks uncertainty estimates! Communicating the estimated uncertainty surrounding our predictions is a crucial part of scientific communication."
  },
  {
    "objectID": "Visualization/data_viz_model_predictions.html#logistic-regression-with-binary-independent-variable-of-interest-1",
    "href": "Visualization/data_viz_model_predictions.html#logistic-regression-with-binary-independent-variable-of-interest-1",
    "title": "Visualizing quantities of interest based on regression models",
    "section": "Logistic regression with binary independent variable of interest",
    "text": "Logistic regression with binary independent variable of interest\nAgain, we will start with our Binder (2018) example, which we now expand to include simulated confidence intervals.\n\nlibrary(MASS) \nlibrary(sandwich) \n\n1set.seed(4761)\n2binder_sim_betas &lt;- mvrnorm(n = 1000,\n3                            mu = coefficients(binder_model),\n4                            Sigma = vcovHAC(binder_model, cluster = dodgingrules$stateid))\n\n\n5binder_sim_linear_predictors &lt;- binder_sim_betas %*% t(binder_scenarios)\n6binder_sim_predicted_probabilities &lt;- inverse_logit(binder_sim_linear_predictors)\n\n7dim(binder_sim_predicted_probabilities)\nhead(binder_sim_predicted_probabilities)\n\n\n1\n\nWe will draw random numbers. We can ensure that the results reproduce by using set.seed(), which we just supply an arbitrary number.\n\n2\n\nWe simulate coefficients using mvrnorm(). n = 1000 means that we get 1000 different draws from the distribution (i.e. 1000 different sets of coefficients).\n\n3\n\nThe mean (mu) values should be our estimated coefficients\n\n4\n\nThe variance-covariance (Sigma) should be given by our variance-covariance matrix. Here we use use the clustered variance covariance matrix using vcovHAC() from sandwich.\n\n5\n\nWe calculated linear predictors like before, but now for all the different draws from the distribution\n\n6\n\nWe calculate predicted probabilities using our inverse_logit() function which we defined above.\n\n7\n\nPrinting out dimensions and first rows of binder_sim_predicted_probabilities, we see that we now have matrix with 1000 rows and 2 columns. The rows are the different draws and the columns are the two different scenarios.\n\n\n\n\n[1] 1000    2\n          [,1]       [,2]\n[1,] 0.7691250 0.89562705\n[2,] 0.8274384 0.57637870\n[3,] 0.8459521 0.09446499\n[4,] 0.8890986 0.48234520\n[5,] 0.7785926 0.25106514\n[6,] 0.7577103 0.29323668\n\n\nWe need to summarize this information to make a visualization. Using apply() and quantile() we can get the median predictions and confidence intervals. If we want a 95% confidence intervals, we need to discard 5/2 = 2.5 per cent of draws on each side of the distribution (so the borders of our confidence interval be at the 2.5% and the 97.5% lowest/highest values in the distribution, which correspond to the probabilities 0.025 and 0.975).\n\n1binder_plot_values &lt;- apply(binder_sim_predicted_probabilities,\n2                            MARGIN = 2,\n3                            FUN = quantile,\n4                            probs = c(.025,.5,.975))\n\n\n1\n\nWe want to apply a function across our matrix with predicted probabilities.\n\n2\n\nWe want to apply the function across the (two columns).\n\n3\n\nThe function to use is quantile()\n\n4\n\nWe can add additional arguments to the function. We want the 0.025, the 0.5, and the 0.975 quantiles of the distribution.\n\n\n\n\nThe resulting matrix looks like this:\n\nprint(binder_plot_values)\n\n           [,1]      [,2]\n2.5%  0.3819593 0.1557909\n50%   0.8474618 0.6536688\n97.5% 0.9810671 0.9435478\n\n\nTo use these values in ggplot2, we need to take the transpose and convert the matrix to a data.frame. It will also be helpful to add labels explaining what the different scenarios are:\n\nbinder_plot_values &lt;- binder_plot_values %&gt;% \n  t() %&gt;% \n  as.data.frame() %&gt;% \n  mutate(scenario = c(\"Republicans not running for reelection in 2018\", \n                      \"Republicans running for reelection in 2018\"))\n\nWe are now ready to make a graph!\n\nggplot(binder_plot_values, \n       aes(x = scenario, \n1           y = `50%`,\n           ymin = `2.5%`,\n           ymax = `97.5%`)) +\n2  geom_errorbar(width = 0)+\n  geom_point()+\n  ylim(0,1)+\n  labs(y = \"Predicted probability of signing\\nletter committing to save the filibuster\", \n       x = \"\", \n       caption = \"Other variables fixed at their median values\")+\n  theme_classic()\n\n\n1\n\nRecall that R doesn’t like variable names that start with number and include symbols such as “%”. Those are, however, the column names we got when transposing the matrix produced by apply()! We can circumvent the problem by wrapping the variable names with the `` sign. Alternatively we could have renamed the columns in the data.frame before supplying it to ggplot().\n\n2\n\nWe use geom_errobar() to create confidence intervals. The boundaries are defined by ymin and ymax.\n\n\n\n\n\n\n\nFigure 2: Predicted probabilities for signing the letter for Republicans running for election in 2018 and for Republicans not running for reelection in 2018. Error bars indicate 95% confidence intervals.\n\n\n\n\nFigure 2 adds important information compared to the coefficient plot or regression table. It shows that while the predicted probability of signing the letter is indeed lower for republicans running for election than for republicans not running, there is massive uncertainty surrounding both predictions."
  },
  {
    "objectID": "Visualization/data_viz_model_predictions.html#probit-regression-with-continuous-independent-variable-of-interest-1",
    "href": "Visualization/data_viz_model_predictions.html#probit-regression-with-continuous-independent-variable-of-interest-1",
    "title": "Visualizing quantities of interest based on regression models",
    "section": "Probit regression with continuous independent variable of interest",
    "text": "Probit regression with continuous independent variable of interest\nWe can do the same for the Carrubba, Gabel, and Hankla (2008). Since our independent variable of interest in this example is a continuous, we would want a curve with a “ribbon” indicating the confidence interval rather than points with error bars:\n\n1set.seed(315)\necj_sim_betas &lt;- mvrnorm(n = 1000, \n                            mu = coefficients(ecj_probit), \n                            Sigma = vcovHAC(ecj_probit, cluster = ecj_data$caseid)) \necj_sim_linear_predictors &lt;- ecj_sim_betas %*% t(ecj_scenarios) \n2ecj_sim_predicted_probabilities &lt;- pnorm(ecj_sim_linear_predictors)\necj_plot_values &lt;- apply(ecj_sim_predicted_probabilities, \n                            MARGIN = 2, \n                            FUN = quantile, \n                            probs = c(.025,.5,.975))\necj_plot_values&lt;- ecj_plot_values%&gt;% \n  t() %&gt;% \n  as.data.frame() %&gt;% \n  mutate(normnetwobs = as.vector(ecj_scenarios[,2]))\n\nggplot(ecj_plot_values, \n  aes(x = normnetwobs, \n           y = `50%`, \n           ymin = `2.5%`, \n           ymax = `97.5%`)) +\n3  geom_ribbon(alpha = 0.2)+\n  geom_line(size = 1)+\n  theme_classic()+\n    labs(y = \"Predicted probability of pro-plaintiff ruling\", \n      x = \"Net weighted observations for plaintiff\",\n       caption = \"A government is the litigant and other variables are kept at their median values\")\n\n\n1\n\nWe set a different seed just to mix things up.\n\n2\n\nWe remember to use the appropriate function for calculating predicted probabilities based on what kind of model we have. For a probit, you use pnorm().\n\n3\n\ngeom_ribbon() can be used to add a confidence interval around a line. The alpha determines how “transparent” the confidence interval will be.\n\n\n\n\n\n\n\nFigure 3: Predicted probabilities of pro-plaintiff ruling at different levels of support for the plaintiff from EU member states. Shaded areas indicate the 95% confidence intervals"
  },
  {
    "objectID": "Visualization/data_viz_in_ggplot.html",
    "href": "Visualization/data_viz_in_ggplot.html",
    "title": "Data Visualization in ggplot2",
    "section": "",
    "text": "Data visualization is a crucial part of quantitative political science:\nWe will use ggplot2 to make our visualizations in R. The ggplot2 package implements a “layered grammar of graphics” (Wickham 2010), providing a coherent and flexible framework for building and customizing our visualizations. Thus, instead of relying on a few standardized types of graphs as we would in many other graphics packages, we can tailor our visualizations to our specific research question and to our aesthetic preferences.\nAs you will see, ggplot2 is built around the idea of making visualizations layer by layer. This makes it easy to iteratively add layers to customize your visualization making it look exactly like you want it to."
  },
  {
    "objectID": "Visualization/data_viz_in_ggplot.html#the-three-components-of-any-graph",
    "href": "Visualization/data_viz_in_ggplot.html#the-three-components-of-any-graph",
    "title": "Data Visualization in ggplot2",
    "section": "The three components of any graph:",
    "text": "The three components of any graph:\nIn ggplot2, all visualizations will be based on the following three components:\n\nA (tidy) dataset.\nA set of aesthetic mappings connecting variables in the dataset to the visual properties of the graph. These will often be coordinates, but they can also be things that colors and sizes.\nOne or more layers describing how observations from the dataset are displayed in the visualization. These layers are usually geometric objects (or geoms in the ggplot2 jargon) such as lines, points, or polygons (Wickham, Navarro, and Pedersen, n.d.).\n\nYou will build your visualization layer by layer. You will start by supplying the data and declaring the aesthetic mappings, before adding geometric objects and other layers to your graphs using + between each layer.\nLet’s consider an example. We’ll reload the Varieties of Democracy data with levels of democracy and estimated gross domestic product per capita measured in 1848, which we introduced here. The data are taken from historical V-Dem Knutsen et al. (2019)], but the GDP/capita and population estimates that we will look at are from Fariss et al. (2022).\n\nlibrary(haven)\nlibrary(dplyr)\ncountries_1848 &lt;- read_dta(\"../data/wealth_and_democracy/wealth_and_democracy_in_1848.dta\")\ncountries_1848 &lt;- countries_1848 %&gt;% \n  mutate(log_e_gdppc = log(e_gdppc))\nhead(countries_1848)\n\n# A tibble: 6 × 6\n  country_name  country_id  year v2x_polyarchy e_gdppc log_e_gdppc\n  &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n1 Mexico                 3  1848         0.23    1.07       0.0714\n2 Sweden                 5  1848         0.309   1.76       0.563 \n3 Switzerland            6  1848         0.329   2.63       0.969 \n4 Japan                  9  1848         0.028   1.38       0.321 \n5 Burma/Myanmar         10  1848         0.032   0.829     -0.188 \n6 Russia                11  1848         0.016   1.56       0.445 \n\n\nWe will visualize this data. The first step is to feed the data to the `ggplot() function, which will open an empty canvas:\n\nlibrary(ggplot2) \nggplot(data = countries_1848)\n\n\n\n\nUsing the %&gt;% operator, we can also write the above code like this:\n\ncountries_1848 %&gt;% \n  ggplot()\n\n\n\n\nThe second step is to add some aesthetic mappings. If we want to visualize the relationship between v2x_polyarchy and log_e_gdppc in a scatter plot, we will need to map the variables to the x and y axis in a coordinate system. We can do so using aes(), which should be supplied to the mapping argument of ggplot(). Adding the mapping will add a coordinate system with an x-axis and a y-axis to our canvas:\n\nggplot(data = countries_1848,\n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))\n\n\n\n\nThe third step is to add a layer with a geometric object, using one of the geom_-functions. For a scatter plot, the obvious choice is to add points, which we can do using geom_point():\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point()"
  },
  {
    "objectID": "Visualization/data_viz_in_ggplot.html#adding-additional-geometic-objects",
    "href": "Visualization/data_viz_in_ggplot.html#adding-additional-geometic-objects",
    "title": "Data Visualization in ggplot2",
    "section": "Adding additional geometic objects",
    "text": "Adding additional geometic objects\nPoints are not the only reasonable option. We could instead have used geom_text() to display the name of each country in the graph. If so, we will need an additional aesthetic mapping, connecting labels to be displayed in the graph with a relevant variable in our dataset:\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy, \n                     label = country_name))+\n  geom_text()\n\n\n\n\nIt perfectly fine to have multiple geoms in the same visualization, for instance:\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy, \n                     label = country_name))+\n  geom_text(nudge_y = 0.01)+\n  geom_point()\n\n\n\n\nNotice that we add the different geoms layer by layer using + between each layer allowing us to iteratively customize our graph.\nNotice also that some geom_ functions may have additional arguments that can help you customize your graph. For the geom_text() we used the nudge_y argument to move each label slightly to the top of each point in the graph. A key thing to remember is that if arguments are not used to map variables from the data to aesthetic attributes, these arguments should not be supplied to aes() but just to the relevant geom_ function."
  },
  {
    "objectID": "Visualization/data_viz_in_ggplot.html#statistical-summaries",
    "href": "Visualization/data_viz_in_ggplot.html#statistical-summaries",
    "title": "Data Visualization in ggplot2",
    "section": "Statistical summaries",
    "text": "Statistical summaries\nSome geoms will involve summarizing or transforming the underlying data. For instance, we can add a linear regression line to the scatter plot.\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy, \n                     label = country_name))+\n  geom_text(nudge_y = 0.01)+\n  geom_point()+\n  geom_smooth(method = \"lm\")\n\n\n\n\nOr we can add a local regression (or LOESS) smooth, allowing us to accommodate potential non-linear relationships in the scatter plot.\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy, \n                     label = country_name))+\n  geom_text(nudge_y = 0.01)+\n  geom_point()+\n  geom_smooth(method = \"loess\")"
  },
  {
    "objectID": "Visualization/data_viz_in_ggplot.html#different-aesthetic-mappings",
    "href": "Visualization/data_viz_in_ggplot.html#different-aesthetic-mappings",
    "title": "Data Visualization in ggplot2",
    "section": "Different aesthetic mappings",
    "text": "Different aesthetic mappings\nWe currently have two variables in our visualization, one mapped to the x-axis and one mapped to the y-axis. What if we want to include more variables in the visualization?\nWe can add other variables to other aesthetic attributes of the visualization, like different colors, different shapes, different line types, different sizes, etc. Like for x and y, we will use aes() to map variables to these various aesthetic attributes.\nTo illustrate, we will add some complexity by adding some more variables to our dataset.1\n\n\nCode\nlibrary(vdemdata)\ncountries_1848 &lt;- vdem %&gt;% \n  filter(year == 1848) %&gt;% \n  select(country_name, country_id, year, v2x_polyarchy, e_gdppc, e_pop, e_regionpol) %&gt;% \n  mutate(log_e_gdppc = log(e_gdppc),\n         region = case_when(e_regionpol == 1 ~ \"Eastern Europe\", \n                            e_regionpol == 2 ~ \"Latin America\", \n                            e_regionpol == 3 ~ \"North Africa and Middle East\", \n                              e_regionpol == 4 ~ \"Sub–Saharan Africa\", \n                              e_regionpol == 5 ~ \"Western Europe and North America\", \n                            e_regionpol &gt; 5 ~ \"Eastern and Southern Asia\")) %&gt;% \n  select(-e_regionpol)\n\n\nThe top rows of the expanded dataset look like this:\n\nhead(countries_1848)\n\n   country_name country_id year v2x_polyarchy e_gdppc    e_pop log_e_gdppc\n1        Mexico          3 1848         0.230   1.074  823.762   0.0713900\n2        Sweden          5 1848         0.309   1.756  360.610   0.5630385\n3   Switzerland          6 1848         0.329   2.634  249.157   0.9685036\n4         Japan          9 1848         0.028   1.379 3387.306   0.3213586\n5 Burma/Myanmar         10 1848         0.032   0.829  421.038  -0.1875351\n6        Russia         11 1848         0.016   1.560 7320.535   0.4446858\n                            region\n1                    Latin America\n2 Western Europe and North America\n3 Western Europe and North America\n4        Eastern and Southern Asia\n5        Eastern and Southern Asia\n6                   Eastern Europe\n\n\nWe’ve added a variable measuring the population of each state (e_pop) and a variable measuring their political-geographic region (region).\nAs a first extension, let’s use color to distinguish between states from different geographic regions:\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy, \n                     label = country_name, \n                     color = region))+\n  geom_text(nudge_y = 0.01)+\n  geom_point()+\n  geom_smooth(method = \"loess\")\n\n\n\n\nAdding color = region changes the color of the points, lines, and text, and we now automatically get a legend explaining how color is mapped to the variable region.\nBecause color is mapped to a variable in the dataset and color is also an aesthetic attribute of geom_smooth(), we get different smoothed lines for each region. Maybe this is a bit too much and we just want the points and to vary in color. If so, we can that specify this mapping only for geom_point():\n\nggplot(data = countries_1848, \n1       mapping = aes(x = log_e_gdppc,\n                     y = v2x_polyarchy,\n                     label = country_name))+\n  geom_text(nudge_y = 0.01)+\n2  geom_point(aes(color = region))+\n  geom_smooth(method = \"loess\")\n\n\n1\n\nSupplying the general mapping to be applied across all geom_s like before.\n\n2\n\nHere we supply an an additional mapping that will apply only to geom_point()Adding aes() arguments to a specific geom_ means that they will applied only to that geometric object. The different aes() arguments that are not specified will still be added from the main mapping.\n\n\n\n\n\n\n\nHow can incorporate population size in the figure? Well, we could let the size of the points vary by population size (and lets’ also get rid of the geom_text() which adds a bit too much noise):\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")\n\n\n\n\nIf we are letting the size of the points vary by population size, we might also let their influence on the local regression vary depending on population size, which we may do using, weights:\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\", aes(weight = e_pop))\n\n\n\n\nThere are many different aesthetic attributes that we can map variables. Here are some different attributes:\n\nThe x and y coordinates\nThe xmin,xmax, ymin, and ymax coordinates to display upper and lower bounds for confidence intervals\nThe color of points, lines, etc.\nThe fill which is the color filling polygons, segments exactly\nThe alpha determining how transparent a geom is.\nThe linetype of lines\nThe shape of points\nThe size of points, text, etc.\nThe label for included text."
  },
  {
    "objectID": "Visualization/data_viz_in_ggplot.html#faceting",
    "href": "Visualization/data_viz_in_ggplot.html#faceting",
    "title": "Data Visualization in ggplot2",
    "section": "Faceting",
    "text": "Faceting\nImagine that we didn’t just want to create our visualization for the year 1848, but that wanted to illustrate the relationship also for the years 1789, 1815, and 1914. If so, might want to use faceting to create the same plot for each value the variable year in dataset including observations from all of these years. If so, we can use facet_wrap()\n\n\nCode\ncountries_selected_years &lt;- vdem %&gt;% \n  filter(year == 1789 | year == 1815| year == 1848 | year == 1914) %&gt;% \n  select(country_name, country_id, year, v2x_polyarchy, e_gdppc, e_pop, e_regionpol) %&gt;% \n  mutate(log_e_gdppc = log(e_gdppc),\n         region = case_when(e_regionpol == 1 ~ \"Eastern Europe\", \n                            e_regionpol == 2 ~ \"Latin America\", \n                            e_regionpol == 3 ~ \"North Africa and Middle East\", \n                              e_regionpol == 4 ~ \"Sub–Saharan Africa\", \n                              e_regionpol == 5 ~ \"Western Europe and North America\", \n                            e_regionpol &gt; 5 ~ \"Eastern and Southern Asia\")) %&gt;% \n  select(-e_regionpol)\n\n\n\nggplot(data = countries_selected_years, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\", aes(weight = e_pop))+\n1  facet_wrap(vars(year))\n\n\n1\n\nUsing vars we can select the the variable(s) for which there should be different facets for each value.\n\n\n\n\n\n\n\nWe can also change how many columns and rows there should be as the facets are arranged:\n\nggplot(data = countries_selected_years, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\", aes(weight = e_pop))+\n  facet_wrap(vars(year), nrow = 1)"
  },
  {
    "objectID": "Visualization/data_viz_in_ggplot.html#improving-the-aesthetics-and-easing-interpretation",
    "href": "Visualization/data_viz_in_ggplot.html#improving-the-aesthetics-and-easing-interpretation",
    "title": "Data Visualization in ggplot2",
    "section": "Improving the aesthetics and easing interpretation",
    "text": "Improving the aesthetics and easing interpretation\nWe now have some different visualizations, but they are admittedly not very pretty and readers unfamiliar with the variable names from the Varieties of Democracy dataset might have a hard to time interpreting what we have actually visualized.\nFortunately, we can improve both the aesthetic qualities and the interpretability of our graphs by adding additional layers:\n\nChanging the limits and labels of the axes.\nWe should always give our axes informative names. We may do so using ylab() and xlab():\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from the Varieties of Democracy project\")+\n  xlab(\"log(gross domestic product per capita)\")\n\n\n\n\nIt sometimes also makes sense to change limits of axis. For instance, the polyarchy index ranges from 0 to 1, so if we wanted to emphasize how none of the included states were very democratic by today’s standards, we could adjust ylim() to include the complete theoretical range.\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from the Varieties of Democracy project\")+\n  xlab(\"log(gross domestic product per capita)\")+\n  ylim(0,1)\n\n\n\n\nBecause the confidence interval for our local regression includes values just below 0, the confidence interval is now omitted for some observations. We can avoid this by expanding the ylim():\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from the Varieties of Democracy project\")+\n  xlab(\"log(gross domestic product per capita)\")+\n  ylim(-0.15,1)\n\n\n\n\n\n\nAdjusting and labeling the various scales\nWe also need more informative names for our legends. Perhaps, we are also unhappy with the colors used for the points or how many different sizes are used to distinguish countries by their population size. Perhaps we want more or less breaks on the x-axis? All these things can be changed using the various scale_ functions corresponding to the aesthetic attribute we are trying to adjust.\n\nColor\nLet’s start with color.\nIn the figures we have created, we have used color to distinguish between different discrete (or categorical) values. We can use one of the scale_color_*() functions to adjust this type of scale. We can change the title of this scale used in the legend using the name argument to scale_color_discrete()\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from the Varieties of Democracy project\")+\n  xlab(\"log(gross domestic product per capita)\")+\n  ylim(-0.15,1)+\n  scale_color_discrete(name = \"Political-geographic\\nregion\")\n\n\n\n\nIf we also want to change the color palette that is used, we can instead using scale_color_brewer() and select one of the many palettes that are available:\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from the Varieties of Democracy project\")+\n  xlab(\"log(gross domestic product per capita)\")+\n  ylim(-0.15,1)+\n  scale_colour_brewer(name = \"Political-geographic\\nregion\", palette = \"Spectral\")\n\n\n\n\nAlternatively, you may use scale_color_manual() to manually set the different colors:\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from the Varieties of Democracy project\")+\n  xlab(\"log(gross domestic product per capita)\")+\n  ylim(-0.15,1)+\n  scale_color_manual(name = \"Political-geographic\\nregion\", \n                       values = c(\"tomato2\", \"cornflowerblue\", \"darkgreen\", \n                       \"pink\", \"orange\", \"brown\"))\n\n\n\n\nLisa Charlotte Muth has excellent suggestions for how to choose and use different colors to communicate effectively.\n\n\nSize\nWe can also change the size legend title. This attribute scales a continuous variable and the attribute is size rather than color, so we will use scale_size_continuous()\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from the Varieties of Democracy project\")+\n  xlab(\"log(gross domestic product per capita)\")+\n  ylim(-0.15,1)+\n  scale_color_discrete(name = \"Political-geographic\\nregion\")+\n  scale_size_continuous(name = \"Population size (in 10000s)\")\n\n\n\n\nMaybe we want more different sizes? You can change the breaks:\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from the Varieties of Democracy project\")+\n  xlab(\"log(gross domestic product per capita)\")+\n  ylim(-0.15,1)+\n  scale_color_discrete(name = \"Political-geographic\\nregion\")+\n  scale_size_continuous(name = \"Population size (in 10000s)\", breaks = seq(0,40000, by = 5000))\n\n\n\n\n\n\nThe x and y axes.\nThe axes of the coordinate system are similarly scales. Since both variables are continuous, we can use scale_x_continuous() and scale_y_continuous() to change these scales, the labels displayed, etc.\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from the Varieties of Democracy project\")+\n  xlab(\"log(gross domestic product per capita)\")+\n  ylim(-0.15,1)+\n  scale_color_discrete(name = \"Political-geographic\\nregion\")+\n  scale_size_continuous(name = \"Population size (in 10000s)\", breaks = seq(0,40000, by = 5000))+\n  scale_x_continuous(breaks = seq(-0.5,1.5, by = 0.75))\n\n\n\n\n\n\n\nChanging the theme()\nThe default design in ggplot2 contains a grid for the coordinate system, a grey background to the canvas, a default size for the text included, etc.. You may change and adjust all of these different features using the theme() layer.\nFor instance, we may change the color of the background:\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from the Varieties of Democracy project\")+\n  xlab(\"log(gross domestic product per capita)\")+\n  scale_color_discrete(name = \"Political-geographic\\nregion\")+\n  scale_size_continuous(name = \"Population size (in 10000s)\")+\n  theme(panel.background = element_rect( fill = \"lightyellow\"))\n\n\n\n\nOr we may remove the background completely:\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from the Varieties of Democracy project\")+\n  xlab(\"log(gross domestic product per capita)\")+\n  scale_color_discrete(name = \"Political-geographic\\nregion\")+\n  scale_size_continuous(name = \"Population size (in 10000s)\")+\n  theme(panel.background = element_blank())\n\n\n\n\nUps… We now also removed the axes lines and grid, but we can add them back in:\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from the Varieties of Democracy project\")+\n  xlab(\"log(gross domestic product per capita)\")+\n  scale_color_discrete(name = \"Political-geographic\\nregion\")+\n  scale_size_continuous(name = \"Population size (in 10000s)\")+\n  theme(panel.background = element_blank(), \n        panel.grid = element_line(color = \"lightgrey\"), \n        axis.line = element_line(color = \"black\"))\n\n\n\n\nWe might want to change the size of the text of the axes titles:\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from the\\nVarieties of Democracy project\")+\n  xlab(\"log(gross domestic\\nproduct per capita)\")+\n  scale_color_discrete(name = \"Political-geographic\\nregion\")+\n  scale_size_continuous(name = \"Population size (in 10000s)\")+\n  theme(panel.background = element_blank(), \n        panel.grid = element_line(color = \"lightgrey\"), \n        axis.line = element_line(color = \"black\"), \n        axis.title = element_text(size = 16))\n\n\n\n\n\n\nUsing and modifying existing themes\nThere are a number of built-in themes that you may use to quickly improve the aesthetics of your visualizations. For instance, you may simply go for theme_classic()\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from the Varieties of Democracy project\")+\n  xlab(\"log(gross domestic product per capita)\")+\n  scale_color_discrete(name = \"Political-geographic\\nregion\")+\n  scale_size_continuous(name = \"Population size (in 10000s)\")+\n theme_classic()\n\n\n\n\nOr perhaps theme_dark() is more your thing:\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from the Varieties of Democracy project\")+\n  xlab(\"log(gross domestic product per capita)\")+\n  scale_color_discrete(name = \"Political-geographic\\nregion\")+\n  scale_size_continuous(name = \"Population size (in 10000s)\")+\n theme_dark()\n\n\n\n\nYou can make additional adjustments to the theme also after having chosen a specific theme:\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from the\\nVarieties of Democracy project\")+\n  xlab(\"log(gross domestic\\nproduct per capita)\")+\n  scale_color_discrete(name = \"Political-geographic\\nregion\")+\n  scale_size_continuous(name = \"Population size (in 10000s)\")+\n theme_classic()+\n  theme(axis.title = element_text(size = 20, color = \"cornflowerblue\"))\n\n\n\n\nIt’s possible to build your own theme, which may be a useful thing to do if you repeatedly make the same adjustments to the themes of your figures.\n\ntheme_uio &lt;- theme(text = element_text(color = \"#B60000\"), \n                   panel.background = element_rect(fill = \"#ffffff\"), \n                   axis.title = element_text(size = 16),\n                   axis.line= element_line(color = \"#B60000\"), \n                   panel.grid = element_blank())\n\nOnce defined, you can add the theme as a layer:\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\", color = \"#B60000\")+\n  ylab(\"Polyarchy score from the\\nVarieties of Democracy project\")+\n  xlab(\"log(gross domestic\\nproduct per capita)\")+\n  scale_color_discrete(name = \"Political-geographic\\nregion\")+\n  scale_size_continuous(name = \"Population size (in 10000s)\")+\n theme_uio\n\n\n\n\nYour imagination sets the limits in terms of customizing your themes. Jané (2023) provides a series of popular culture inspired themes. For instance, Nintendo fans, might appreciate the Zelda theme.\n\nlibrary(ThemePark)\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from\\nthe Varieties of Democracy project\")+\n  xlab(\"log(gross domestic product per capita)\")+\n  scale_color_discrete(name = \"Political-geographic\\nregion\")+\n  scale_size_continuous(name = \"Population size (in 10000s)\")+\n  theme_zelda(zelda_font = TRUE)\n\n\n\n\nor perhaps you prefer Star Wars:\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from the Varieties of Democracy project\")+\n  xlab(\"log(gross domestic product per capita)\")+\n  scale_color_discrete(name = \"Political-geographic\\nregion\")+\n  scale_size_continuous(name = \"Population size (in 10000s)\")+\n  theme_starwars(starwars_font = TRUE)\n\n\n\n\nThe `ggthemes package allows you to pretend that you work for The Economist or that you made your graph Stata:\n\nlibrary(ggthemes)\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from the Varieties of Democracy project\")+\n  xlab(\"log(gross domestic product per capita)\")+\n  scale_color_discrete(name = \"Political-geographic\\nregion\")+\n  scale_size_continuous(name = \"Population size (in 10000s)\")+\n  theme_economist()\n\n\n\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from the Varieties of Democracy project\")+\n  xlab(\"log(gross domestic product per capita)\")+\n  scale_color_discrete(name = \"Political-geographic\\nregion\")+\n  scale_size_continuous(name = \"Population size (in 10000s)\")+\n  theme_stata()\n\n\n\n\nWe don’t suggest you use any of these themes, but they illustrate how you can customize your ggplot2 visualization in pretty much any way you would like."
  },
  {
    "objectID": "Visualization/data_viz_in_ggplot.html#saving-your-visualizations",
    "href": "Visualization/data_viz_in_ggplot.html#saving-your-visualizations",
    "title": "Data Visualization in ggplot2",
    "section": "Saving your visualizations",
    "text": "Saving your visualizations\nWe recommend writing in Quarto and including the visualizations from R chunks. You can then specify labels, captions, figure size, etc. from the chunk execution options, which is what we did for all the figures above and for figure Figure 1 here:\n\n```{r}\n#| label: fig-nicest\n#| fig-cap: \"This is perhaps the nicest version of our figure\"\n#| fig-width: 5\n#| fig-height: 5\nggplot(data = countries_1848, \n       mapping = aes(x = log_e_gdppc, \n                     y = v2x_polyarchy))+\n  geom_point(aes(color = region, size = e_pop))+\n  geom_smooth(method = \"loess\")+\n  ylab(\"Polyarchy score from the Varieties of Democracy project\")+\n  xlab(\"log(gross domestic product per capita)\")+\n  scale_color_discrete(name = \"Political-geographic\\nregion\")+\n  scale_size_continuous(name = \"Population size (in 10000s)\")+\n theme_classic()\n```\n\n\n\n\nFigure 1: This is perhaps the nicest version of our figure\n\n\n\n\nHowever, you may also save the figure to a location on your computer using ggsave(). The following code will save the latest figure we created to “nicest.pdf” set both the height and width of the figure to be 7 inches.\n\nggsave(file = \"nicest.pdf\", height = 7, width = 7, unit = \"in\" )"
  },
  {
    "objectID": "Visualization/data_viz_in_ggplot.html#footnotes",
    "href": "Visualization/data_viz_in_ggplot.html#footnotes",
    "title": "Data Visualization in ggplot2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe have hidden the code adding the additional variables as it is not terribly interesting for the topic at hand, but you can click on “code” to display it).↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STV4030A: Digital Data in Political Science",
    "section": "",
    "text": "Welcome to the course website for STV4030A - Digital Data in Political Science!\n\n\nThe aim of this course is to prepare you to use R and RStudio to gather your own data and write a quantitative MA thesis in political science or peace and conflict studies.\nIn previous courses, you have learned how to use R and RStudio to open datasets, recode variables, and fit statistical models. In this course, we will focus on all the other skills you need to do quantitative political science:\nSpecifically, this website contains a set of resources on how to:\n\ncollect data from various digital sources\nprepare the data for statistical analysis\nvisualize different types of data and results from statistical models\nhow to combine text and code in Quarto documents\nmake the full research process from data collection to a final MA thesis efficient and reproducible.\n\nThe materials on this website serve as the curriculum for STV4030A - Digital Data in Political Science, but the materials may also be useful for students working on their MA theses or any other project involving quantitative data.\nWe believe that the skills and tools covered in this course are better learned through practice than by reading textbooks and attending lectures. Instead of a traditional lecture-based course, we will therefore provide you with a set of weekly challenges/assignments that you should complete during the STV4030A - Digital Data in Political Science course. You will work on these challenges during the seminars (where we will be available to help you) and on your own.\nThe materials on this website aim to assist you in solving the weekly challenges (and in working on similar problems in your MA projects).\nThe course will run for five weeks with one mandatory challenge/assignment each week. Here is a list of the material that will useful to review prior to solving each week’s assignment:\n\nFor the first week, you should review the sections on “Working in R”, “Writing Quarto documents”, “Data Wrangling” and “Data visualization”. These topics provide the foundation for the rest of the course and will stay relevant also for the remainder of the assignments!\nFor the second week, you should review all the materials on “Text as Data”, covering how to load texts into R, how to use regular expressions define patterns in the texts that we are interested in detecting, removing, or extracting, how How to extract data from texts, and how to preprocess text data for further statistical analysis.\nFor the third week, you should review the materials on webscraping.\nFor the fourth week, you should review the materials on APIs.\nFor the final week, you should review the materials on spatial data.\n\nThis course assumes previous familiarity with R from STV4022, PECOS4022, or a similar course, but we include a brief review of R and how to work in R and RStudio here. Whereas those courses focused on estimating statistical models in R using datasets that were already prepared for analysis, we will now focus on all the other tools you will need to write a quantitative MA thesis in political science or peace and conflict studies – from data collection to writing up your thesis in Quarto.\nWe will use a variety of example datasets. You can access all of them here.\nWe hope you will enjoy the course!"
  },
  {
    "objectID": "spatial_data/basic_maps.html",
    "href": "spatial_data/basic_maps.html",
    "title": "Creating maps",
    "section": "",
    "text": "Building maps in R\nWe use the ggplot and sf packages to build maps in R. If you’re already familiar with ggplot (see the previous lesson on ggplot for a refresher), many of the following principles will be familiar to you.\nThe key principle of creating maps in R using ggplot is that we build the map in layers. We put different elements of the map on top of each other, using the + operator to combine several geoms.\nOur main work horse function is the geom_sf() function. It mostly works like any other geom, but has a few extras. Let’s look at an example.\nBut first, we need load and prepare our map data from the previous example.\n\n\n\n\n\n\nRecap exercise\n\n\n\nThe code below is a concise summary of all the steps from the previous lesson, with minimal explanations. It’s a good exercise to go through each line below and try to think about what it does and why. The code block is collapsed for readibility reasons.\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\n\n# read the Bosnia shapefile with st_read() =&gt; it's already an sf data frame\n# because of the .shp (shapefile) format!\nbosnia_shp &lt;- st_read(\"../data/spatial_data/datasets/bosnia_shapefile/bosnia.shp\", \n                      crs = 4326, quiet = T)\n\n# read the Bosnia GED events\nged &lt;- read_csv(\"../data/spatial_data/datasets/ged.csv\")\n\n# transform GED events into sf data frame\nged &lt;- ged %&gt;% \n  st_as_sf(coords = c(\"longitude\", \"latitude\"), \n           crs = 4326)\n\n# rename the ID columns\nged &lt;- ged %&gt;% \n  rename(id_event = id)\n\nbosnia_shp &lt;- bosnia_shp %&gt;% \n  rename(id_munic = id)\n\n\n# perform the spatial join\nged_munic &lt;- st_join(ged, \n                     bosnia_shp) \n\n# aggregate events on municipality level\nged_munic_ag &lt;- ged_munic %&gt;% \n  st_drop_geometry() %&gt;% \n  group_by(id_munic) %&gt;% \n  summarise(conflict_event_count = n()) \n\n# join aggregated events back into the main municipalitiy shapefile\nbosnia_shp_ged &lt;- left_join(bosnia_shp, \n                            ged_munic_ag, \n                            by = \"id_munic\")\n\n# replace missings in the conflict event count\nbosnia_shp_ged &lt;- bosnia_shp_ged %&gt;% \n  replace_na(list(conflict_event_count = 0))\n\n\n\n\nMaps and layers\nLet’s build a basic map first to see the commonalities and differences to the regular ggplot procedure.\n\n1basic_map &lt;- ggplot() +\n2  geom_sf(data = bosnia_shp_ged,\n3          aes(fill = conflict_event_count))\n\nbasic_map\n\n\n1\n\nA key difference to a usual ggplot is that we don’t give the data argument to the main ggplot() call. Instead we simply initiate an empty canvas with a blank ggplot() call.\n\n2\n\nWe explicitly hand over the data bosnia_shp_ged, our main sf data frame with the Bosnian municipalities, to the geom_sf() geom call. That way, we know that this geom_sf() refers to the main data layer.\n\n3\n\nSimilar to a regular ggplot we can then map plot elements to data using arguments to the aes() function within the geom_sf(). Here we want to fill the municipalities according to the number of conflict events in a municipality, so we’re using fill = conflict_event_count.\n\n\n\n\n\n\n\nWe’ll talk about how to improve the coloring scheme in a minute, but let me first show you how to layer other map elements on top of this map.\n\nbasic_map &lt;- ggplot() + \n  # municipality layer\n  geom_sf(data = bosnia_shp_ged,  \n          aes(fill = conflict_event_count)) +\n  # point layer (conflict events)\n  geom_sf(data = ged, \n          color = \"firebrick\")\n\nTo layer map elements on top of each other, we simply add another geom_sf() on top of the existing geom_sf() that plots the municipalities. I’ve given the points the color = \"firebrick to make the points better visible on top of the map. This is how this looks like:\n\nbasic_map\n\n\n\n\nWe could repeat this layering with other data frames, for instance a spatial data frame containing the coordinates of major cities in Bosnia as reference points.\n\n\nCategorizing continuous variables\nNow, the previous example looks admittedly a bit ugly. The problem is that the automatically picked color fill scale is somewhat off. The reason is that the underlying data is very heavily right-skewed, meaning that one municipality (Sarajevo) has waaaay more conflict events than the other municipalities.\nThe main way to deal with such skewed data is to categorize the underlying conflict event count data. In fact, that’s a general rule when your map coloring is off: categorize the underlying data and then try to plot it.\nSo let’s do that. We use the classify_intervals() function from the classInt package to put the continous conflict_event_count variable into a categorical variable.\n\nlibrary(classInt)\n\n# classify continuous variable\nbosnia_shp_ged &lt;- bosnia_shp_ged %&gt;% \n1  mutate(conflict_event_count_cat = classify_intervals(conflict_event_count,\n2                                                       n = 6,\n3                                                       intervalClosure = \"left\",\n4                                                       style = \"quantile\"))\n\n\n1\n\nFirst argument to classify_intervals() is the variable name that we want to classify\n\n2\n\nThe n argument tells the function how many categories we want. As a rule of thumb, you don’t want more categories than 8 or 9, since the human eye can’t distinguish between more color shades than that.\n\n3\n\nintervalClosure = \"left\" tells the function that we want one separate category for the zeros, since it’s usually useful to know where the true zeros are (i.e. municipalities without conflict)\n\n4\n\nstyle = \"quantile\" assigns the algorithm used to categorize the continuous variable. Use help(classify_intervals) to display other algorithm options.\n\n\n\n\nHere’s how the result looks like. We also use the scale_fill_brewer() argument to ggplot to assign a nicer, continuous color palette to the categorized conflict events variable.\n\nbasic_map &lt;- ggplot() + \n  # municipality layer\n  geom_sf(data = bosnia_shp_ged,  \n          aes(fill = conflict_event_count_cat)) +\n  scale_fill_brewer()\n  \n\nbasic_map\n\n\n\n\nThis looks much nicer! On the right hand side, the legend displays the different categories that we created with classify_intervals(). A ) means that the number just left to the ) is NOT included in the range indicated by the two numbers.\n\n\nPicking a color scale\nThe color scale we used with scale_fill_brewer() is the Blues color scales from the website ColorBrewer This website has a number of color schemes that look good on a map. You can go to the website and pick a color scale that might be more suitable to the style in which you want to display the map.\nI’ve circled in red where you find the name of the color palette on the ColorBrewer website.\n\nHere’s how we change the color palette:\n\nbasic_map &lt;- ggplot() + \n  # municipality layer\n  geom_sf(data = bosnia_shp_ged,  \n          aes(fill = conflict_event_count_cat)) +\n  scale_fill_brewer(palette = \"YlOrRd\")\n  \n\nbasic_map\n\n\n\n\nMuch better\n\n\nFine-tuning the legend\nWe can fine tune the legend even more by assigning more human-readable labels and putting it below the map.\n\nbasic_map &lt;- ggplot() + \n  # municipality layer\n  geom_sf(data = bosnia_shp_ged,  \n          aes(fill = conflict_event_count_cat)) +\n1  scale_fill_brewer(\"Number of conflict events:\",\n2                    palette = \"YlOrRd\",\n3                    labels = c(\"No events\",\n                               \"1\",\n                               \"2-3\", \n                               \"4-8\", \n                               \"9 or more\")) +\n4  theme(legend.position = \"bottom\")\n  \n\nbasic_map\n\n\n1\n\nWe assign a title to the legend.\n\n2\n\nThe palette we want to use (see above)\n\n3\n\nHere come the labels. Note that they should be in the same order as the categorical variable. Also note that I’ve correctly labelled the range based on the excluded number as indicated by the ) in the category name.\n\n4\n\nAnd we put the legend to the bottom using the theme() function\n\n\n\n\n\n\n\n\n\nBackground and borders\nThis almost looks like a publishable map!\nWe can remove the background and axis labels since they are not so informative on a map like this:\n\nbasic_map &lt;- ggplot() + \n  # municipality layer\n  geom_sf(data = bosnia_shp_ged,  \n          aes(fill = conflict_event_count_cat)) +\n  scale_fill_brewer(\"Number of conflict events:\", \n                    palette = \"YlOrRd\", \n                    labels = c(\"No events\", \n                               \"1\",\n                               \"2-3\", \n                               \"4-8\", \n                               \"9 or more\")) +\n  # remove background\n  theme_void() + \n  theme(legend.position = \"bottom\") \n  \n\nbasic_map\n\n\n\n\nAnd we can also style the border of the municipalities a bit to make them thinner.\n\nbasic_map &lt;- ggplot() + \n  # municipality layer\n  geom_sf(data = bosnia_shp_ged,  \n          aes(fill = conflict_event_count_cat), \n          # style the border width\n          linewidth = .1) +\n  scale_fill_brewer(\"Number of conflict events:\", \n                    palette = \"YlOrRd\", \n                    labels = c(\"No events\", \n                               \"1\",\n                               \"2-3\", \n                               \"4-8\", \n                               \"9 or more\")) +\n  # remove background\n  theme_void() + \n  theme(legend.position = \"bottom\") +\n  labs(title = \"Conflict events in Bosnia\")\n\nbasic_map\n\n\n\n\nVoilá! Here is our final map!\n\n\nCreating a world map\nA frequent task in research is to create a world map and visualize the global distribution of some variable.\nLet’s visualize democracy scores across the world (see this example from a prior lesson on democracy scores). We use the V-Dem democracy scores and the cshapes package (do an install.packages(\"cshapes\") if you haven’t installed the package yet).\nThe cshp function from the cshapes package gives us a map of every country in the world at a certain date as specified by the date = argument. We want the world at January 1st, 2015.\n\nlibrary(cshapes)\n\n# load democracy scores data\ndemocracy_scores &lt;- read_delim(\"../data/democracy_scores.csv\", delim = \",\")\n\n# load a world map data\nworld_map_data &lt;- cshp(date = as.Date(\"2015-01-01\"))\n\nLet’s look at the world map data:\n\nworld_map_data\n\nSimple feature collection with 174 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -55.90223 xmax: 180 ymax: 83.11387\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   gwcode             country_name      start        end      status owner\n3       2 United States of America 1959-08-21 2019-12-31 independent     2\n9      20                   Canada 1948-07-22 2019-12-31 independent    20\n12     31                  Bahamas 1973-07-10 2019-12-31 independent    31\n14     40                     Cuba 1902-05-20 2019-12-31 independent    40\n17     41                    Haiti 1934-08-15 2019-12-31 independent    41\n19     42       Dominican Republic 1886-01-01 2019-12-31 independent    42\n21     51                  Jamaica 1962-08-06 2019-12-31 independent    51\n23     52      Trinidad and Tobago 1962-08-31 2019-12-31 independent    52\n25     53                 Barbados 1966-11-30 2019-12-31 independent    53\n28     70                   Mexico 1886-01-01 2019-12-31 independent    70\n          capname   caplong   caplat b_def fid                       geometry\n3      Washington -77.03667 38.89500     1   3 MULTIPOLYGON (((-155.0056 1...\n9          Ottawa -75.69000 45.42083     1   9 MULTIPOLYGON (((-53.53555 4...\n12         Nassau -77.34300 25.07735     1  12 MULTIPOLYGON (((-73.03585 2...\n14         Havana -82.36417 23.13194     1  14 MULTIPOLYGON (((-77.34175 2...\n17 Port-au-Prince -72.33500 18.53920     1  17 MULTIPOLYGON (((-72.79945 1...\n19  Santo Domingo -69.90000 18.46667     1  19 MULTIPOLYGON (((-68.57445 1...\n21       Kingston -76.80000 18.00000     1  21 MULTIPOLYGON (((-76.6577 17...\n23  Port-of-Spain -61.51667 10.65000     1  23 MULTIPOLYGON (((-60.94063 1...\n25     Bridgetown -59.61666 13.10000     1  25 MULTIPOLYGON (((-59.538 13....\n28    Mexico City -99.13861 19.43417     1  28 MULTIPOLYGON (((-109.7865 2...\n\n\nConveniently, the cshp() function returns an sf data frame, i.e. the data already comes in a spatial data frame format. There are several columns, including columns that indicate the coordinates for the country’s capital at the time.\nThat means we can plot the map already with the tools we have!\n\nworld_map &lt;- ggplot() + \n  geom_sf(data = world_map_data)\n\nworld_map\n\n\n\n\nNow we need to merge the democracy score data into the world map data frame.\nTo do that we first need to create a column in the democracy scores data frame with the correct Gleditsch and Ward country code. See here for a refresher on country codes from a prior lesson.\n\nlibrary(countrycode)\n\ndemocracy_scores &lt;- democracy_scores %&gt;% \n  # only year 2015\n  filter(year == 2015) %&gt;% \n  # assign Gleditsch and Ward country codes since those are used in the cshp package\n  mutate(gwcode = countrycode(country_name, \n                              \"country.name\", \n                              \"gwn\"))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `gwcode = countrycode(country_name, \"country.name\", \"gwn\")`.\nCaused by warning in `countrycode_convert()`:\n! Some values were not matched unambiguously: Hong Kong, Palestine/Gaza, Palestine/West Bank, Sao Tome and Principe, Seychelles, Somaliland, Vanuatu, Yemen\n\n\nThis throws some errors, since some of the countries in V-Dem do not have a dedicated GW ID. But we’ll ignore that error for now, since it’s not super relevant.\nLet’s merge the two datasets, but filter out the v2x_polyarchy variable first so we get a clean world map data set with only one variable, the main democracy indicator of V-Dem, v2x_polyarchy.\n\nworld_map_data &lt;- left_join(world_map_data, \n                            democracy_scores %&gt;% \n                              filter(democracy_index == \"v2x_polyarchy\"), \n                            by = \"gwcode\")\n\nworld_map_data\n\nSimple feature collection with 174 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -55.90223 xmax: 180 ymax: 83.11387\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   gwcode           country_name.x      start        end      status owner\n1       2 United States of America 1959-08-21 2019-12-31 independent     2\n2      20                   Canada 1948-07-22 2019-12-31 independent    20\n3      31                  Bahamas 1973-07-10 2019-12-31 independent    31\n4      40                     Cuba 1902-05-20 2019-12-31 independent    40\n5      41                    Haiti 1934-08-15 2019-12-31 independent    41\n6      42       Dominican Republic 1886-01-01 2019-12-31 independent    42\n7      51                  Jamaica 1962-08-06 2019-12-31 independent    51\n8      52      Trinidad and Tobago 1962-08-31 2019-12-31 independent    52\n9      53                 Barbados 1966-11-30 2019-12-31 independent    53\n10     70                   Mexico 1886-01-01 2019-12-31 independent    70\n          capname   caplong   caplat b_def fid           country_name.y\n1      Washington -77.03667 38.89500     1   3 United States of America\n2          Ottawa -75.69000 45.42083     1   9                   Canada\n3          Nassau -77.34300 25.07735     1  12                     &lt;NA&gt;\n4          Havana -82.36417 23.13194     1  14                     Cuba\n5  Port-au-Prince -72.33500 18.53920     1  17                    Haiti\n6   Santo Domingo -69.90000 18.46667     1  19       Dominican Republic\n7        Kingston -76.80000 18.00000     1  21                  Jamaica\n8   Port-of-Spain -61.51667 10.65000     1  23      Trinidad and Tobago\n9      Bridgetown -59.61666 13.10000     1  25                 Barbados\n10    Mexico City -99.13861 19.43417     1  28                   Mexico\n   country_id year democracy_index score                       geometry\n1          20 2015   v2x_polyarchy 0.901 MULTIPOLYGON (((-155.0056 1...\n2          66 2015   v2x_polyarchy 0.813 MULTIPOLYGON (((-53.53555 4...\n3          NA   NA            &lt;NA&gt;    NA MULTIPOLYGON (((-73.03585 2...\n4         155 2015   v2x_polyarchy 0.168 MULTIPOLYGON (((-77.34175 2...\n5          26 2015   v2x_polyarchy 0.404 MULTIPOLYGON (((-72.79945 1...\n6         114 2015   v2x_polyarchy 0.576 MULTIPOLYGON (((-68.57445 1...\n7         120 2015   v2x_polyarchy 0.805 MULTIPOLYGON (((-76.6577 17...\n8         135 2015   v2x_polyarchy 0.764 MULTIPOLYGON (((-60.94063 1...\n9         147 2015   v2x_polyarchy 0.769 MULTIPOLYGON (((-59.538 13....\n10          3 2015   v2x_polyarchy 0.636 MULTIPOLYGON (((-109.7865 2...\n\n\nNow we’re ready to map the democracy score:\n\nworld_map_democracy &lt;- ggplot() + \n  geom_sf(data = world_map_data, \n          aes(fill = score))\n\nworld_map_democracy\n\n\n\n\nLooking good! Western Europe and North America look pretty democratic, parts of Africa, Russia and Central Asia look more autocratic, exactly as we would expect. Now we can style the map a bit, using the tricks from above.\nI’m not going to explain each step here, so try to read the code and figure out what each step does.\n\nworld_map_democracy &lt;- ggplot() + \n  geom_sf(data = world_map_data, \n          aes(fill = score), \n          linewidth = .1) +\n  scale_fill_distiller(\"Democracy score:\", \n                       palette = \"Oranges\", direction = 1) +\n  theme_void() + \n  theme(legend.position = \"bottom\") +\n  labs(title = \"V-Dem democracy scores around the world, 2015\")\n\nworld_map_democracy\n\n\n\n\nAnd this is our world map!\n\n\nFinal thoughts\nNow you know:\n\nWhat spatial data is and what types of spatial data exist\nHow to get spatial data into R\nHow to combine spatial information using spatial joins\nAnd how to create nicely looking maps in R\n\nHave fun exploring the spatial side of R!"
  },
  {
    "objectID": "spatial_data/reading_spatial_data.html",
    "href": "spatial_data/reading_spatial_data.html",
    "title": "Reading spatial data",
    "section": "",
    "text": "Now that we know what spatial data looks like, how do we actually work with it in R?\nThere are two main ways to read spatial in R: (1) reading point data from a regular data file (e.g. .csv or .xslx) and converting it into a spatial data frame or (2) reading “shapefiles,” the name for a collection of data files that represent some spatial feature, such as polygons.\nLet’s look at both ways in more detail. I’ll be using the example from Nils Weidmann’s book in which we try to figure out how many conflict events took place in which Bosnian municipalities during Bosnia and Herzegovina’s civil war in the 1990s. Essentially, we want to find out how conflict intensity varied across the country."
  },
  {
    "objectID": "spatial_data/reading_spatial_data.html#converting-point-data-into-a-spatial-data-frame",
    "href": "spatial_data/reading_spatial_data.html#converting-point-data-into-a-spatial-data-frame",
    "title": "Reading spatial data",
    "section": "Converting point data into a spatial data frame",
    "text": "Converting point data into a spatial data frame\nTo convert the ged data frame into a spatial data frame we use the st_as_sf() command:\n\nged &lt;- ged %&gt;% \n  st_as_sf(coords = c(\"longitude\", \"latitude\"), \n           crs = 4326)\n\nThere’s a couple of things going on in that command:\n\nFirst you’ll see that we’re using the pipe %&gt;% command to manipulate the data set just like we would use it in the tidyverse using mutate() for example.\nThe coords = argument takes the names of the longitude and latitude columns as argument. Note that the longitude column always comes before the latitude column.\nThe crs = 4326 argument assigns the coordinate reference system. The number 4326 is the name of the WGS 84 coordinate reference as specified by the European Petroleum Survey Group, an organization that classifies and defines different coordinate reference systems. Point data almost always comes in the WGS 84 CRS, so you usually add the crs = 4326 argument when you convert regular data frames into spatial data frames.\n\nHow does st_as_sf() change the ged dataset? Let’s have a look:\n\nged\n\nSimple feature collection with 1136 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 15.78194 ymin: 42.71194 xmax: 19.53556 ymax: 45.18944\nGeodetic CRS:  WGS 84\n# A tibble: 1,136 × 4\n       id date_start  best            geometry\n *  &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;         &lt;POINT [°]&gt;\n 1 199885 1993-02-01     6 (18.80833 44.87278)\n 2 199767 1994-03-03     1 (15.91861 44.84444)\n 3 200575 1995-03-12     1    (18.38333 43.85)\n 4 200731 1995-07-07     1 (19.29694 44.10639)\n 5 200732 1995-07-08     1 (19.29694 44.10639)\n 6 200733 1995-07-09     2 (19.29694 44.10639)\n 7 199829 1992-04-29     1  (17.98972 45.1375)\n 8 200424 1992-05-17     1    (18.38333 43.85)\n 9 199360 1992-11-10    13 (18.42694 44.88083)\n10 200412 1992-11-22     4    (18.38333 43.85)\n# ℹ 1,126 more rows\n\n\nThe output above now tells us that the data frame is a spatial data frame through the title “Simple feature collection.” It also tells us that this is a POINT data frame, which is useful (remember the different types of spatial data we encountered in the previous section. Finally the output shows that the data frame now has a geometry column. As we learned in the previous section, this indicates that we now are working with a “spatially enhanced” version of the GED conflict event data frame.\nBut did it actually work? It is always a good idea to visually check whether converting the data frame into a spatial data frame actually worked. We can do that by simply running the following line. The st_geometry() reduces the data set only to its spatial information, i.e. the points and their location in space, which is what we want.\n\nged %&gt;% st_geometry() %&gt;% plot()\n\n\n\n\nThis looks pretty decent! We have several points that cluster in space, which is what we would expect. There are more advanced ways to plot this, which we will learn about in the next chapter."
  },
  {
    "objectID": "Wrangling/tidy_data.html",
    "href": "Wrangling/tidy_data.html",
    "title": "Tidy Data",
    "section": "",
    "text": "As quantitative political scientists gathering various forms of digital data, much of our time will be spent transforming our data into a format suitable for analysis. What do we mean by “a format suitable for analysis”? Well, we typically want our data to be what Wickham, Çetinkaya-Rundel, and Grolemund (2023) refer to as “tidy”, meaning that:\n\nEach variable is a column and each column is a variable.\nEach observation is a row and each row is an observation\nEach value is a cell and each cell is a single value.\n\nIf we have different types of observational units, it is best to organize them in different data.frames.\nUnfortunately, our data will not always be provided to us in a tidy format. This is often true even when we work with existing, supposedly ready-for-analysis, data. Consider for instance the following dataset which contains data from the World Bank on the population of different countries and territories each year from 1960 to 2021.\n\nlibrary(readr)\nwb_population_data &lt;- read_delim(\"../data/WBpopulation.csv\", delim = \",\")\nhead(wb_population_data)\n\n# A tibble: 6 × 66\n  `Series Name`     `Series Code` `Country Name` `Country Code` `1960 [YR1960]`\n  &lt;chr&gt;             &lt;chr&gt;         &lt;chr&gt;          &lt;chr&gt;          &lt;chr&gt;          \n1 Population, total SP.POP.TOTL   Afghanistan    AFG            8622466        \n2 Population, total SP.POP.TOTL   Albania        ALB            1608800        \n3 Population, total SP.POP.TOTL   Algeria        DZA            11394307       \n4 Population, total SP.POP.TOTL   American Samoa ASM            20085          \n5 Population, total SP.POP.TOTL   Andorra        AND            9443           \n6 Population, total SP.POP.TOTL   Angola         AGO            5357195        \n# ℹ 61 more variables: `1961 [YR1961]` &lt;chr&gt;, `1962 [YR1962]` &lt;chr&gt;,\n#   `1963 [YR1963]` &lt;chr&gt;, `1964 [YR1964]` &lt;chr&gt;, `1965 [YR1965]` &lt;chr&gt;,\n#   `1966 [YR1966]` &lt;chr&gt;, `1967 [YR1967]` &lt;chr&gt;, `1968 [YR1968]` &lt;chr&gt;,\n#   `1969 [YR1969]` &lt;chr&gt;, `1970 [YR1970]` &lt;chr&gt;, `1971 [YR1971]` &lt;chr&gt;,\n#   `1972 [YR1972]` &lt;chr&gt;, `1973 [YR1973]` &lt;chr&gt;, `1974 [YR1974]` &lt;chr&gt;,\n#   `1975 [YR1975]` &lt;chr&gt;, `1976 [YR1976]` &lt;chr&gt;, `1977 [YR1977]` &lt;chr&gt;,\n#   `1978 [YR1978]` &lt;chr&gt;, `1979 [YR1979]` &lt;chr&gt;, `1980 [YR1980]` &lt;chr&gt;, …\n\nnames(wb_population_data)\n\n [1] \"Series Name\"   \"Series Code\"   \"Country Name\"  \"Country Code\" \n [5] \"1960 [YR1960]\" \"1961 [YR1961]\" \"1962 [YR1962]\" \"1963 [YR1963]\"\n [9] \"1964 [YR1964]\" \"1965 [YR1965]\" \"1966 [YR1966]\" \"1967 [YR1967]\"\n[13] \"1968 [YR1968]\" \"1969 [YR1969]\" \"1970 [YR1970]\" \"1971 [YR1971]\"\n[17] \"1972 [YR1972]\" \"1973 [YR1973]\" \"1974 [YR1974]\" \"1975 [YR1975]\"\n[21] \"1976 [YR1976]\" \"1977 [YR1977]\" \"1978 [YR1978]\" \"1979 [YR1979]\"\n[25] \"1980 [YR1980]\" \"1981 [YR1981]\" \"1982 [YR1982]\" \"1983 [YR1983]\"\n[29] \"1984 [YR1984]\" \"1985 [YR1985]\" \"1986 [YR1986]\" \"1987 [YR1987]\"\n[33] \"1988 [YR1988]\" \"1989 [YR1989]\" \"1990 [YR1990]\" \"1991 [YR1991]\"\n[37] \"1992 [YR1992]\" \"1993 [YR1993]\" \"1994 [YR1994]\" \"1995 [YR1995]\"\n[41] \"1996 [YR1996]\" \"1997 [YR1997]\" \"1998 [YR1998]\" \"1999 [YR1999]\"\n[45] \"2000 [YR2000]\" \"2001 [YR2001]\" \"2002 [YR2002]\" \"2003 [YR2003]\"\n[49] \"2004 [YR2004]\" \"2005 [YR2005]\" \"2006 [YR2006]\" \"2007 [YR2007]\"\n[53] \"2008 [YR2008]\" \"2009 [YR2009]\" \"2010 [YR2010]\" \"2011 [YR2011]\"\n[57] \"2012 [YR2012]\" \"2013 [YR2013]\" \"2014 [YR2014]\" \"2015 [YR2015]\"\n[61] \"2016 [YR2016]\" \"2017 [YR2017]\" \"2018 [YR2018]\" \"2019 [YR2019]\"\n[65] \"2020 [YR2020]\" \"2021 [YR2021]\"\n\n\nThe tidy way of organizing this data would be to have different columns for the different variables: “country”, “year”, and “population” and to have one row per country per year. Using head() and names() we quickly see that the this dataset is not organized in this way. Instead, we have one row per country and multiple columns for the population variable, each of them measuring the population in the country in question in a particular year (In addition the variable names contain spaces and square brackets and sometimes start with numbers, which will also provide some problems).\nThe reason for why the data is structured in this way may be that it may appear as a convenient format for people working in spreadsheet, for instance using Microsoft Excel, but I will be inconvenient to work with in R so we will have to wrangle it into the right format. We will show you how to do that shortly. For now just consider the tidy version of the population dataset:\n\nwb_population_tidy &lt;- read_delim(\"../data/wb_population_tidy.csv\", delim = \",\")\nhead(wb_population_tidy)\n\n# A tibble: 6 × 6\n  series_name       series_code country_name country_code  year population\n  &lt;chr&gt;             &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 Population, total SP.POP.TOTL Afghanistan  AFG           1960    8622466\n2 Population, total SP.POP.TOTL Afghanistan  AFG           1961    8790140\n3 Population, total SP.POP.TOTL Afghanistan  AFG           1962    8969047\n4 Population, total SP.POP.TOTL Afghanistan  AFG           1963    9157465\n5 Population, total SP.POP.TOTL Afghanistan  AFG           1964    9355514\n6 Population, total SP.POP.TOTL Afghanistan  AFG           1965    9565147\n\nnames(wb_population_tidy)\n\n[1] \"series_name\"  \"series_code\"  \"country_name\" \"country_code\" \"year\"        \n[6] \"population\"  \n\n\nIn this data.frame, each variable has its own column and each observational unit (here the country-year) has its own row. Each cell contains the value of on one variable for one observation. This is what we want!\nWhile our goal is always to make our data tidy, to how our observations organized in rows and our variables organized in columns, how to get there will depend on the original structure of the data we are trying to make tidy. Tidying up the World Bank population data will look somewhat different than tidying up the dataset of European Court of Justice judgments that Louisa discusses in the video above.\nHowever, even as the exact workflow of cleaning up a dataset will vary from case to case, some combination of the wrangling tools provided in the dplyr and tidyr packages will usually get the job done. It is to these tools that we now turn.\n\n\n\n\nReferences\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. 2nd edition. \" O’Reilly Media, Inc.\"."
  },
  {
    "objectID": "Wrangling/wrangling_in_dplyr_and_tidyr.html",
    "href": "Wrangling/wrangling_in_dplyr_and_tidyr.html",
    "title": "Wrangling in dplyr and tidyr",
    "section": "",
    "text": "We now turn to how you can tidy your data so that each observation is a row, each variable is a column, and each value is a cell and how you can transform or manipulate your dataset so that it contains exactly the information you need for your analysis, for instance by subsetting to a particular set of observations or recoding some of the variables. We will refer to these tasks as data wrangling.\nWrangling is a fundamental part of quantitative political science! If we cannot get our data in the right format, we cannot do our advanced statistical analyses or make our beautiful visualizations.\nFor our data wrangling we will rely heavily on the dplyr and tidyr packages (both part of the so-called tidyverse). While tidyr contains a set of functions for making dataset tidy (for instance by pivoting/reshaping them), dplyr contains a set of functions for manipulating data (for instance by filtering observations or recoding variables).\nBefore we go in detail on the various wrangling functions, the following two videos provide an overview of functions we will often be using:\nNow that we have a sense of what some of the frequently used wrangling functions are, we will go in more detail in text and code.\nLet’s start by introducing some of the dplyr functions for manipulating data. Even when our dataset already is in a “tidy” format, many of these functions will be useful to further manipulate the data to suit our particular research needs!"
  },
  {
    "objectID": "Wrangling/wrangling_in_dplyr_and_tidyr.html#filter",
    "href": "Wrangling/wrangling_in_dplyr_and_tidyr.html#filter",
    "title": "Wrangling in dplyr and tidyr",
    "section": "filter()",
    "text": "filter()\nLet’s revisit the WhoGov dataset by Nyrup and Bramwell (2020), which we introduced here.\n\nlibrary(readr)\nwho_gov &lt;- read_delim(\"../data/WhoGov_within_V2.0.csv\", delim = \",\")\n\nSuppose we are only interested in observations from Norway. If so, we want to filter() the data:\n\nlibrary(dplyr)\n1who_gov_norway &lt;- filter(who_gov,\n2                         country_isocode == \"NOR\")\n\n\n1\n\nThe first argument is the name of a data.frame.\n\n2\n\nThe second argument is some condition that we use to filter() the data. Here the condition is that we only want observations for which country_isocode == \"NOR\" evaluates to TRUE.\n\n\n\n\nSuppose that we are also only interested in ministers and would like to remove other types of people also included in the dataset (we definitely don’t care about the kings!). The variable called classification contains a standardized classification of the different positions people in the dataset held (the codebook says so!).\nUsing table() we can figure out how the Norwegian observations are classified:\n\ntable(who_gov_norway$classification)\n\n\n     Ambassador to the United States                Deputy Prime Minister \n                                  29                                    2 \n             Governor (Central Bank)                 Member, Royal Family \n                                  36                                   56 \n                Minister (Full Rank)                       Prime Minister \n                                 958                                   56 \nRepresentative to the United Nations \n                                  26 \n\n\nIf we are interested in ministers, we want all the observations with the value “Prime Minister” or the value “Minister (Full Rank)” on the variable who_gov_norway$classification. Again, we can filter() the dataset:\n\nwho_gov_norway_ministers &lt;- filter(who_gov_norway, \n1                        classification == \"Prime Minister\" |\n                          classification == \"Minister (Full Rank)\" )\n\n\n1\n\nWe use the “or” operator “|” to filter the data so that observations need to either have the value “Prime Minister” or the value “Minister (Full Rank)” on the variable who_gov_norway$classification to be included in our filtered dataset.\n\n\n\n\nWe could also have combined these two conditions in one call to filter() using , to separate between different conditions that filtered observations need to satisfy:\n\n1who_gov_norway_ministers &lt;- filter(who_gov,\n2                                   country_isocode == \"NOR\",\n3                        classification == \"Prime Minister\" |\n                          classification == \"Minister (Full Rank)\") \n\n\n1\n\nWe are starting with the full who_gov data.frame.\n\n2\n\nWe filter observations for which country_isocode == \"NOR\".\n\n3\n\nAnd we filter observations for which either classification == \"Prime Minister\" or classification == \"Minister (Full Rank)\" evaluate to TRUE."
  },
  {
    "objectID": "Wrangling/wrangling_in_dplyr_and_tidyr.html#select",
    "href": "Wrangling/wrangling_in_dplyr_and_tidyr.html#select",
    "title": "Wrangling in dplyr and tidyr",
    "section": "select()",
    "text": "select()\nWhile filter() filters the observations in the dataset, select() selects specific columns in the dataset.\nSuppose we only want the variables “year”,“country_isocode”, “id”, “position”, “name”, “gender”, “birthyear”, “deadyear”, “party”, and “classification”. If so, we can select() these columns.\n\n1who_gov_norway_ministers &lt;-  select(who_gov_norway_ministers,\n2                                    year,country_isocode, id, position, name,\n                                    gender,  birthyear, deadyear, party, classification)\n\n\n1\n\nWe start with who_gov_norway_ministers and here we will also overwrite the existing who_gov_norway_ministers rather than assigning the result to a new object\n\n2\n\nWe select the variables we would like to keep.\n\n\n\n\nWe may also use select() to remove variables. To remove variables we simply add a minus sign before the variable names. Perhaps we figure out that we don’t need the country_isocode variable as all the observations are from Norway. If so, we can:\n\n who_gov_norway_ministers &lt;- select(who_gov_norway_ministers, \n1                                    -country_isocode)\n\n\n1\n\nRemoving the “country_isocode variable by putting a - in front of it in the select() code.\n\n\n\n\n\n\n\n\n\n\nhelper functions for select()\n\n\n\nSometimes we need to select many columns according to some rule or condition. There are multiple helper functions for such cases. Consider all the variables in the who_gov dataset:\n\nnames(who_gov)\n\n [1] \"...1\"                \"year\"                \"country_isocode\"    \n [4] \"country_name\"        \"id\"                  \"position\"           \n [7] \"name\"                \"title\"               \"gender\"             \n[10] \"birthyear\"           \"deadyear\"            \"party\"              \n[13] \"party_english\"       \"party_otherlanguage\" \"whogov_partyid\"     \n[16] \"partyfacts_id\"       \"core\"                \"minister\"           \n[19] \"leader\"              \"classification\"      \"portfolio_1\"        \n[22] \"prestige_1\"          \"portfolio_2\"         \"prestige_2\"         \n[25] \"portfolio_3\"         \"prestige_3\"          \"portfolio_4\"        \n[28] \"prestige_4\"          \"m_finance\"           \"m_defense\"          \n[31] \"m_agriculture\"       \"m_foreignaffairs\"   \n\n\nSuppose for instance, that we want to select all the columns with names that start with “portfolio”. If so, we can use the helper function starts_with()\n\nportfolios &lt;- select(who_gov, \n                     starts_with(\"portfolio\"))\n\nnames(portfolios)\n\n[1] \"portfolio_1\" \"portfolio_2\" \"portfolio_3\" \"portfolio_4\"\n\n\nMost likely, we don’t want a data.frame with only the portfolios. We would likely also keep the “name” and “year” of the observation and maybe we also want all the variables with “party” somewhere in the variable name. If so, we could run:\n\nportfolios_and_party &lt;- select(who_gov, \n                     name, year, \n                     starts_with(\"portfolio\"), \n                     contains(\"party\"))\n\nnames(portfolios_and_party)\n\n [1] \"name\"                \"year\"                \"portfolio_1\"        \n [4] \"portfolio_2\"         \"portfolio_3\"         \"portfolio_4\"        \n [7] \"party\"               \"party_english\"       \"party_otherlanguage\"\n[10] \"whogov_partyid\"      \"partyfacts_id\"      \n\n\nThere are multiple other helper functions. For an overview, you can run the following line in your console:\n\n?tidyr_tidy_select"
  },
  {
    "objectID": "Wrangling/wrangling_in_dplyr_and_tidyr.html#rename",
    "href": "Wrangling/wrangling_in_dplyr_and_tidyr.html#rename",
    "title": "Wrangling in dplyr and tidyr",
    "section": "rename()",
    "text": "rename()\nIt is straightforward to rename() variables.\nSuppose we find the variable name “classification” to not really be that informative now that it only takes the values “Minister (Full Rank)” and “Prime Minister”.\nPerhaps we want to rename() it as “minister_type”. If so we can run:\n\n who_gov_norway_ministers &lt;- rename(who_gov_norway_ministers, \n1                                    minister_type = classification)\n\n\n1\n\nWe need to specify new_name = old_name to rename() a variable."
  },
  {
    "objectID": "Wrangling/wrangling_in_dplyr_and_tidyr.html#mutate",
    "href": "Wrangling/wrangling_in_dplyr_and_tidyr.html#mutate",
    "title": "Wrangling in dplyr and tidyr",
    "section": "mutate()",
    "text": "mutate()\nWe can add new variables (or recode existing ones) using mutate(). For instance, we can create a variable measuring the age of each minister like this:\n\nwho_gov_norway_ministers &lt;- mutate(who_gov_norway_ministers, \n1                                   age = year - birthyear)\n\n\n1\n\nWe create a new variable called age which is year minus birthyear.\n\n\n\n\nSo, basically you need to supply the name of the new variable followed by the = sign and then you specify how this new variable should be created.\nIf the new variable name is the same as an existing variable name, the existing variable will be replaced by the new variable you are creating.\nYou can use various R functions to create the new variables. For instance, we will often use the ifelse() function (which we introduced here).\nIf we inspect the who_gov_norway_ministers carefully, we notice that there is a minister of fisheries and a minister of agriculture which both have missing values on multiple variables including on birthyear and who have the value “unknown” on the variable party. Using filter() we can print out these rows to the console:\n\nfilter(who_gov_norway_ministers, party == \"unknown\")\n\n# A tibble: 5 × 10\n   year    id position name  gender birthyear deadyear party minister_type   age\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;\n1  1968    41 Min. Of… Moyn… Unkno…        NA &lt;NA&gt;     unkn… Minister (Fu…    NA\n2  1969    57 Min. Of… Moyn… Unkno…        NA &lt;NA&gt;     unkn… Minister (Fu…    NA\n3  1970    73 Min. Of… Moyn… Unkno…        NA &lt;NA&gt;     unkn… Minister (Fu…    NA\n4  1998   600 Min. Of… Kare… Male          NA &lt;NA&gt;     unkn… Minister (Fu…    NA\n5  1999   624 Min. Of… Kare… Male          NA &lt;NA&gt;     unkn… Minister (Fu…    NA\n\n\nAs political scientists trained in Norway we do know our ministers of fisheries1 and we certainly don’t recognize this “Moynes Einsi” who supposedly was minister of fisheries from 1968 to 1970. Einar Hole Moxnes was, however, minister of fisheries in the that period and we will use mutate() and ifelse() to recode these three observations with data on him:\n\nwho_gov_norway_ministers &lt;- mutate(who_gov_norway_ministers, \n                                   name = ifelse(name == \"Moynes Einsi\", \n1                                                 \"Einar Hole Moxnes\", name ),\n                                   party = ifelse(name == \"Einar Hole Moxnes\", \n2                                                  \"sp\", party),\n                                   birthyear = ifelse(name == \"Einar Hole Moxnes\", \n                                                       1921, birthyear),\n                                   deadyear = ifelse(name == \"Einar Hole Moxnes\", \n                                                     2006,deadyear),\n                                   gender = ifelse(name == \"Einar Hole Moxnes\", \n                                                   \"Male\", gender ),\n3                                   age = year - birthyear)\n\n\n1\n\nIf the name is “Moynes Einsi”, we replace it with “Einar Hole Moxnes”, otherwise we keep what is stored in name\n\n2\n\nWe recode the different variables for observations for which the name is (now) “Einar Hole Moxnes” and keep the other variables as they currently.\n\n3\n\nIf we calculate the age after the operations above, we avoid the missing value on age for “Einar Hole Moxnes”.\n\n\n\n\nWe similarly quickly realize that Kare Gennes should probably be Kåre Gjønnes who represented Kristelig Folkeparti. We correct these two observations as well (his gender was already coded correctly):\n\nwho_gov_norway_ministers &lt;- mutate(who_gov_norway_ministers, \n                                   name = ifelse(name == \"Kare Gennes\", \n                                                 \"Kare Gjonnes\", name ), \n                                   party = ifelse(name == \"Kare Gjonnes\", \n                                                  \"krf\", party), \n                                   birthyear = ifelse(name == \"Kare Gjonnes\", \n                                                       1942, birthyear), \n                                   deadyear = ifelse(name == \"Kare Gjonnes\", \n                                                     2021,deadyear),\n                                   age = year - birthyear) \n\n\nCoding variables based on multiple logical conditions using case_when()\nWe will often find ourselves creating or recoding variables based on multiple logical conditions. Suppose, for instance, that we wanted a categorical age variable with the values “30-39 years”, “40-49 years”, “50-59 years”, “60-69 years”.2\nThere are different ways in which we could create such a variable. In the past we have frequently relied on combining multiple ifelse() calls. We could write:\n\nwho_gov_norway_ministers &lt;- mutate(who_gov_norway_ministers, \n1                                   age_categorical = NA,\n2                                   age_categorical = ifelse(age &lt; 40,\n                                                            \"30-39 years\",\n                                                            age_categorical),\n3                                  age_categorical = ifelse(age&gt; 39 & age &lt; 50,\n                                                           \"40-49 years\",\n                                                           age_categorical),\n                                  age_categorical = ifelse(age &gt; 49 & age &lt; 60,\n                                                           \"50-59 years\",\n                                                           age_categorical),\n                                    age_categorical = ifelse(age &gt; 59 & age &lt; 70,\n                                                             \"60-69 years\",\n                                                             age_categorical))\n\n\n1\n\nWe first create a new variable containing only NAs.\n\n2\n\nWe replace values on the new variable for observations with age less than 40 and keep the existing values for the other observations\n\n3\n\nThen we sequentially replace the values using the appropriate logical conditions (and always keeping the existing values for the observations that don’t satisfy the condition).\n\n\n\n\nWe can get a quick sense of the variable, we created using table()\n\ntable(who_gov_norway_ministers$age_categorical)\n\n\n30-39 years 40-49 years 50-59 years 60-69 years \n        122         401         397          94 \n\n\nThe above code, works just fine, but this approach is a little cluttered. case_when() allows you to simplify this type of code. Let’s rewrite the same code, using case_when()\n\nwho_gov_norway_ministers &lt;- mutate(who_gov_norway_ministers, \n1                                   age_categorical = case_when(age &lt; 40 ~ \"30-39 years\",\n2                                                               age &lt; 50 ~ \"40-49 years\",\n3                                                               age &lt; 60 ~ \"50-59 years\",\n                                                               age &lt; 70 ~ \"60-69 years\"))\n\n\n1\n\nWe want a new variable called “age_categorical” to be created using case_when(). If age is less than 40 this variable should take the value “30-39 years”.\n\n2\n\nIf the above condition was not TRUE, then we move on to evaluate the next condition which is whether age is less than 50. If so, the variable should get the value “40-49 years”.\n\n3\n\nWe follow the same logic for the remaining two conditions. Note that because the conditions are evaluated sequentially, we don’t to specify e.g. age &gt; 39 & age &lt; 50. age &lt; 50 suffices as we have already taken care of the observations that are younger than 40.\n\n\n\n\n\n1table(who_gov_norway_ministers$age_categorical)\n\n\n1\n\nIf we compare the output of this table() with the one created above, we see that two code chunks produce the same variable. The case_when() code is just much simpler. Woot!\n\n\n\n\n\n30-39 years 40-49 years 50-59 years 60-69 years \n        122         401         397          94 \n\n\nIn the case_when() code above observations that don’t specify any of the conditions will get NAs on the variable that is created. Sometimes, we want to use case_when() to recode a variable for specific conditions and keep the original value for those observations that don’t match any of the conditions. Using the placeholder TRUE in the case_when() code, we can specify what happens with those observations that don’t satisfy any of the conditions. We could exploit this to simplify our code for correcting the coding of Einar Hole Moxnes and Kåre Gjønnes:\n\nwho_gov_norway_ministers &lt;- who_gov_norway_ministers %&gt;% \n  mutate(name = case_when(name == \"Moynes Einsi\" ~ \"Einar Hole Moxnes\", \n                          name == \"Kare Gennes\" ~ \"Kare Gjonnes\", \n1                          TRUE ~ name),\n         party  = case_when(name == \"Einar Hole Moxnes\" ~ \"sp\", \n                          name == \"Kare Gjonnes\" ~ \"krf\", \n                          TRUE ~ party ),\n          birthyear  = case_when(name == \"Einar Hole Moxnes\" ~ 1921, \n                          name == \"Kare Gjonnes\" ~ 1942, \n                          TRUE ~ birthyear ),\n          deadyear  = case_when(name == \"Einar Hole Moxnes\" ~ 2006, \n                          name == \"Kare Gjonnes\" ~ 2021, \n                          TRUE ~ birthyear ),\n         gender  = case_when(name == \"Einar Hole Moxnes\" ~ \"Male\", \n                          TRUE ~ gender ))\n\n\n1\n\nUsing TRUE ~ we can specify what happens to those observations that don’t satisfy any of the conditions. Otherwise those observations would get NAs."
  },
  {
    "objectID": "Wrangling/wrangling_in_dplyr_and_tidyr.html#using-pipes",
    "href": "Wrangling/wrangling_in_dplyr_and_tidyr.html#using-pipes",
    "title": "Wrangling in dplyr and tidyr",
    "section": "Using pipes (%>%)",
    "text": "Using pipes (%&gt;%)\nPipes (%&gt;%) allow you to combine a sequence of multiple operations, such as multiple dplyr verbs that need to be executed sequentially to produce the out we want. Pipes make our code easier to read and understand.\nIn RStudio, you can get a %&gt;% by pressing ctrl+shift+m/cmd+shift+m\nThe %&gt;% operator comes from the magrittr, but is automatically loaded with all tidyverse packages such as dplyr and using pipes is particularly useful in data wrangling as we typically will perform a long chain of operations to produce the dataset that we need for our analysis.\nUsing pipes, we can perform all the operations we did to produce who_gov_norway_ministers like this:\n\n1who_gov_norway_ministers &lt;- who_gov %&gt;%\n2  filter(country_isocode == \"NOR\",\n         classification == \"Prime Minister\" |\n           classification == \"Minister (Full Rank)\") %&gt;%\n3  select(year, id, position, name,\n         gender,  birthyear, deadyear, party, classification) %&gt;%\n4  rename(minister_type = classification) %&gt;%\n5  mutate(name = case_when(name == \"Moynes Einsi\" ~ \"Einar Hole Moxnes\",\n                          name == \"Kare Gennes\" ~ \"Kare Gjonnes\",\n                          TRUE ~ name),\n         party  = case_when(name == \"Einar Hole Moxnes\" ~ \"sp\", \n                          name == \"Kare Gjonnes\" ~ \"krf\",\n                          TRUE ~ party ),\n          birthyear  = case_when(name == \"Einar Hole Moxnes\" ~ 1921,\n                          name == \"Kare Gjonnes\" ~ 1942,\n                          TRUE ~ birthyear ),\n          deadyear  = case_when(name == \"Einar Hole Moxnes\" ~ 2006,\n                          name == \"Kare Gjonnes\" ~ 2021,\n                          TRUE ~ birthyear ),\n         gender  = case_when(name == \"Einar Hole Moxnes\" ~ \"Male\",\n                          TRUE ~ gender),\n         age = year - birthyear,\n         age_categorical = case_when(age &lt; 40 ~ \"30-39 years\",\n                                     age &lt; 50 ~ \"40-49 years\",\n                                     age &lt; 60 ~ \"50-59 years\",\n                                     age &lt; 70 ~ \"60-69 years\"))\n\n\n1\n\nWe start a sequence of operations with a data.frame, here who_gov. We may choose to assign the result of the sequence of operation that follows to an object. Here we assign the result to who_gov_norway_ministers. We end the line with a %&gt;% which tells R that more commands will follow to produce our desired result.\n\n2\n\nWe then filter() the data. We don’t need to specify the dataset in filter() (or in any of the subsequent operations) because R will continue to use the data from the line above in the sequence of operations. We end the line with a %&gt;% to tell R that more functions are coming.\n\n3\n\nWe then select()the columns we want and end the line with a %&gt;% to let R know that more is coming.\n\n4\n\nWe thenrename().\n\n5\n\nFinally, we use mutate() to calculate new variables and recode existing ones. Because the line doesn’t end in a %&gt;%, the sequence of operations ends here.\n\n\n\n\nWe think that pipes are great for making sequences of operations performed on the same input data more readable! And now that you have seen how they work, we will be using them a lot in the what follows!\n\n\n\n\n\n\n%&gt;% vs. |&gt;\n\n\n\nPipes were introduced to R with the magrittr package which is where the %&gt;% operator comes from. Pipes proved immensely popular and in version 4.1 of R (released in 2021), a pipe operator (|&gt;) was included in base R.\nFor the most part, the “native” pipe operator, |&gt;, works like the %&gt;% operator, but there are some important exceptions, which you can read more about here.\nWe will be sticking to the %&gt;% operator and this course, but the |&gt; operator is still useful to be aware of as you are likely to encounter it in code written by others."
  },
  {
    "objectID": "Wrangling/wrangling_in_dplyr_and_tidyr.html#grouping-data-using-group_by",
    "href": "Wrangling/wrangling_in_dplyr_and_tidyr.html#grouping-data-using-group_by",
    "title": "Wrangling in dplyr and tidyr",
    "section": "Grouping data using group_by()",
    "text": "Grouping data using group_by()\nWe often have datasets in which the observations are grouped in some way.\nIn who_gov_norway_ministers there are multiple observations of the same people (identified by who_gov_norway_ministers$name), multiple observations from the same year (identified by who_gov_norway_ministers$year), multiple observations of the same positions (identified by who_gov_norway_ministers$position), and so on. Perhaps, we want to create a new variable capturing which cabinet each observation was part of, thus creating a new potential grouping variable.\nOften we want to calculations within each group. For instance, we may want to calculate the age of each person when they first achieved a position in a Norwegian cabinet.3 To do that we first need to group_by(name). After we have grouped the data, we can do operations on the data within each “group”.\n\nwho_gov_norway_ministers &lt;- who_gov_norway_ministers %&gt;% \n  group_by(name) %&gt;% \n  mutate(age_when_first_entering_a_cabinet = min(age, na.rm = TRUE))\n\nIf we print out the first few rows on selected variables, we will see that who_gov_norway_ministers$age_when_first_entering_a_cabinet varies by who_gov_norway_ministers$name, but is constant within each who_gov_norway_ministers$name:\n\nwho_gov_norway_ministers %&gt;% \n  select(name, year, age, age_when_first_entering_a_cabinet) %&gt;% \n  head(20)\n\n# A tibble: 20 × 4\n# Groups:   name [15]\n   name                         year   age age_when_first_entering_a_cabinet\n   &lt;chr&gt;                       &lt;dbl&gt; &lt;dbl&gt;                             &lt;dbl&gt;\n 1 Per Borten                   1966    53                                53\n 2 Bjarne Lyngstad              1966    65                                65\n 3 Kjell Bondevik               1966    65                                65\n 4 Kaare Willoch                1966    38                                38\n 5 Haakon Kyllingmark           1966    51                                51\n 6 Otto Grieg Tidemand          1966    45                                45\n 7 Ole Myrvoll                  1966    55                                55\n 8 Oddmund Myklebust            1966    51                                51\n 9 John Lyng                    1966    61                                61\n10 Sverre Walter Rostoft        1966    54                                54\n11 Ragnhild Schweigaard Selmer  1966    43                                43\n12 Helge Seip                   1966    47                                47\n13 Egil Aarvik                  1966    54                                54\n14 Dagfinn Varvik               1966    42                                42\n15 Else Skjerven                1966    47                                47\n16 Per Borten                   1967    54                                53\n17 Bjarne Lyngstad              1967    66                                65\n18 Kjell Bondevik               1967    66                                65\n19 Kaare Willoch                1967    39                                38\n20 Haakon Kyllingmark           1967    52                                51\n\n\nWhen we are done with applying functions within a particular group, it is useful to ungroup() the data so that R will not apply subsequent functions within the group. Running the following code will remove the grouping of the dataset.\n\nwho_gov_norway_ministers &lt;- who_gov_norway_ministers %&gt;% \n  ungroup()"
  },
  {
    "objectID": "Wrangling/wrangling_in_dplyr_and_tidyr.html#summarize",
    "href": "Wrangling/wrangling_in_dplyr_and_tidyr.html#summarize",
    "title": "Wrangling in dplyr and tidyr",
    "section": "summarize()",
    "text": "summarize()\nSuppose we are interested in the share of female ministers each year. We then need first to group_by(year). we could use mutate() to add new variable with the share of female ministers each year to the who_gov_norway_ministers data, but this might not be those most convenient format, if we want to report the annual shares in a table or figure. Instead we want to summarize():\n\n1share_female_ministers &lt;- who_gov_norway_ministers %&gt;%\n2  group_by(year) %&gt;%\n3  summarize(share_female_ministers = mean(gender == \"Female\", na.rm = TRUE))\n \n\ntable(who_gov$year) \n\n\n1\n\nWe want to use who_gov_norway_ministers as our input data.frame, but we will assign the result to a new object share_female_ministers\n\n2\n\nWe group_by(year) to the operations that follow within each year.\n\n3\n\nsummarize() will calculate the share of female ministers each year and aggregate the data to have one row per year. The only columns will be the grouping variable (year) and the variable(s) we create inside the summary call.\n\n\n\n\n\n1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 \n  14   14   14 2767 2743 2809 2879 2961 3086 3217 3290 3314 3495 3534 3640 3687 \n1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 \n3782 3813 3955 3975 4153 4187 4100 4167 4215 4203 4216 4074 4027 4331 4671 4852 \n1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 \n5040 5236 5242 5191 5239 5229 5375 5348 5233 5177 5115 5175 5255 5211 5289 5332 \n2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 \n5272 5353 5366 5329 5271 5247 5242 5280 5243 5249 5233 \n\n\nThe resulting data.frame looks like this:\n\nprint(share_female_ministers, n = nrow(share_female_ministers))\n\n# A tibble: 56 × 2\n    year share_female_ministers\n   &lt;dbl&gt;                  &lt;dbl&gt;\n 1  1966                 0.133 \n 2  1967                 0.133 \n 3  1968                 0.133 \n 4  1969                 0.133 \n 5  1970                 0.133 \n 6  1971                 0.0667\n 7  1972                 0.0667\n 8  1973                 0.2   \n 9  1974                 0.2   \n10  1975                 0.188 \n11  1976                 0.25  \n12  1977                 0.25  \n13  1978                 0.235 \n14  1979                 0.25  \n15  1980                 0.125 \n16  1981                 0.25  \n17  1982                 0.235 \n18  1983                 0.222 \n19  1984                 0.222 \n20  1985                 0.222 \n21  1986                 0.444 \n22  1987                 0.444 \n23  1988                 0.444 \n24  1989                 0.444 \n25  1990                 0.421 \n26  1991                 0.474 \n27  1992                 0.474 \n28  1993                 0.421 \n29  1994                 0.421 \n30  1995                 0.421 \n31  1996                 0.421 \n32  1997                 0.45  \n33  1998                 0.5   \n34  1999                 0.4   \n35  2000                 0.421 \n36  2001                 0.421 \n37  2002                 0.421 \n38  2003                 0.421 \n39  2004                 0.421 \n40  2005                 0.421 \n41  2006                 0.474 \n42  2007                 0.474 \n43  2008                 0.429 \n44  2009                 0.45  \n45  2010                 0.476 \n46  2011                 0.476 \n47  2012                 0.5   \n48  2013                 0.524 \n49  2014                 0.5   \n50  2015                 0.5   \n51  2016                 0.444 \n52  2017                 0.45  \n53  2018                 0.45  \n54  2019                 0.45  \n55  2020                 0.4   \n56  2021                 0.4   \n\n\nWe might want more than one summary statistics per year. For instance, we might want to add the count of female ministers, and the count of ministers in total and the share of ministers from each of the different parties:\n\n1shares_and_counts &lt;- who_gov_norway_ministers %&gt;%\n2  group_by(year) %&gt;%\n3  summarize(share_female_ministers = mean(gender == \"Female\", na.rm = TRUE),\n            count_female_ministers = sum(gender == \"Female\", na.rm = TRUE),\n4            count_ministers = n(),\n            share_dna = mean(party == \"dna\", na.rm = TRUE),\n            share_hoyre= mean(party == \"h\", na.rm = TRUE), \n                       share_sp= mean(party == \"sp\", na.rm = TRUE), \n               share_venstre = mean(party == \"v\", na.rm = TRUE), \n            share_frp = mean(party == \"fr\", na.rm = TRUE), \n            share_sv = mean(party == \"sv\", na.rm = TRUE), \n            share_krf = mean(party == \"krf\", na.rm = TRUE)) \n\nprint(shares_and_counts)\n\n\n1\n\nWe use who_gov_norway_ministers as the input data, but we don’t assign the result to a new object. The result will then instead be printed in the console.\n\n2\n\nWe group the data by year.\n\n3\n\nWe now calculate multiple summary statistics for each year.\n\n4\n\nThe n() function can be used to find the total number of observations within a group.\n\n\n\n\n# A tibble: 56 × 11\n    year share_female_ministers count_female_ministers count_ministers share_dna\n   &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;int&gt;           &lt;int&gt;     &lt;dbl&gt;\n 1  1966                 0.133                       2              15         0\n 2  1967                 0.133                       2              15         0\n 3  1968                 0.133                       2              15         0\n 4  1969                 0.133                       2              15         0\n 5  1970                 0.133                       2              15         0\n 6  1971                 0.0667                      1              15         1\n 7  1972                 0.0667                      1              15         1\n 8  1973                 0.2                         3              15         0\n 9  1974                 0.2                         3              15         1\n10  1975                 0.188                       3              16         1\n# ℹ 46 more rows\n# ℹ 6 more variables: share_hoyre &lt;dbl&gt;, share_sp &lt;dbl&gt;, share_venstre &lt;dbl&gt;,\n#   share_frp &lt;dbl&gt;, share_sv &lt;dbl&gt;, share_krf &lt;dbl&gt;\n\n\nYou can also use summarize() with a group. If so, summarize() will calculate the summary statistics for the full dataset. For instance we can get the share of female ministers in the who_gov_norway_ministers as a whole like this:\n\nwho_gov_norway_ministers %&gt;% \n  summarize(share_female_ministers = mean(gender == \"Female\", na.rm = TRUE))\n\n# A tibble: 1 × 1\n  share_female_ministers\n                   &lt;dbl&gt;\n1                  0.365\n\n\nThe above code produces a single number because the share is calculated across all the observations in the dataset."
  },
  {
    "objectID": "Wrangling/wrangling_in_dplyr_and_tidyr.html#mutate-or-summarize-across-multiple-columns",
    "href": "Wrangling/wrangling_in_dplyr_and_tidyr.html#mutate-or-summarize-across-multiple-columns",
    "title": "Wrangling in dplyr and tidyr",
    "section": "mutate() or summarize() across() multiple columns",
    "text": "mutate() or summarize() across() multiple columns\nWe often need to the the same mutate() or summarize() operations across() multiple columns. We across() and the helper functions for select() to do the same operation for a defined set of columns. Suppose for instance that we wanted to recode all the variables with “shares” in the shares_and_counts data.frame we created above to percentages, but we wanted to keep the “counts” unchanged. Combining across() and an appropriate select() helper functions makes this straightforward:\n\npercentages_and_counts &lt;- shares_and_counts %&gt;% \n1  mutate(across(.cols = contains(\"share\"),\n2                .fns = function (x) {\n3                  x *100\n                }))\n\nprint(percentages_and_counts)\n\n\n1\n\nWe use across() inside mutate() (we may also use it inside summarize()). Using the .cols argument and the select() helper function contains() we specify that we want to do something with all the columns with “share” in the column name.\n\n2\n\nFor the .fns argument we specify the function that should be applied across the selected columns. We may give the name of an existing function or we may define a function using function(). Our knowledge of how to define our own function now comes in handy!\n\n3\n\nOur function is in this case very simple. It takes the value x and multiplies it with 100. This will turn our shares into percentages.\n\n\n\n\n# A tibble: 56 × 11\n    year share_female_ministers count_female_ministers count_ministers share_dna\n   &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;int&gt;           &lt;int&gt;     &lt;dbl&gt;\n 1  1966                  13.3                       2              15         0\n 2  1967                  13.3                       2              15         0\n 3  1968                  13.3                       2              15         0\n 4  1969                  13.3                       2              15         0\n 5  1970                  13.3                       2              15         0\n 6  1971                   6.67                      1              15       100\n 7  1972                   6.67                      1              15       100\n 8  1973                  20                         3              15         0\n 9  1974                  20                         3              15       100\n10  1975                  18.8                       3              16       100\n# ℹ 46 more rows\n# ℹ 6 more variables: share_hoyre &lt;dbl&gt;, share_sp &lt;dbl&gt;, share_venstre &lt;dbl&gt;,\n#   share_frp &lt;dbl&gt;, share_sv &lt;dbl&gt;, share_krf &lt;dbl&gt;"
  },
  {
    "objectID": "Wrangling/wrangling_in_dplyr_and_tidyr.html#arrange",
    "href": "Wrangling/wrangling_in_dplyr_and_tidyr.html#arrange",
    "title": "Wrangling in dplyr and tidyr",
    "section": "arrange()",
    "text": "arrange()\nWe can change the order of the rows in the dataset by using arrange(). For instance, let’s say we wanted the rows in our share_female_ministers dataset to be sorted not by year, but by the share of female ministers. We would then then:\n\nshare_female_ministers &lt;- share_female_ministers %&gt;% \n  arrange(share_female_ministers)\n\nIf we print out share_female_ministers, the data.frame will now be sorted by the values on share_female_ministers$share_female_ministers:\n\nprint(share_female_ministers, n = 33)\n\n# A tibble: 56 × 2\n    year share_female_ministers\n   &lt;dbl&gt;                  &lt;dbl&gt;\n 1  1971                 0.0667\n 2  1972                 0.0667\n 3  1980                 0.125 \n 4  1966                 0.133 \n 5  1967                 0.133 \n 6  1968                 0.133 \n 7  1969                 0.133 \n 8  1970                 0.133 \n 9  1975                 0.188 \n10  1973                 0.2   \n11  1974                 0.2   \n12  1983                 0.222 \n13  1984                 0.222 \n14  1985                 0.222 \n15  1978                 0.235 \n16  1982                 0.235 \n17  1976                 0.25  \n18  1977                 0.25  \n19  1979                 0.25  \n20  1981                 0.25  \n21  1999                 0.4   \n22  2020                 0.4   \n23  2021                 0.4   \n24  1990                 0.421 \n25  1993                 0.421 \n26  1994                 0.421 \n27  1995                 0.421 \n28  1996                 0.421 \n29  2000                 0.421 \n30  2001                 0.421 \n31  2002                 0.421 \n32  2003                 0.421 \n33  2004                 0.421 \n# ℹ 23 more rows\n\n\nBy default, the data will be arranged in ascending order (with the lowest values first). To instead arrange() the data in descending order (with the highest values first), we can use desc():\n\nshare_female_ministers &lt;- share_female_ministers %&gt;% \n  arrange(desc(share_female_ministers))\nprint(share_female_ministers, n = 33)\n\n# A tibble: 56 × 2\n    year share_female_ministers\n   &lt;dbl&gt;                  &lt;dbl&gt;\n 1  2013                  0.524\n 2  1998                  0.5  \n 3  2012                  0.5  \n 4  2014                  0.5  \n 5  2015                  0.5  \n 6  2010                  0.476\n 7  2011                  0.476\n 8  1991                  0.474\n 9  1992                  0.474\n10  2006                  0.474\n11  2007                  0.474\n12  1997                  0.45 \n13  2009                  0.45 \n14  2017                  0.45 \n15  2018                  0.45 \n16  2019                  0.45 \n17  1986                  0.444\n18  1987                  0.444\n19  1988                  0.444\n20  1989                  0.444\n21  2016                  0.444\n22  2008                  0.429\n23  1990                  0.421\n24  1993                  0.421\n25  1994                  0.421\n26  1995                  0.421\n27  1996                  0.421\n28  2000                  0.421\n29  2001                  0.421\n30  2002                  0.421\n31  2003                  0.421\n32  2004                  0.421\n33  2005                  0.421\n# ℹ 23 more rows"
  },
  {
    "objectID": "Wrangling/wrangling_in_dplyr_and_tidyr.html#distinct",
    "href": "Wrangling/wrangling_in_dplyr_and_tidyr.html#distinct",
    "title": "Wrangling in dplyr and tidyr",
    "section": "distinct()",
    "text": "distinct()\ndistinct() is useful for making sure that observations in the data are not duplicated. Let’s say we want to produce a list of all the prime ministers in Norway. We could start by using filter() to subset the data to prime ministers.\n\nwho_gov_norway_ministers %&gt;% \n  filter(minister_type == \"Prime Minister\")\n\n# A tibble: 56 × 12\n    year    id position      name  gender birthyear deadyear party minister_type\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;        \n 1  1966     2 Prime Minist… Per … Male        1913     1913 sp    Prime Minist…\n 2  1967    18 Prime Minist… Per … Male        1913     1913 sp    Prime Minist…\n 3  1968    34 Prime Minist… Per … Male        1913     1913 sp    Prime Minist…\n 4  1969    50 Prime Minist… Per … Male        1913     1913 sp    Prime Minist…\n 5  1970    66 Prime Minist… Per … Male        1913     1913 sp    Prime Minist…\n 6  1971    82 Prime Minist… Tryg… Male        1910     1910 dna   Prime Minist…\n 7  1972    98 Prime Minist… Tryg… Male        1910     1910 dna   Prime Minist…\n 8  1973   114 Prime Minist… Lars… Male        1916     1916 krf   Prime Minist…\n 9  1974   130 Prime Minist… Tryg… Male        1910     1910 dna   Prime Minist…\n10  1975   146 Prime Minist… Tryg… Male        1910     1910 dna   Prime Minist…\n# ℹ 46 more rows\n# ℹ 3 more variables: age &lt;dbl&gt;, age_categorical &lt;chr&gt;,\n#   age_when_first_entering_a_cabinet &lt;dbl&gt;\n\n\nThe above code subsets to the prime ministers, but we now get repeated observations of the prime ministers (one row for each year they were in office). We can use distinct() to only get one observation per who_gov_norway_ministers$name like this:\n\nwho_gov_norway_ministers %&gt;% \n  filter(minister_type == \"Prime Minister\") %&gt;% \n  distinct(name)\n\n# A tibble: 11 × 1\n   name                 \n   &lt;chr&gt;                \n 1 Per Borten           \n 2 Trygve Bratteli      \n 3 Lars Korvald         \n 4 Odvar Nordli         \n 5 Gro Harlem Brundtland\n 6 Kaare Willoch        \n 7 Jan P. Syse          \n 8 Thorbjorn Jagland    \n 9 Kjell Magne Bondevik \n10 Jens Stoltenberg     \n11 Erna Solberg         \n\n\nWe now only have one observation per person, but we lost all of the other variables. We can change this behavior in two different ways, first, we can add the other variables we want (and which do not vary within each prime minister) inside the distinct() call:\n\nwho_gov_norway_ministers %&gt;% \n  filter(minister_type == \"Prime Minister\") %&gt;% \n  distinct(name, birthyear, deadyear, party)\n\n# A tibble: 11 × 4\n   name                  birthyear deadyear party\n   &lt;chr&gt;                     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;\n 1 Per Borten                 1913     1913 sp   \n 2 Trygve Bratteli            1910     1910 dna  \n 3 Lars Korvald               1916     1916 krf  \n 4 Odvar Nordli               1927     1927 dna  \n 5 Gro Harlem Brundtland      1939     1939 dna  \n 6 Kaare Willoch              1928     1928 h    \n 7 Jan P. Syse                1930     1930 h    \n 8 Thorbjorn Jagland          1950     1950 dna  \n 9 Kjell Magne Bondevik       1947     1947 krf  \n10 Jens Stoltenberg           1959     1959 dna  \n11 Erna Solberg               1961     1961 h    \n\n\nAlternatively, we could say that we want to .keep_all the variables:\n\nwho_gov_norway_ministers %&gt;% \n  filter(minister_type == \"Prime Minister\") %&gt;% \n  distinct(name, .keep_all = TRUE)\n\n# A tibble: 11 × 12\n    year    id position      name  gender birthyear deadyear party minister_type\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;        \n 1  1966     2 Prime Minist… Per … Male        1913     1913 sp    Prime Minist…\n 2  1971    82 Prime Minist… Tryg… Male        1910     1910 dna   Prime Minist…\n 3  1973   114 Prime Minist… Lars… Male        1916     1916 krf   Prime Minist…\n 4  1976   163 Prime Minist… Odva… Male        1927     1927 dna   Prime Minist…\n 5  1981   249 Prime Minist… Gro … Female      1939     1939 dna   Prime Minist…\n 6  1982   266 Prime Minist… Kaar… Male        1928     1928 h     Prime Minist…\n 7  1990   423 Prime Minist… Jan … Male        1930     1930 h     Prime Minist…\n 8  1997   575 Prime Minist… Thor… Male        1950     1950 dna   Prime Minist…\n 9  1998   599 Prime Minist… Kjel… Male        1947     1947 krf   Prime Minist…\n10  2000   647 Prime Minist… Jens… Male        1959     1959 dna   Prime Minist…\n11  2014   979 Prime Min.    Erna… Female      1961     1961 h     Prime Minist…\n# ℹ 3 more variables: age &lt;dbl&gt;, age_categorical &lt;chr&gt;,\n#   age_when_first_entering_a_cabinet &lt;dbl&gt;"
  },
  {
    "objectID": "Wrangling/wrangling_in_dplyr_and_tidyr.html#joining-also-called-merging-data",
    "href": "Wrangling/wrangling_in_dplyr_and_tidyr.html#joining-also-called-merging-data",
    "title": "Wrangling in dplyr and tidyr",
    "section": "Joining (also called “merging”) data",
    "text": "Joining (also called “merging”) data\nWe can use mutating joins, to combine datasets that share keys or ID-variables, meaning one or more variables that uniquely identify the observations at the level on which they will be joined.\nIn some cases, you will only need a single variable to have your key. For instance, if you are joining two datasets that have one row per country, you my join the two datasets if you have one or more variables uniquely identifying each country in both dataset. If you are joining two datasets both containing one row per country per year, you may use two variables to uniquely identify each country and each year in both datasets (or alternatively, you may have a “country-year” variable in both datasets).\n\n\n\n\n\n\nThe key needs to be consistent across the datasets!\n\n\n\nNote that the variables need to be identical across the datasets you want to join. If you have the value “Ivory Coast” on the country variable in one of the datasets and “Côte d’Ivoire” in the other dataset, R will not know that these observations are of the same country. For this reason, it is useful to use one of the many standardized ID variables for your countries, such as the Gleditsch and Ward (1999) country codes or perhaps one of the country code standards from the International Organization for Standardization.\nOf course, this advice also applies to other types of keys. If we want to match two datasets where the unit of observations are individuals (such as government ministers, members of parliament, or judges) we would need a variable that uniquely identifies each individual in both datasets.\n\n\n\n\n\n\n\n\nUse the countrycode package to convert country names to country codes and translate between different country codes\n\n\n\nArel-Bundock, Enevoldsen, and Yetman (2018) provide an excellent resource for students and scholars working with country data. The countrycode() function from their countrycode package converts country names into various types of country codes and translates between different types of country codes.\nTo illustrate, let’s generate country codes that correspond to the country codes used by the Varieties of Democracy project for all observations in the who_gov dataset:\n\nlibrary(countrycode)\nwho_gov &lt;- who_gov %&gt;% \n1  mutate(vdem_countrycode = countrycode(sourcevar = country_name,\n2                            origin = \"country.name\",\n3                            destination = \"vdem\"))\n\n\n1\n\nWe need to specify sourcevar the variable used to generate the country codes. Often this will be the country names, but it may also be one of the different types of country codes if we need to translate between different standards.\n\n2\n\nWe need to specify how the source.var is coded. We set origin to “country.name” if we are generating the codes from country names. When translating between different country codes, we would set origin to whatever standard we are translating from.\n\n3\n\nWe supply the label for the type of country code we would like to generate to destination. You can get a list of the different standards and their labels by running ?countrycode::codelist in your console.\n\n\n\n\nNotice that the above code produces a warning message about country names that could not be unambiguously matched to a Varieties of Democracy country code. These observations will get NAs on the new who_gov$vdem_countrycode. To avoid losing them from your analysis, it would be useful to either set their Varieties of Democracy country code manually or to recode the whogov$country_name variable so that also these country names can be matched.\n\n\nLet’s consider an example. Say, we want investigate the relationship between female representation in Norwegian cabinets and female representation in the Norwegian parliament. We already used WhoGov to investigate female representation in Norwegian cabinets.4 Let’s also load some data on female representation in Norwegian parliament.5\n\nload(\"../data/stortinget_female_share_by_period.RData\")\n\nprint(stortinget_female_share_by_period, n = 20)\n\n# A tibble: 20 × 2\n   period_id share_female_mps\n   &lt;chr&gt;                &lt;dbl&gt;\n 1 1945-49             0.0467\n 2 1950-53             0.0467\n 3 1954-57             0.0464\n 4 1958-61             0.0667\n 5 1961-65             0.0867\n 6 1965-69             0.08  \n 7 1969-73             0.0933\n 8 1973-77             0.153 \n 9 1977-81             0.239 \n10 1981-85             0.258 \n11 1985-89             0.344 \n12 1989-93             0.358 \n13 1993-97             0.394 \n14 1997-2001           0.364 \n15 2001-2005           0.364 \n16 2005-2009           0.379 \n17 2009-2013           0.396 \n18 2013-2017           0.396 \n19 2017-2021           0.408 \n20 2021-2025           0.450 \n\n\nThe dataset stortinget_female_share_by_period contains the share of members of parliament that were female for each legislative period from the “1945-49” period to the “2021-2025” period.\nIf we are to join the stortinget_female_share_by_period with data on female representation in Norwegian cabinets from WhoGov, we need a key that uniquely identifies each observation in both datasets. In this case, we will need to make some choices as stortinget_female_share_by_period is measured at the level of the legislative period, while who_gov contains annual data on the composition of the cabinet.\nOne solution would be to create a legislative period variable in our share_female_ministers dataset. We already know how to do that using mutate() and case_when().6\n\nshare_female_ministers &lt;- share_female_ministers %&gt;% \n  mutate(period_id = case_when(year &lt; 1974 ~ \"1969-73\", \n                               year &lt; 1978 ~ \"1973-77\", \n                               year &lt; 1982 ~ \"1977-81\", \n                               year &lt; 1986 ~ \"1981-85\", \n                               year &lt; 1990 ~ \"1985-89\", \n                               year &lt; 1994 ~ \"1989-93\", \n                               year &lt; 1998 ~ \"1993-97\", \n                               year &lt; 2002 ~ \"1997-2001\", \n                               year &lt; 2006 ~ \"2001-2005\", \n                               year &lt; 2010 ~ \"2005-2009\",\n                               year &lt; 2014 ~ \"2009-2013\",\n                               year &lt; 2018 ~ \"2013-2017\", \n                               year &lt; 2022 ~ \"2017-2021\"))\n\nWe now have a variable period_id in both share_female_ministers and stortinget_female_share_by_period. This variable uniquely identifies each observation in stortinget_female_share_by_period, but not in stortinget_female_share_by_period. We can use this key to join data at the level of the legislative period.\n\nleft_join()\nThere are different types of joins, but left_join() is likely the one you will use the most. left_join() takes two dataset and join observations from the second dataset to the first dataset (i.e to the dataset on the left in our left_join() code.) by the key they have in common. Let’s join stortinget_female_share_by_period to share_female_ministers.\n\n1share_female_ministers_and_mps &lt;- left_join(share_female_ministers, stortinget_female_share_by_period,\n2                                            by = \"period_id\" )\n\n\n1\n\nWe use left_join() and specify the two datasets. Variables from the second dataset will be added to corresponding observations in the first dataset.\n\n2\n\nThe variable to join by.\n\n\n\n\nThis is the resulting data.frame:\n\nprint(share_female_ministers_and_mps, n = nrow(share_female_ministers_and_mps))\n\n# A tibble: 56 × 4\n    year share_female_ministers period_id share_female_mps\n   &lt;dbl&gt;                  &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;\n 1  2013                 0.524  2009-2013           0.396 \n 2  1998                 0.5    1997-2001           0.364 \n 3  2012                 0.5    2009-2013           0.396 \n 4  2014                 0.5    2013-2017           0.396 \n 5  2015                 0.5    2013-2017           0.396 \n 6  2010                 0.476  2009-2013           0.396 \n 7  2011                 0.476  2009-2013           0.396 \n 8  1991                 0.474  1989-93             0.358 \n 9  1992                 0.474  1989-93             0.358 \n10  2006                 0.474  2005-2009           0.379 \n11  2007                 0.474  2005-2009           0.379 \n12  1997                 0.45   1993-97             0.394 \n13  2009                 0.45   2005-2009           0.379 \n14  2017                 0.45   2013-2017           0.396 \n15  2018                 0.45   2017-2021           0.408 \n16  2019                 0.45   2017-2021           0.408 \n17  1986                 0.444  1985-89             0.344 \n18  1987                 0.444  1985-89             0.344 \n19  1988                 0.444  1985-89             0.344 \n20  1989                 0.444  1985-89             0.344 \n21  2016                 0.444  2013-2017           0.396 \n22  2008                 0.429  2005-2009           0.379 \n23  1990                 0.421  1989-93             0.358 \n24  1993                 0.421  1989-93             0.358 \n25  1994                 0.421  1993-97             0.394 \n26  1995                 0.421  1993-97             0.394 \n27  1996                 0.421  1993-97             0.394 \n28  2000                 0.421  1997-2001           0.364 \n29  2001                 0.421  1997-2001           0.364 \n30  2002                 0.421  2001-2005           0.364 \n31  2003                 0.421  2001-2005           0.364 \n32  2004                 0.421  2001-2005           0.364 \n33  2005                 0.421  2001-2005           0.364 \n34  1999                 0.4    1997-2001           0.364 \n35  2020                 0.4    2017-2021           0.408 \n36  2021                 0.4    2017-2021           0.408 \n37  1976                 0.25   1973-77             0.153 \n38  1977                 0.25   1973-77             0.153 \n39  1979                 0.25   1977-81             0.239 \n40  1981                 0.25   1977-81             0.239 \n41  1978                 0.235  1977-81             0.239 \n42  1982                 0.235  1981-85             0.258 \n43  1983                 0.222  1981-85             0.258 \n44  1984                 0.222  1981-85             0.258 \n45  1985                 0.222  1981-85             0.258 \n46  1973                 0.2    1969-73             0.0933\n47  1974                 0.2    1973-77             0.153 \n48  1975                 0.188  1973-77             0.153 \n49  1966                 0.133  1969-73             0.0933\n50  1967                 0.133  1969-73             0.0933\n51  1968                 0.133  1969-73             0.0933\n52  1969                 0.133  1969-73             0.0933\n53  1970                 0.133  1969-73             0.0933\n54  1980                 0.125  1977-81             0.239 \n55  1971                 0.0667 1969-73             0.0933\n56  1972                 0.0667 1969-73             0.0933\n\n\nNotice that the new data.frame called share_female_ministers_and_mps will have the same number of rows as share_female_ministers. It is good idea to check that this is indeed the case. We can verify using\n\nnrow(share_female_ministers) == nrow(share_female_ministers_and_mps)\n\n[1] TRUE\n\n\nIf the above statement had evaluated to FALSE, it would have indicated a problem with our key resulting in the duplication of some rows. We would have wanted to investigate and correct our code before proceeding!\nLet’s say we instead wanted to left_join share_female_ministers to stortinget_female_share_by_period using the first year of the legislative period as they key. We can add a year variable to our stortinget_female_share_by_period like this:\n\nstortinget_female_share_by_period &lt;- stortinget_female_share_by_period %&gt;% \n1  mutate(year = substr(period_id, 1,4),\n2         year = as.integer(year),\n3         year = year +1)\n\n\n1\n\nsubstr(period_id, 1,4) extracts the first 4 characters from each element of period_id, so for “1969-73” it extracts “1969”, for “2005-2009” it extracts “2005”, and so on.\n\n2\n\nThe extracted years will be character strings, so we need to turn them into integers, which we can do using as.integer().\n\n3\n\nWe add 1 to the start of each year to account for how the legislative sessions all start in October, while the WhoGov data are updated each July.\n\n\n\n\nWe can now left_join stortinget_female_share_by_period and share_female_ministers using year as the key:\n\nshare_female_ministers_and_mps_by_year &lt;- left_join(stortinget_female_share_by_period, share_female_ministers, \n                                             by = \"year\")\n\nnrow(share_female_ministers_and_mps_by_year) == nrow(stortinget_female_share_by_period)\n\n[1] TRUE\n\n\nThat worked. But let’s look at the result:\n\nprint(share_female_ministers_and_mps_by_year, n = nrow(share_female_ministers_and_mps_by_year))\n\n# A tibble: 20 × 5\n   period_id.x share_female_mps  year share_female_ministers period_id.y\n   &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt;                  &lt;dbl&gt; &lt;chr&gt;      \n 1 1945-49               0.0467  1946                 NA     &lt;NA&gt;       \n 2 1950-53               0.0467  1951                 NA     &lt;NA&gt;       \n 3 1954-57               0.0464  1955                 NA     &lt;NA&gt;       \n 4 1958-61               0.0667  1959                 NA     &lt;NA&gt;       \n 5 1961-65               0.0867  1962                 NA     &lt;NA&gt;       \n 6 1965-69               0.08    1966                  0.133 1969-73    \n 7 1969-73               0.0933  1970                  0.133 1969-73    \n 8 1973-77               0.153   1974                  0.2   1973-77    \n 9 1977-81               0.239   1978                  0.235 1977-81    \n10 1981-85               0.258   1982                  0.235 1981-85    \n11 1985-89               0.344   1986                  0.444 1985-89    \n12 1989-93               0.358   1990                  0.421 1989-93    \n13 1993-97               0.394   1994                  0.421 1993-97    \n14 1997-2001             0.364   1998                  0.5   1997-2001  \n15 2001-2005             0.364   2002                  0.421 2001-2005  \n16 2005-2009             0.379   2006                  0.474 2005-2009  \n17 2009-2013             0.396   2010                  0.476 2009-2013  \n18 2013-2017             0.396   2014                  0.5   2013-2017  \n19 2017-2021             0.408   2018                  0.45  2017-2021  \n20 2021-2025             0.450   2022                 NA     &lt;NA&gt;       \n\n\nThere are several things to note:\n\nWe no longer retain the yearly observations for the share_female_ministers variables. We retain only those years that exist in stortinget_female_share_by_period$year. If we wanted a dataset on the legislative period level, it would arguably have been better to create the period_id variable in the who_gov_norway_ministers dataset and calculate the share of female ministers within each period (this would again create additional questions concerning for instance if we wanted to adjust for how long each minister was in office. These things are not as straightforward as we might wish!).\nWe keep the periods for which there are no observations on share_female_ministers. left_join() will retain all observations in the “left” dataset. These observations get NA values on the variables from the dataset “to the right”.\nWe duplicated the period_id variable. Since this variable was present in both datasets and we didn’t use it as a key, left_join() has retained a copy of each and named them period_id.x and period_id.y. We could have avoided this by including the variable as part of the key (by = c(\"year\", \"period_id\") or by omitting this variable from one of the dataset before joining them.\n\n\n\n\n\n\n\nright_join()\n\n\n\nThere is, of course, also a right_join() which works exactly like left_join, except that it will keep all the observations in the second dataset (“the dataset to the right”) and add variables from the first dataset to this second dataset.\n\n\n\n\ninner_join()\nWe can also inner_join(), in which case we will only keep those variables that are present in both datasets. Consider:\n\nshare_female_ministers_and_mps_by_year_inner &lt;- inner_join(stortinget_female_share_by_period, share_female_ministers, \n                                                     by = c(\"year\", \"period_id\"))\n\n\nprint(share_female_ministers_and_mps_by_year_inner, n = nrow(share_female_ministers_and_mps_by_year_inner))\n\n# A tibble: 13 × 4\n   period_id share_female_mps  year share_female_ministers\n   &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt;                  &lt;dbl&gt;\n 1 1969-73             0.0933  1970                  0.133\n 2 1973-77             0.153   1974                  0.2  \n 3 1977-81             0.239   1978                  0.235\n 4 1981-85             0.258   1982                  0.235\n 5 1985-89             0.344   1986                  0.444\n 6 1989-93             0.358   1990                  0.421\n 7 1993-97             0.394   1994                  0.421\n 8 1997-2001           0.364   1998                  0.5  \n 9 2001-2005           0.364   2002                  0.421\n10 2005-2009           0.379   2006                  0.474\n11 2009-2013           0.396   2010                  0.476\n12 2013-2017           0.396   2014                  0.5  \n13 2017-2021           0.408   2018                  0.45 \n\n\nThe key difference is that we now omit all observations that are not present in both datasets!\n\n\nfull_join()\nWe can instead full_join() to keep all observations that are present in at least one of the datasets:\n\nshare_female_ministers_and_mps_by_year_full &lt;- full_join(stortinget_female_share_by_period, share_female_ministers, \n                                                     by = c(\"year\", \"period_id\"))\n\nprint(share_female_ministers_and_mps_by_year_full, n = nrow(share_female_ministers_and_mps_by_year_full))\n\n# A tibble: 63 × 4\n   period_id share_female_mps  year share_female_ministers\n   &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt;                  &lt;dbl&gt;\n 1 1945-49             0.0467  1946                NA     \n 2 1950-53             0.0467  1951                NA     \n 3 1954-57             0.0464  1955                NA     \n 4 1958-61             0.0667  1959                NA     \n 5 1961-65             0.0867  1962                NA     \n 6 1965-69             0.08    1966                NA     \n 7 1969-73             0.0933  1970                 0.133 \n 8 1973-77             0.153   1974                 0.2   \n 9 1977-81             0.239   1978                 0.235 \n10 1981-85             0.258   1982                 0.235 \n11 1985-89             0.344   1986                 0.444 \n12 1989-93             0.358   1990                 0.421 \n13 1993-97             0.394   1994                 0.421 \n14 1997-2001           0.364   1998                 0.5   \n15 2001-2005           0.364   2002                 0.421 \n16 2005-2009           0.379   2006                 0.474 \n17 2009-2013           0.396   2010                 0.476 \n18 2013-2017           0.396   2014                 0.5   \n19 2017-2021           0.408   2018                 0.45  \n20 2021-2025           0.450   2022                NA     \n21 2009-2013          NA       2013                 0.524 \n22 2009-2013          NA       2012                 0.5   \n23 2013-2017          NA       2015                 0.5   \n24 2009-2013          NA       2011                 0.476 \n25 1989-93            NA       1991                 0.474 \n26 1989-93            NA       1992                 0.474 \n27 2005-2009          NA       2007                 0.474 \n28 1993-97            NA       1997                 0.45  \n29 2005-2009          NA       2009                 0.45  \n30 2013-2017          NA       2017                 0.45  \n31 2017-2021          NA       2019                 0.45  \n32 1985-89            NA       1987                 0.444 \n33 1985-89            NA       1988                 0.444 \n34 1985-89            NA       1989                 0.444 \n35 2013-2017          NA       2016                 0.444 \n36 2005-2009          NA       2008                 0.429 \n37 1989-93            NA       1993                 0.421 \n38 1993-97            NA       1995                 0.421 \n39 1993-97            NA       1996                 0.421 \n40 1997-2001          NA       2000                 0.421 \n41 1997-2001          NA       2001                 0.421 \n42 2001-2005          NA       2003                 0.421 \n43 2001-2005          NA       2004                 0.421 \n44 2001-2005          NA       2005                 0.421 \n45 1997-2001          NA       1999                 0.4   \n46 2017-2021          NA       2020                 0.4   \n47 2017-2021          NA       2021                 0.4   \n48 1973-77            NA       1976                 0.25  \n49 1973-77            NA       1977                 0.25  \n50 1977-81            NA       1979                 0.25  \n51 1977-81            NA       1981                 0.25  \n52 1981-85            NA       1983                 0.222 \n53 1981-85            NA       1984                 0.222 \n54 1981-85            NA       1985                 0.222 \n55 1969-73            NA       1973                 0.2   \n56 1973-77            NA       1975                 0.188 \n57 1969-73            NA       1966                 0.133 \n58 1969-73            NA       1967                 0.133 \n59 1969-73            NA       1968                 0.133 \n60 1969-73            NA       1969                 0.133 \n61 1977-81            NA       1980                 0.125 \n62 1969-73            NA       1971                 0.0667\n63 1969-73            NA       1972                 0.0667\n\n\nWhen using full_join() we get all the combinations of year and period_id that are present in one of the datasets. We get NAs on the share_female_ministers if the observation doesn’t appear in the share_female_ministers dataset and NAs on the share_female_mps variable if the the observation doesn’t appear in the stortinget_female_share_by_period dataset."
  },
  {
    "objectID": "Wrangling/wrangling_in_dplyr_and_tidyr.html#binding-data",
    "href": "Wrangling/wrangling_in_dplyr_and_tidyr.html#binding-data",
    "title": "Wrangling in dplyr and tidyr",
    "section": "Binding data",
    "text": "Binding data\nImagine that instead of the having the stortinget_female_share_by_period dataset with the share of female members of parliament for each legislative period, we had multiple files each containing the share of female members of parliament in one particular legislative period. For instance, we may have a set of .csv files such as these:\n\n1dir(\"../data/stortinget_periods\")\n\n\n1\n\nThe dir() function gives you the names of all the files in a specified directory on your computer.\n\n\n\n\n [1] \"stortinget_female_share_1945_period.csv\"\n [2] \"stortinget_female_share_1950_period.csv\"\n [3] \"stortinget_female_share_1954_period.csv\"\n [4] \"stortinget_female_share_1958_period.csv\"\n [5] \"stortinget_female_share_1961_period.csv\"\n [6] \"stortinget_female_share_1965_period.csv\"\n [7] \"stortinget_female_share_1969_period.csv\"\n [8] \"stortinget_female_share_1973_period.csv\"\n [9] \"stortinget_female_share_1977_period.csv\"\n[10] \"stortinget_female_share_1981_period.csv\"\n[11] \"stortinget_female_share_1985_period.csv\"\n[12] \"stortinget_female_share_1989_period.csv\"\n[13] \"stortinget_female_share_1993_period.csv\"\n[14] \"stortinget_female_share_1997_period.csv\"\n[15] \"stortinget_female_share_2001_period.csv\"\n[16] \"stortinget_female_share_2005_period.csv\"\n[17] \"stortinget_female_share_2009_period.csv\"\n[18] \"stortinget_female_share_2013_period.csv\"\n[19] \"stortinget_female_share_2017_period.csv\"\n[20] \"stortinget_female_share_2021_period.csv\"\n\n\nConsider the files stortinget_female_share_2017_period.csv\" and stortinget_female_share_2021_period.csv:\n\nstortinget_female_share_2017_period &lt;- read_delim(\"../data/stortinget_periods/stortinget_female_share_2017_period.csv\", delim = \",\")\nprint(stortinget_female_share_2017_period)\n\n# A tibble: 1 × 2\n  period_id share_female_mps\n  &lt;chr&gt;                &lt;dbl&gt;\n1 2017-2021            0.408\n\nstortinget_female_share_2021_period &lt;- read_delim(\"../data/stortinget_periods/stortinget_female_share_2021_period.csv\", delim = \",\")\nprint(stortinget_female_share_2021_period)\n\n# A tibble: 1 × 2\n  period_id share_female_mps\n  &lt;chr&gt;                &lt;dbl&gt;\n1 2021-2025            0.450\n\n\nEach of these files contain information about the same variables and with the same variable names. But there are different observations in each file. We cannot join them to combine these data, but we may bind the files together:\n\nbind_rows()\nWe we would want to add the rows from these different datasets on top of each other. We can do so using the bind_rows() function dplyr. You supply bind_rows() with two or more data.frames which will be added on top of each other. The output will contain all columns that exist in at least one of the data.frames and columns with the same names will be treated as the same variable:\n\nstortinget_female_share_2017_and_2021_periods &lt;- bind_rows(stortinget_female_share_2017_period, \n                                                           stortinget_female_share_2021_period)\n\nThe result looks like this:\n\nprint(stortinget_female_share_2017_and_2021_periods)\n\n# A tibble: 2 × 2\n  period_id share_female_mps\n  &lt;chr&gt;                &lt;dbl&gt;\n1 2017-2021            0.408\n2 2021-2025            0.450\n\n\n\nBinding many datasets together\nOf course, we may want to bind all the different .csv-files together. If so, it makes sense to exploit what we already know about iteration in R, for instance by writing a for-loop:\n\n1files &lt;- paste(\"../data/stortinget_periods\", dir(\"../data/stortinget_periods\"), sep = \"/\")\n2stortinget_female_share_combined &lt;- data.frame()\n3for(i in files){\n4  tmp &lt;- read_csv(i)\n5  stortinget_female_share_combined &lt;- bind_rows(stortinget_female_share_combined,\n                                                tmp)\n}\n\n\n1\n\nWe start by creating a vector with the path to all the different .csv-files.\n\n2\n\nNext we create an empty data.frame that we will add the data to\n\n3\n\nWe loop over the different files\n\n4\n\nWe read each file and assign it temporarily as tmp\n\n5\n\nFinally, we bind stortinget_female_share_combined and tmp and assign the result to stortinget_female_share_combined. For each iteration we thus overwrite stortinget_female_share_combined making it longer.\n\n\n\n\n\nprint(stortinget_female_share_combined)\n\n   period_id share_female_mps\n1    1945-49       0.04666667\n2    1950-53       0.04666667\n3    1954-57       0.04635762\n4    1958-61       0.06666667\n5    1961-65       0.08666667\n6    1965-69       0.08000000\n7    1969-73       0.09333333\n8    1973-77       0.15286624\n9    1977-81       0.23870968\n10   1981-85       0.25806452\n11   1985-89       0.34394904\n12   1989-93       0.35757576\n13   1993-97       0.39393939\n14 1997-2001       0.36363636\n15 2001-2005       0.36363636\n16 2005-2009       0.37869822\n17 2009-2013       0.39644970\n18 2013-2017       0.39644970\n19 2017-2021       0.40828402\n20 2021-2025       0.44970414\n\n\nAlternatively we may use the map() and reduce() functions from purrr to do the same thing:\n\nlibrary(purrr)\n1stortinget_female_share_combined_with_purr &lt;- files %&gt;%\n2  map(read_csv) %&gt;%\n3  reduce(bind_rows)\n\n\n1\n\nWe use the files vector with the name and location of each csv.-file that we created above\n\n2\n\nWith map() we can apply a function to all the elements in a vector. Here we apply read_csv which will load each file with a name and location in the files vector. This will produce a list containing the 20 data.frames\n\n3\n\nWe can use reduce() to combine the elements of a list or vector into a single value. Here we use bind_rows() to combine all the the elements of the list of 20 data.frames into a single data.frame\n\n\n\n\nThe result looks exactly the same as when we used a for-loop:\n\nprint(stortinget_female_share_combined_with_purr)\n\n# A tibble: 20 × 2\n   period_id share_female_mps\n   &lt;chr&gt;                &lt;dbl&gt;\n 1 1945-49             0.0467\n 2 1950-53             0.0467\n 3 1954-57             0.0464\n 4 1958-61             0.0667\n 5 1961-65             0.0867\n 6 1965-69             0.08  \n 7 1969-73             0.0933\n 8 1973-77             0.153 \n 9 1977-81             0.239 \n10 1981-85             0.258 \n11 1985-89             0.344 \n12 1989-93             0.358 \n13 1993-97             0.394 \n14 1997-2001           0.364 \n15 2001-2005           0.364 \n16 2005-2009           0.379 \n17 2009-2013           0.396 \n18 2013-2017           0.396 \n19 2017-2021           0.408 \n20 2021-2025           0.450 \n\n\nWith only 20 files, neither of these solutions is significantly more efficient than the other. It’s a matter of taste. If you have a very large number of files, the purrr approach will likely be faster.\n\n\n\nbind_cols()\nJust like we can bind_rows(), adding different datasets on top of each other, we can bind_cols(), adding different datasets together side by side.\nFor bind_cols() to be appropriate though, all the observations/rows need to be the same and be sorted in the same order in the data.frames we are binding. When we are not absolutely confident that they are, it is better to use left_join() or one of the other “join”-functions."
  },
  {
    "objectID": "Wrangling/wrangling_in_dplyr_and_tidyr.html#unite",
    "href": "Wrangling/wrangling_in_dplyr_and_tidyr.html#unite",
    "title": "Wrangling in dplyr and tidyr",
    "section": "unite()",
    "text": "unite()\nRemember that each variable should have one column only. For most applications, it doesn’t make sense to think of the year, month, and day on which the judgment was rendered (unless your theory is that judges get more grumpy in particular months or something like that). Instead, we would want a single variable capturing the date of the judgment!\nTo get a date variable we need to unite() the year, month and day columns.\nFor the unite() function we need to specify the name of the new column we would like to create. We then list all the columns we would like to unite()\n\nlibrary(tidyr)\n1grand_chamber %&gt;%\n2  unite(judgment_date, year, month, day) %&gt;%\n3  head()\n\n\n1\n\nWe are not assigning it to a new object, because we just want to test how this works. We therefore prefer to just print the result to the console.\n\n2\n\nWe want the new column “judgment_date” based on uniting “year”, “month”, and “day”\n\n3\n\nWe use head() to get a quick look of what the above code produces\n\n\n\n\n# A tibble: 6 × 4\n  file_name  title                                conclusions      judgment_date\n  &lt;chr&gt;      &lt;chr&gt;                                &lt;chr&gt;            &lt;chr&gt;        \n1 001-100413 McFARLANE v. IRELAND                 Preliminary obj… 2010_09_10   \n2 001-100448 SANOMA UITGEVERS B.V. v. NETHERLANDS Violation of Ar… 2010_09_14   \n3 001-100686 MANGOURAS v. SPAIN                   No violation of… 2010_09_28   \n4 001-101568 SAKHNOVSKIY v. RUSSIA                Preliminary obj… 2010_11_02   \n5 001-101579 ŞERİFE YİĞİT v. TURKEY               Preliminary obj… 2010_11_02   \n6 001-101739 TAXQUET v. BELGIUM                   Violation of Ar… 2010_11_16   \n\n\nThe above code removed the columns year, month, and day and replaced them with the new column judgment_date. The new column looks almost like what we want, but we see that years, months, and days are separated by underscores (_). This is not really a common way to format dates. We might want to replace the underscores (_) with hyphens (-). If so, we can specify the sep argument:\n\ngrand_chamber %&gt;%\n1  unite(judgment_date, year, month, day, sep = \"-\") %&gt;%\n  head() \n\n\n1\n\nWe specify that values on the old columns should be separated with - on the new column.\n\n\n\n\n# A tibble: 6 × 4\n  file_name  title                                conclusions      judgment_date\n  &lt;chr&gt;      &lt;chr&gt;                                &lt;chr&gt;            &lt;chr&gt;        \n1 001-100413 McFARLANE v. IRELAND                 Preliminary obj… 2010-09-10   \n2 001-100448 SANOMA UITGEVERS B.V. v. NETHERLANDS Violation of Ar… 2010-09-14   \n3 001-100686 MANGOURAS v. SPAIN                   No violation of… 2010-09-28   \n4 001-101568 SAKHNOVSKIY v. RUSSIA                Preliminary obj… 2010-11-02   \n5 001-101579 ŞERİFE YİĞİT v. TURKEY               Preliminary obj… 2010-11-02   \n6 001-101739 TAXQUET v. BELGIUM                   Violation of Ar… 2010-11-16   \n\n\nThe dates are now formatted in a a reasonable way.\nNotice that by default, the old columns are removed. If we don’t want to remove the year, month, and day columns, we can set remove = FALSE.\n\ngrand_chamber %&gt;%\n1  unite(judgment_date, year, month, day, sep = \"-\", remove = FALSE) %&gt;%\n  head() \n\n\n1\n\nsetting remove = FALSE in order to retain the input columns.\n\n\n\n\n# A tibble: 6 × 7\n  file_name  title                   conclusions judgment_date year  month day  \n  &lt;chr&gt;      &lt;chr&gt;                   &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 001-100413 McFARLANE v. IRELAND    Preliminar… 2010-09-10    2010  09    10   \n2 001-100448 SANOMA UITGEVERS B.V. … Violation … 2010-09-14    2010  09    14   \n3 001-100686 MANGOURAS v. SPAIN      No violati… 2010-09-28    2010  09    28   \n4 001-101568 SAKHNOVSKIY v. RUSSIA   Preliminar… 2010-11-02    2010  11    02   \n5 001-101579 ŞERİFE YİĞİT v. TURKEY  Preliminar… 2010-11-02    2010  11    02   \n6 001-101739 TAXQUET v. BELGIUM      Violation … 2010-11-16    2010  11    16   \n\n\nTo save the result to the grand_chamber dataset, we need to assign the result back to the data.frame\n\ngrand_chamber &lt;- grand_chamber %&gt;% \n  unite(judgment_date, year, month, day, sep = \"-\", remove = FALSE)\n\nhead(grand_chamber)\n\n# A tibble: 6 × 7\n  file_name  title                   conclusions judgment_date year  month day  \n  &lt;chr&gt;      &lt;chr&gt;                   &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 001-100413 McFARLANE v. IRELAND    Preliminar… 2010-09-10    2010  09    10   \n2 001-100448 SANOMA UITGEVERS B.V. … Violation … 2010-09-14    2010  09    14   \n3 001-100686 MANGOURAS v. SPAIN      No violati… 2010-09-28    2010  09    28   \n4 001-101568 SAKHNOVSKIY v. RUSSIA   Preliminar… 2010-11-02    2010  11    02   \n5 001-101579 ŞERİFE YİĞİT v. TURKEY  Preliminar… 2010-11-02    2010  11    02   \n6 001-101739 TAXQUET v. BELGIUM      Violation … 2010-11-16    2010  11    16"
  },
  {
    "objectID": "Wrangling/wrangling_in_dplyr_and_tidyr.html#separate",
    "href": "Wrangling/wrangling_in_dplyr_and_tidyr.html#separate",
    "title": "Wrangling in dplyr and tidyr",
    "section": "separate()",
    "text": "separate()\nConsider the variable grand_chamber$title.\n\ngrand_chamber$title[1:10]\n\n [1] \"McFARLANE v. IRELAND\"                \n [2] \"SANOMA UITGEVERS B.V. v. NETHERLANDS\"\n [3] \"MANGOURAS v. SPAIN\"                  \n [4] \"SAKHNOVSKIY v. RUSSIA\"               \n [5] \"ŞERİFE YİĞİT v. TURKEY\"              \n [6] \"TAXQUET v. BELGIUM\"                  \n [7] \"PERDIGAO v. PORTUGAL\"                \n [8] \"A, B AND C v. IRELAND\"               \n [9] \"PAKSAS v. LITHUANIA\"                 \n[10] \"M.S.S. v. BELGIUM, GREECE\"           \n\n\nThe case name contains the name of the applicant(s) in the case (the people claiming that their human rights have been violated) and the name of the respondent state(s) separated by “.v”. Suppose we are interested in having the applicant(s) and the respondent state(s) in separate variables. For instance, we might be interested in learning how many cases there are against different states.\nWe need to separate() the column “title” into two new columns, which we may call “applicant” and “respondent_state”. We can exploit the fact that these two pieces of the data are always separated by the a “v.” in the “title” column. We need to specify (1) the column we would like to separate() using the col argument, (2) the columns it should be separated into using the into argument, which takes a character vector with the new column names, and (3) the separator, using the sep argument:\n\ngrand_chamber %&gt;% \n1  separate(col = title,\n2           into = c(\"applicant\", \"respondent_state\"),\n3           sep = \"v. \") %&gt;%\n  head()\n\n\n1\n\ntitle is the column we would like to separate\n\n2\n\n“applicant” and “respondent_state” are the names of the new columns that title should be separated into.\n\n3\n\nThe separator is “v.”.\n\n\n\n\n# A tibble: 6 × 8\n  file_name  applicant    respondent_state conclusions judgment_date year  month\n  &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;\n1 001-100413 \"McFARLANE \" IRELAND          Preliminar… 2010-09-10    2010  09   \n2 001-100448 \"SANOMA UIT… NETHERLANDS      Violation … 2010-09-14    2010  09   \n3 001-100686 \"MANGOURAS \" SPAIN            No violati… 2010-09-28    2010  09   \n4 001-101568 \"SAKHNOVSKI… RUSSIA           Preliminar… 2010-11-02    2010  11   \n5 001-101579 \"ŞERİFE YİĞ… TURKEY           Preliminar… 2010-11-02    2010  11   \n6 001-101739 \"TAXQUET \"   BELGIUM          Violation … 2010-11-16    2010  11   \n# ℹ 1 more variable: day &lt;chr&gt;\n\n\nWe now got two new columns applicant and respondent_state. Like for unite() the default behavior is to remove the old column. In this case, it probably makes sense to keep the title though, so we will override this behavior using the remove argument:\n\ngrand_chamber &lt;- grand_chamber %&gt;% \n  separate(col = title,  \n           into = c(\"applicant\", \"respondent_state\"), \n           sep = \"v. \", \n           remove = FALSE) \nhead(grand_chamber)\n\n# A tibble: 6 × 9\n  file_name  title    applicant respondent_state conclusions judgment_date year \n  &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;\n1 001-100413 McFARLA… \"McFARLA… IRELAND          Preliminar… 2010-09-10    2010 \n2 001-100448 SANOMA … \"SANOMA … NETHERLANDS      Violation … 2010-09-14    2010 \n3 001-100686 MANGOUR… \"MANGOUR… SPAIN            No violati… 2010-09-28    2010 \n4 001-101568 SAKHNOV… \"SAKHNOV… RUSSIA           Preliminar… 2010-11-02    2010 \n5 001-101579 ŞERİFE … \"ŞERİFE … TURKEY           Preliminar… 2010-11-02    2010 \n6 001-101739 TAXQUET… \"TAXQUET… BELGIUM          Violation … 2010-11-16    2010 \n# ℹ 2 more variables: month &lt;chr&gt;, day &lt;chr&gt;\n\n\n\nseparate() when the number of values differ between rows\nLet’s tabulate the grandchamber$respondent_state variable, we just created:\n\ntable(grand_chamber$respondent_state)\n\n\n                                                           ARMENIA \n                                                                 3 \n                                                           AUSTRIA \n                                                                 4 \n                                                        AZERBAIJAN \n                                                                 2 \n                                                           BELGIUM \n                                                                 9 \n                                                   BELGIUM, GREECE \n                                                                 1 \n                                            BOSNIA AND HERZEGOVINA \n                                                                 3 \nBOSNIA AND HERZEGOVINA, CROATIA, SERBIA, SLOVENIA, NORTH MACEDONIA \n                                                                 1 \n                                                          BULGARIA \n                                                                 5 \n                                                           CROATIA \n                                                                 7 \n                                                            CYPRUS \n                                                                 4 \n                                                    CZECH REPUBLIC \n                                                                 5 \n                                                           DENMARK \n                                                                 4 \n                                                           ESTONIA \n                                                                 1 \n                                                           FINLAND \n                                                                 6 \n                                                            FRANCE \n                                                                37 \n                                                           GEORGIA \n                                                                 2 \n                                                           GERMANY \n                                                                19 \n                                                            GREECE \n                                                                15 \n                                                           HUNGARY \n                                                                 8 \n                                                           IRELAND \n                                                                 5 \n                                                             ITALY \n                                                                46 \n                                                            LATVIA \n                                                                12 \n                                                     LIECHTENSTEIN \n                                                                 1 \n                                                         LITHUANIA \n                                                                 5 \n                                                        LUXEMBOURG \n                                                                 1 \n                                                             MALTA \n                                                                 3 \n                                                           MOLDOVA \n                                                                 4 \n                                                   MOLDOVA, RUSSIA \n                                                                 3 \n                                                       NETHERLANDS \n                                                                12 \n                                                   NORTH MACEDONIA \n                                                                 1 \n                                                            NORWAY \n                                                                 4 \n                                                            POLAND \n                                                                10 \n                                                          PORTUGAL \n                                                                 7 \n                                                           ROMANIA \n                                                                14 \n                                                            RUSSIA \n                                                                16 \n                                                        SAN MARINO \n                                                                 1 \n                                                            SERBIA \n                                                                 1 \n                                                          SLOVAKIA \n                                                                 1 \n                                                          SLOVENIA \n                                                                 5 \n                                                             SPAIN \n                                                                 5 \n                                                            SWEDEN \n                                                                 6 \n                                                       SWITZERLAND \n                                                                14 \n                                                            TURKEY \n                                                                42 \n                                                           UKRAINE \n                                                                 3 \n                                                    UNITED KINGDOM \n                                                                55 \n\n\nAs you can see most, judgments had a single respondent state, but a few had two, and one judgment had five respondent states (“BOSNIA AND HERZEGOVINA, CROATIA, SERBIA, SLOVENIA, NORTH MACEDONIA”). We might want to separate these into different columns and, if so, we can exploit the fact that different states are separated by commas. The problem is we don’t have the same number of respondent states for all the rows. Fortunately, separate() can easily deal with this problem by filling in NA for rows with less than five respondent states. We just need to specify that we want NAs to be filled in to the right of the columns with\n\ngrand_chamber %&gt;% \n  separate(col = respondent_state, \n1           into = paste(\"respondent_state\", 1:5, sep = \"_\"),\n           sep = \",\", \n2           fill = \"right\") %&gt;%\n  head()\n\n\n1\n\npaste(\"respondent_state\", 1:5, sep = \"_\") will produce the list of column names we want. If you run paste(\"respondent_state\", 1:5, sep = \"_\"), you get \"respondent_state_1\" \"respondent_state_2\" \"respondent_state_3\" \"respondent_state_4\" \"respondent_state_5\", which is what we want.\n\n2\n\nWe just need to specify where the NA should be added.\n\n\n\n\n# A tibble: 6 × 13\n  file_name  title               applicant respondent_state_1 respondent_state_2\n  &lt;chr&gt;      &lt;chr&gt;               &lt;chr&gt;     &lt;chr&gt;              &lt;chr&gt;             \n1 001-100413 McFARLANE v. IRELA… \"McFARLA… IRELAND            &lt;NA&gt;              \n2 001-100448 SANOMA UITGEVERS B… \"SANOMA … NETHERLANDS        &lt;NA&gt;              \n3 001-100686 MANGOURAS v. SPAIN  \"MANGOUR… SPAIN              &lt;NA&gt;              \n4 001-101568 SAKHNOVSKIY v. RUS… \"SAKHNOV… RUSSIA             &lt;NA&gt;              \n5 001-101579 ŞERİFE YİĞİT v. TU… \"ŞERİFE … TURKEY             &lt;NA&gt;              \n6 001-101739 TAXQUET v. BELGIUM  \"TAXQUET… BELGIUM            &lt;NA&gt;              \n# ℹ 8 more variables: respondent_state_3 &lt;chr&gt;, respondent_state_4 &lt;chr&gt;,\n#   respondent_state_5 &lt;chr&gt;, conclusions &lt;chr&gt;, judgment_date &lt;chr&gt;,\n#   year &lt;chr&gt;, month &lt;chr&gt;, day &lt;chr&gt;"
  },
  {
    "objectID": "Wrangling/wrangling_in_dplyr_and_tidyr.html#separate_longer_delim",
    "href": "Wrangling/wrangling_in_dplyr_and_tidyr.html#separate_longer_delim",
    "title": "Wrangling in dplyr and tidyr",
    "section": "separate_longer_delim()",
    "text": "separate_longer_delim()\nRather than having multiple respondent_state variables, it might make sense to change the data structure so that we have one row per judgment per respondent state. If so, we can use separate_longer_delim(). This function works like separate but instead of splitting the column supplied to col into multiple columns, it will add addition rows so that each row has a single value on col column. For instance, we may:\n\ngrand_chamber_per_state &lt;- grand_chamber %&gt;% \n  separate_longer_delim(col = respondent_state, \n1           delim = \",\")\n\ntable(grand_chamber_per_state$respondent_state)\n\n\n1\n\nNote that separate_longer_delim uses the argument delim instead of sep.\n\n\n\n\n\n               CROATIA                 GREECE        NORTH MACEDONIA \n                     1                      1                      1 \n                RUSSIA                 SERBIA               SLOVENIA \n                     3                      1                      1 \n               ARMENIA                AUSTRIA             AZERBAIJAN \n                     3                      4                      2 \n               BELGIUM BOSNIA AND HERZEGOVINA               BULGARIA \n                    10                      4                      5 \n               CROATIA                 CYPRUS         CZECH REPUBLIC \n                     7                      4                      5 \n               DENMARK                ESTONIA                FINLAND \n                     4                      1                      6 \n                FRANCE                GEORGIA                GERMANY \n                    37                      2                     19 \n                GREECE                HUNGARY                IRELAND \n                    15                      8                      5 \n                 ITALY                 LATVIA          LIECHTENSTEIN \n                    46                     12                      1 \n             LITHUANIA             LUXEMBOURG                  MALTA \n                     5                      1                      3 \n               MOLDOVA            NETHERLANDS        NORTH MACEDONIA \n                     7                     12                      1 \n                NORWAY                 POLAND               PORTUGAL \n                     4                     10                      7 \n               ROMANIA                 RUSSIA             SAN MARINO \n                    14                     16                      1 \n                SERBIA               SLOVAKIA               SLOVENIA \n                     1                      1                      5 \n                 SPAIN                 SWEDEN            SWITZERLAND \n                     5                      6                     14 \n                TURKEY                UKRAINE         UNITED KINGDOM \n                    42                      3                     55 \n\n\nEach row now only has a single value in the respondent_state. While this is a nice way to structure these data, a key thing to remember is that we have now changed the observational unit from “judgment” to “judgment*respondent”.\n\n\n\n\n\n\nseparate_wider_delim()\n\n\n\nThere is also a separate_wider_delim() which works in the same way as the regular separate() function we have already discussed: it separates into multiple columns. The key difference is that it uses the argument delim instead of sep. The intention of the tidyr developers is that this function will replace separate()."
  },
  {
    "objectID": "Wrangling/wrangling_in_dplyr_and_tidyr.html#pivot_longer",
    "href": "Wrangling/wrangling_in_dplyr_and_tidyr.html#pivot_longer",
    "title": "Wrangling in dplyr and tidyr",
    "section": "pivot_longer()",
    "text": "pivot_longer()\nIn other words, the main problem problem with wb_population_data is that is in a too wide format. We need to increase the number of rows and decrease the number of columns, thus making the data longer.\nWe can do this using pivot_longer() from tidyr. We want to gather all the columns with names that start with “x” into one column and call that column population. We will also take the column names for all the variables that start with “x” and add them to a new variable, which we will call year so that we can keep track of which year each population value is for. [As we showed you above], we can use the select() helper function starts_with() to select all the columns that with names that start with the same character(s):\n\nwb_population_tidy &lt;- wb_population_data %&gt;% \n1  mutate(across(starts_with(\"x\"),\n                .fns = as.numeric)) %&gt;%\n2  pivot_longer(cols = starts_with(\"x\"),\n3               names_to = \"year\",\n4               values_to = \"population\")\n\n\n1\n\nFirst we want to make sure that all these columns are of the same data type. Otherwise they cannot be combined. We use mutate() and across() to change multiple columns. We use starts_with(\"x\") to select all columns with names that start with “x” as the columns we would like to change, and we specify that as.numeric is the function we would like to apply.\n\n2\n\nTo pivot_longer(), we first need to select the columns that should be combined into a single variable using the col argument. Again we use starts_with(\"x\") to select all columns with names that start with “x”.\n\n3\n\nWe then specify the name for the new variable containing the column names for all those columns with names starting with “x”. The information that is useful in those column names is what year the value is for, so we will call this variable “year”\n\n4\n\nFor values_to we specify the name for the new and longer variables that we want to add all the values on the different columns we are pivoting.\n\n\n\n\nThe first rows of the result look like this:\n\nhead(wb_population_tidy)\n\n# A tibble: 6 × 6\n  series_name       series_code country_name country_code year        population\n  &lt;chr&gt;             &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt;\n1 Population, total SP.POP.TOTL Afghanistan  AFG          x1960_yr19…    8622466\n2 Population, total SP.POP.TOTL Afghanistan  AFG          x1961_yr19…    8790140\n3 Population, total SP.POP.TOTL Afghanistan  AFG          x1962_yr19…    8969047\n4 Population, total SP.POP.TOTL Afghanistan  AFG          x1963_yr19…    9157465\n5 Population, total SP.POP.TOTL Afghanistan  AFG          x1964_yr19…    9355514\n6 Population, total SP.POP.TOTL Afghanistan  AFG          x1965_yr19…    9565147\n\n\nOf course, the year variable doesn’t look like exactly what we want. But we already learned about the substr() function, which handles this slight complication:\n\nwb_population_tidy &lt;- wb_population_tidy %&gt;% \n  mutate(year = substr(year, 2,5), \n         year = as.integer(year))\nhead(wb_population_tidy)\n\n# A tibble: 6 × 6\n  series_name       series_code country_name country_code  year population\n  &lt;chr&gt;             &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;        &lt;int&gt;      &lt;dbl&gt;\n1 Population, total SP.POP.TOTL Afghanistan  AFG           1960    8622466\n2 Population, total SP.POP.TOTL Afghanistan  AFG           1961    8790140\n3 Population, total SP.POP.TOTL Afghanistan  AFG           1962    8969047\n4 Population, total SP.POP.TOTL Afghanistan  AFG           1963    9157465\n5 Population, total SP.POP.TOTL Afghanistan  AFG           1964    9355514\n6 Population, total SP.POP.TOTL Afghanistan  AFG           1965    9565147"
  },
  {
    "objectID": "Wrangling/wrangling_in_dplyr_and_tidyr.html#pivot_wider",
    "href": "Wrangling/wrangling_in_dplyr_and_tidyr.html#pivot_wider",
    "title": "Wrangling in dplyr and tidyr",
    "section": "pivot_wider()",
    "text": "pivot_wider()\nJust like you may pivot_longer, you may pivot_wider() reducing the number of rows and expanding the number of columns. Consider this dataset which contains country-year data on different democracy indeces from the Varieties of Democracy project.\n\ndemocracy_scores &lt;- read_delim(\"../data/democracy_scores.csv\", delim = \",\")\n\nhead(democracy_scores)\n\n# A tibble: 6 × 5\n  country_name country_id  year democracy_index  score\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n1 Mexico                3  1789 v2x_libdem       0.042\n2 Mexico                3  1789 v2x_polyarchy    0.028\n3 Mexico                3  1789 v2x_partipdem    0.006\n4 Mexico                3  1789 v2x_egaldem     NA    \n5 Mexico                3  1790 v2x_libdem       0.042\n6 Mexico                3  1790 v2x_polyarchy    0.028\n\n\nInstead of one row per country-year, the dataset has one row per democracy index per country-year. This format can sometimes be useful for data visualization in ggplot2, but it is inappropriate for doing country-year level analysis. We need to pivot_wider() the dataset.\nTo pivot_wider(), we need to specify the column in the existing dataset that contains the names of the new columns we will create to widen the data (the names_from argument) and the column in the existing dataset where the values on the new columns in the wider dataset should come from (the values_from argument):\n\ndemocracy_scores_country_year &lt;- democracy_scores %&gt;% \n1  pivot_wider(names_from = \"democracy_index\",\n2              values_from = \"score\")\n\n\n1\n\nThe names of the new columns should come from the column democracy_index.\n\n2\n\nThe values for the new columns should come the column score.\n\n\n\n\nThe top rows of the new data.frame look like this:\n\nhead(democracy_scores_country_year)\n\n# A tibble: 6 × 7\n  country_name country_id  year v2x_libdem v2x_polyarchy v2x_partipdem\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 Mexico                3  1789      0.042         0.028         0.006\n2 Mexico                3  1790      0.042         0.028         0.006\n3 Mexico                3  1791      0.042         0.028         0.006\n4 Mexico                3  1792      0.042         0.028         0.006\n5 Mexico                3  1793      0.042         0.028         0.006\n6 Mexico                3  1794      0.042         0.028         0.006\n# ℹ 1 more variable: v2x_egaldem &lt;dbl&gt;\n\n\nWe successfully pivoted the data into a country-year format!"
  },
  {
    "objectID": "Wrangling/wrangling_in_dplyr_and_tidyr.html#footnotes",
    "href": "Wrangling/wrangling_in_dplyr_and_tidyr.html#footnotes",
    "title": "Wrangling in dplyr and tidyr",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr at least we know how to use Wikipedia to learn about them.↩︎\nThere are no ministers younger than 30 or older than 69 in who_gov_norway_ministers.↩︎\nNote that since our data starts in 1966 and some of the people in government in the 1960s and 1970s may also have been in government before the start of our data, this variable will be somewhat inaccurate at the beginning of the time series.↩︎\nIf we are to do this for a proper research project such as an MA thesis, we would have wanted to use a complete dataset on all Norwegian Ministers. In fact Martin Søyland (2017) did just that for his MA thesis which he later published as an article. His data are available here. WhoGov is updated annually and sometimes ministers are replaced more frequently. For instance, Kåre Gjønnes, which we recoded above, was not just Minister of Agriculture from 1997 to 2000. He was also Minister of Nordic Cooperation from January to March 2000. Because he never held this position in July, it is, however, not registered in the WhoGov dataset. WhoGov arguably makes up for omissions such as these by offering a very impressive coverage of most countries in the world over a long time period, making it possible, for instance, to study female access to cabinets across the world Nyrup, Yamagishi, and Bramwell (2023). But if you do a study only on a single country, want to do like Søyland (2017) and collect data on all the ministers!↩︎\nWe grabbed this data from the Norwegian parliament’s website using the R package stortingscrape developed by Søyland (2021).↩︎\nWhen coding the period_id, we keep in mind that each legislative period starts at the beginning of October (at least since 1962), while WhoGov is updated each July.↩︎"
  },
  {
    "objectID": "data_gather/apis.html",
    "href": "data_gather/apis.html",
    "title": "APIs",
    "section": "",
    "text": "Although web pages in .html are what we often see with our eyes when using a browser, it is not necessarily always the case that this is the best way to scrape data. Depending on which website and data you are interested in, there are often back-end databases from which the websites retrieve information based on the user’s clicks. Many such websites have an Application Programming Interface (API) available, which you can use relatively freely. And some websites are themselves an API. Take for example the Star Wars API, which is a database of data on characters, worlds, movies, etc., in the Star Wars universe.\nThe front page of SWAPI shows how, for example, you can retrieve data about a person:\nperson1_url &lt;- \"https://swapi.dev/api/people/1/\"\n\nreadLines(person1_url)\n[1] \"{\\\"name\\\":\\\"Luke Skywalker\\\",\\\"height\\\":\\\"172\\\",\\\"mass\\\":\\\"77\\\",\\\"hair_color\\\":\\\"blond\\\",\\\"skin_color\\\":\\\"fair\\\",\\\"eye_color\\\":\\\"blue\\\",\\\"birth_year\\\":\\\"19BBY\\\",\\\"gender\\\":\\\"male\\\",\\\"homeworld\\\":\\\"https://swapi.dev/api/planets/1/\\\",\\\"films\\\":[\\\"https://swapi.dev/api/films/1/\\\",\\\"https://swapi.dev/api/films/2/\\\",\\\"https://swapi.dev/api/films/3/\\\",\\\"https://swapi.dev/api/films/6/\\\"],\\\"species\\\":[],\\\"vehicles\\\":[\\\"https://swapi.dev/api/vehicles/14/\\\",\\\"https://swapi.dev/api/vehicles/30/\\\"],\\\"starships\\\":[\\\"https://swapi.dev/api/starships/12/\\\",\\\"https://swapi.dev/api/starships/22/\\\"],\\\"created\\\":\\\"2014-12-09T13:50:51.644000Z\\\",\\\"edited\\\":\\\"2014-12-20T21:17:56.891000Z\\\",\\\"url\\\":\\\"https://swapi.dev/api/people/1/\\\"}\""
  },
  {
    "objectID": "data_gather/apis.html#footnotes",
    "href": "data_gather/apis.html#footnotes",
    "title": "APIs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nx &lt;- stop[[1]]↩︎"
  },
  {
    "objectID": "data_gather/webscraping.html",
    "href": "data_gather/webscraping.html",
    "title": "Webscraping",
    "section": "",
    "text": "The internet is a fantastic resource for data and give very efficient ways of collecting data for research. One way of gathering data from the internet is to copy and paste from any given web-page into a sheet or document. This is, however, often inefficient. In this section, we will cover how to automate this process by utilizing various scraping utilities in R."
  },
  {
    "objectID": "data_gather/webscraping.html#sleep",
    "href": "data_gather/webscraping.html#sleep",
    "title": "Webscraping",
    "section": "Sleep",
    "text": "Sleep\nIt is always good practice to not overload the webpages we are scraping from. Most sites have a limit on how many times an IP can ping their page within a given time frame. If you exceed this, you will be timed out for a while before you can resume scraping.\nWorst case, you can overload the server hosting the website and bring it down. That will probably make you unpopular with the server maintainers.\n\n\n\n\n\n\nRemember to add sleep between calls\n\n\n\nIn R, the best way to make sure you have delay between calls is to use the Sys.sleep() command. For instance, Sys.sleep(2) will force R to pause for 2 seconds.\nYou can also add random sleep by using rnorm:\n\nSys.sleep(2 + abs(rnorm(1)))\n\nR will then sleep for 2 seconds plus the absolute value of rnorm."
  },
  {
    "objectID": "data_gather/webscraping.html#save-locally",
    "href": "data_gather/webscraping.html#save-locally",
    "title": "Webscraping",
    "section": "Save locally",
    "text": "Save locally\nIn the same vein, we should also (if possible) save the scraped webpages locally on our computer. By doing this, we do not need to scrape everything each time we want to e.g run an analysis.\nBut, we also want to save locally because the internet is not static; webpages can change over time, which can result in replicability issues for researchers.\n\n\n\n\n\n\nRecommended workflow\n\n\n\n\nMake separate scripts for scraping, preprocessing, and analyses\nTest your scraping script on a couple of pages before running on everything\nSet up iterative code with sleep that downloads the page to your drive\nMake use of control flow to check whether the files are already on disk."
  },
  {
    "objectID": "data_gather/webscraping.html#robots.txt",
    "href": "data_gather/webscraping.html#robots.txt",
    "title": "Webscraping",
    "section": "robots.txt",
    "text": "robots.txt\nMost webpages have a robots.txt that instructs web robots (e.g search engine bots) what data they can and can not fetch from the website. The robots.txt is located in the root URL of the website: “https:///robots.txt”.\nFor instance, New York Times has a robots.txt at https://www.nytimes.com/robots.txt.\nWhen scraping a website, one should always look at the robots.txt file and build the scraper accordingly. If there are disallowed pages for “User Agent”, you should consider not scraping these."
  },
  {
    "objectID": "data_gather/webscraping.html#html",
    "href": "data_gather/webscraping.html#html",
    "title": "Webscraping",
    "section": ".html",
    "text": ".html\nMost webpages are written in Hypertext Mark-up Language (HTML). HTML is a markup language that gives your web-browser information on how a webpage should look for its users. At the core, HTML is build by a hierarchical structure of nodes encased in &lt;&gt; and closed by &lt;/&gt;. For instance, &lt;p&gt; opens a paragraph node, which is closed by &lt;/p&gt;. We will explore this further below.\nAs long as the webpage is openly accessible, the underlying HTML code is also available for users to read. You can access the entire source of any webpage you are browsing by simply pressing ctrl/cmd + u, or you can inspect specific elements of the page by right clicking on the page and then pressing “Inspect” (or pressing ctrl/cmd + i):\n\nYou will then get a tree of HTML nodes in a panel on the right side of your screen, where the node you right clicked on will be highlighted:\n\nAs we usually want to extract specific data from a webpage, excluding ads, menus, and other irrelevant data, these element nodes are very useful for extracting the data we want.\n\n\n\n\n\n\n\nSelector Gadget\n\n\n\nA helpful tool in finding element nodes in webpages is the Selector Gadget plugin for Google Chrome (or similar for your preferred browser). You can find a guide for using this plugin on the rvest webpage.\n\n\nThe most common structure of the HTML hierarchy is that you have the &lt;html&gt; or &lt;!DOCTYPE html&gt; node at the top that encompass the entire page, a &lt;head&gt; node of meta data (language, dates, etc), and a &lt;body&gt; node which contains the content of the webpage. The nodes are often referred to as parents and child. For instance:\n&lt;div&gt;\n  &lt;p&gt;Hello World!&lt;/p&gt;\n&lt;/div&gt;\nHere, &lt;div&gt; is the parent of &lt;p&gt; and &lt;p&gt; is the child of &lt;div&gt;.\n\n\n\n\n\n\nHTML structure\n\n\n\nIf you have a .html file loaded in i R with the rvest package, you can inspect the entire structure of the html tree with xml2::html_structure(). The output might, however, be somewhat verbose.\n\n\nThere are a lot of nodes in HTML markup, but here is a list of some common nodes you will encounter:\n\n\n\n\n\n\n\nHTML code\nNode description\n\n\n\n\n&lt;div&gt;\nPart of the document\n\n\n&lt;section&gt;\nSection of the document\n\n\n&lt;table&gt;\nA table\n\n\n&lt;p&gt;\nA paragraph\n\n\n&lt;h2&gt;\nHeading in size 2\n\n\n&lt;h6&gt;\nHeading in size 6\n\n\n&lt;a&gt;\nAnchor combined with the href attribute for making hyperlinks\n\n\n&lt;img&gt;\nAn image\n\n\n&lt;br&gt;\nVertical line break\n\n\n\n\n.html example\nIn this section, we will use the British Political Speech Archive as an example of scraping .html files.\nFirst, as this is a short example, we want to extract only the speeches by Labour between 1990 and 2000. You can follow the instructions in the video below for how to obtain this link. Following the good manners section above, we download the page one time:\n\n\ndownload.file(\"http://www.britishpoliticalspeech.org/speech-archive.htm?q=&speaker=&party=4&searchRangeFrom=1990&searchRangeTo=2000\",\n              destfile = \"../data/scrape/bps/lab_90-00.html\")\n\n\n\nWe can now read this .html file with the rvest package and extract the table of speeches:\n\n1bps_front &lt;- read_html(\"../data/scrape/bps/lab_90-00.html\")\n\n2lab_speak &lt;- bps_front %&gt;%\n  html_nodes(\"table[class='results-table']\") %&gt;%\n  html_table() %&gt;%\n  bind_rows()\n\n\n1\n\nReading the .html file\n\n2\n\nExtracting the table node and search result class attribute, then converting to a table and binding the rows (unlisting)\n\n\n\n\n\n\n\n1lab_txt_links &lt;- bps_front %&gt;%\n  html_nodes(\"table[class='results-table'] &gt; tbody &gt; tr &gt; td &gt; a\") %&gt;%\n  html_attr(\"href\") %&gt;%\n  str_c(\"http://www.britishpoliticalspeech.org/\", .)\n\n2lab_txt_ids &lt;- str_extract(lab_txt_links, \"[0-9]+$\")\n\nlab_speak$id &lt;- lab_txt_ids\n\n\n1\n\nExtracting the hyperlinks for all speeches (used below) and pasting with the root URL of the webpage\n\n2\n\nExtracting the numerical id of each speech and inserting it to the data frame (also used below)\n\n\n\n\n\n\nIn the next step, we want to download the actual speeches. This involves extracting the links for each item in the list of our front page, download the page from that link, and extract the data we want from that page.\n\n1for(i in 1:length(lab_txt_links)) {\n2  download.file(lab_txt_links[i],\n3                destfile = str_c(\"../data/scrape/bps/txt/\",\n                                 lab_txt_ids[i],\n                                 \".html\"))\n  \n  Sys.sleep(3)\n}\n\n\n1\n\nFor each number in 1 to the length of lab_txt_links\n\n2\n\n… download link i\n\n3\n\n… and store it in the folder data/scrape/bps/txt/, names as its id and the .html file extension.\n\n\n\n\n\n\nFinally, we can iterate over the downloaded speeches and extract the text we want from these pages. Turns out, by inspecting one of the speech pages, that there is a div node with the class attribute and “speech” value. This makes the extraction straightforward:\n\n1bps_files &lt;- list.files(\"../data/scrape/bps/txt\", full.names = TRUE)\n\nlab_speak$text &lt;- NA\n\n2for(i in bps_files) {\n  \n3  tmp &lt;- read_html(i) %&gt;%\n4    html_nodes(\"div[class='speech']\") %&gt;%\n5    html_text() %&gt;%\n6    str_replace_all(\"\\\\s+\", \" \")\n  \n7  lab_speak$text[which(lab_speak$id == str_extract(i, \"[0-9]+\"))] &lt;- tmp\n  \n}\n\n8lab_speak[1:3, c(1:3, 6)]\n\n\n1\n\nListing all files containing the speeches and making a holder variable for the texts in the data frame lab_speak\n\n2\n\nFor each of the files, we want to…\n\n3\n\n… read the html-file\n\n4\n\n… extract the div node with the speech class\n\n5\n\n… convert the speech to text\n\n6\n\n… and, replace all connected whitespace with one space\n\n7\n\nInserts the text to the row in lab_speech with corresponding id\n\n8\n\nPrinting a subset of rows and columns to show how the data looks.\n\n\n\n\n# A tibble: 3 × 4\n  Date       Party  Speaker       text                                          \n  &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;         &lt;chr&gt;                                         \n1 30/11/2000 Labour Brown, Gordon \" Speech at the New Deal Conference, London 2…\n2 26/09/2000 Labour Blair, Tony   \" Leader's speech, Brighton 2000 Tony Blair (…\n3 25/09/2000 Labour Brown, Gordon \" Chancellor's speech, Brighton 2000 Gordon B…\n\n\n\n\nNow, we can use the text for whatever we need the text for. Say I want to look at speech length (number of words) comparisons between Gordon Brown and Tony Blair:1\n\nlibrary(ggplot2)\n\nlab_speak %&gt;% \n  filter(str_detect(Speaker, \"Blair|Brown\")) %&gt;% \n  mutate(nwords = quanteda::ntoken(text),\n         Date = as.Date(Date, \"%d/%m/%Y\")) %&gt;% \n  ggplot(aes(x = Date, y = nwords, color = Speaker)) +\n  geom_point() +\n  geom_line() +\n  ggthemes::theme_wsj()"
  },
  {
    "objectID": "data_gather/webscraping.html#csv",
    "href": "data_gather/webscraping.html#csv",
    "title": "Webscraping",
    "section": ".csv",
    "text": ".csv\nThe data we want to use in our research might, of course, already be available in structured formats. A common way to share data is in the .csv format. See the section on reading data for more information in the .csv format.\n\n1all_csvs &lt;- read_html(\"https://github.com/MainakRepositor/Datasets\")\n\n2all_links &lt;- all_csvs %&gt;% html_nodes(\"a\") %&gt;% html_attr(\"href\")\n\n3csvs &lt;- all_links[which(str_detect(all_links, \"\\\\.csv\"))] %&gt;%\n4  str_remove(\"/MainakRepositor/Datasets/blob/master/\")\n \n5csvs[1:3]\n\n\n1\n\nReading the folder of datasets\n\n2\n\nExtracting all links from the page\n\n3\n\nSubsetting only links ending with .csv\n\n4\n\nRemoving “/MainakRepositor/Datasets/blob/master/” from the links; now the only thing that remains is the file name of the .csv files.\n\n5\n\nPrinting the first three .csv files\n\n\n\n\n\n\n[1] \"2018-personality-data.csv\" \"AB_NYC_2019.csv\"          \n[3] \"Ads_CTR_Optimisation.csv\" \n\n\n\n1raw_links &lt;- str_c(\n  \"https://raw.githubusercontent.com/MainakRepositor/Datasets/master/\",\n  csvs\n  )\n\n2for(i in 1:3) {\n  readr::read_csv(raw_links[i], show_col_types = FALSE) %&gt;%\n    head() %&gt;%\n    print()\n}\n\n\n1\n\nPasting the name of the .csv with the standardized address of raw data on Github\n\n2\n\nLooping over 1 to 3, where the first three links from raw_links is read from the repository, and then the first few rows printed to the console\n\n\n\n\n\n\n# A tibble: 3 × 3\n  userid                           openness agreeableness\n  &lt;chr&gt;                               &lt;dbl&gt;         &lt;dbl&gt;\n1 8e7cebf9a234c064b75016249f2ac65e        5             2\n2 77c7d756a093150d4377720abeaeef76        7             4\n3 b7e8a92987a530cc368719a0e60e26a3        4             3\n\n# A tibble: 3 × 3\n     id name                                host_id\n  &lt;dbl&gt; &lt;chr&gt;                                 &lt;dbl&gt;\n1  2539 Clean & quiet apt home by the park     2787\n2  2595 Skylit Midtown Castle                  2845\n3  3647 THE VILLAGE OF HARLEM....NEW YORK !    4632\n\n# A tibble: 3 × 3\n  `Ad 1` `Ad 2` `Ad 3`\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1      1      0      0\n2      0      0      0\n3      0      0      0"
  },
  {
    "objectID": "data_gather/webscraping.html#javascript-parsed-pages",
    "href": "data_gather/webscraping.html#javascript-parsed-pages",
    "title": "Webscraping",
    "section": "Javascript parsed pages",
    "text": "Javascript parsed pages\nAlthough we can get far with scraping .html sites, some webpages are procedurally generated with JavaScript2. A common way of scraping these types of pages is to use Selenium which automates browser behavior. If you are interested in learning more about using selenium in R, check out this guide."
  },
  {
    "objectID": "data_gather/webscraping.html#footnotes",
    "href": "data_gather/webscraping.html#footnotes",
    "title": "Webscraping",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nObviously, this figure does not really show anything interesting. But take a look at the text data section↩︎\nJavaScript is a scripting language used to add interactivity and dynamic behavior to webpages.↩︎"
  },
  {
    "objectID": "spatial_data/spatial_joins.html",
    "href": "spatial_data/spatial_joins.html",
    "title": "Spatial joins",
    "section": "",
    "text": "In previous lessons, you’ve learned about regular joins of data frames using commands, such as left_join(). With regular joins you combine information from two datasets, using a joint ID or key variable that exists in both datasets and identifies observations in both datasets.\nA spatial join is similar to regular joins that it also describes a process to combine two datasets. But, in contrast to regular joins, we use spatial joins in a very special case, when two conditions are met:\n\nWe don’t have a joint key variable in the two datasets we want to combine. Think of the Bosnia datasets we used before. We had two data frames: one with the location of the events (the ged dataset) and another dataset with the shapes of the Bosnian municipalities (the bosnia_shp dataset). The observations in the ged dataset (i.e. the conflict events) do NOT have an ID variable that tells us in which municipality each event happened.\nWe have two spatial data frames. If the two datasets we want to combine are spatial data frames (i.e. data frames that we’ve transformed into sf data frames), then we can combine the two without having joint IDs.\n\nIf those two conditions are met, we can use spatial joins to combine the data."
  },
  {
    "objectID": "spatial_data/spatial_joins.html#join-aggregated-data-frame-back-into-the-polygon-data-frame",
    "href": "spatial_data/spatial_joins.html#join-aggregated-data-frame-back-into-the-polygon-data-frame",
    "title": "Spatial joins",
    "section": "Join aggregated data frame back into the polygon data frame",
    "text": "Join aggregated data frame back into the polygon data frame\nNow we need to join the aggregated data back into the main bosnia_shp data frame, so we can plot the data and find an answer to our research question.\nThis join is a simple left_join() that we know from the data wrangling lesson. No fancy spatial joins needed, since we now have two data frames with common IDs, namely the ged_munic_ag data frame and the bosnia_shp data frame.\nLet’s join the data…\n\nbosnia_shp_ged &lt;- left_join(bosnia_shp, \n                            ged_munic_ag, \n                            by = \"id_munic\")\n\n…and look at the result:\n\nhead(bosnia_shp_ged)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 17.57853 ymin: 42.56583 xmax: 18.57639 ymax: 43.22457\nGeodetic CRS:  WGS 84\n  id_munic     name conflict_event_count                       geometry\n1      108 Trebinje                    1 POLYGON ((18.02122 42.93654...\n2       17     Neum                   NA POLYGON ((17.83138 43.01822...\n3      116  Lubinje                   NA POLYGON ((18.15364 43.0523,...\n4      112   Bileca                   NA POLYGON ((18.41007 43.13096...\n5       13 Capljina                    1 POLYGON ((17.77296 43.17501...\n6       16   Stolac                    3 POLYGON ((17.97802 43.19037...\n\n\nWe see that we now have for every municipality a measure of the count of conflict events in that municipality–exactly what we wanted. But we also see that in same cases there is an NA for a municipality. That means that in this municipality there were no conflict events. Since we can only match municipality IDs to events, it’s logical that those municipalities are missing where we don’t have any events.\nWe can replace the NAs with zero, since in this case, missing means actually zero.\n\nbosnia_shp_ged &lt;- bosnia_shp_ged %&gt;% \n  replace_na(list(conflict_event_count = 0))\n\nOk, now we’re ready to…"
  },
  {
    "objectID": "spatial_data/what_is_spatial_data.html",
    "href": "spatial_data/what_is_spatial_data.html",
    "title": "What is spatial data?",
    "section": "",
    "text": "In this section, you’ll learn about spatial data, i.e. data that is represented in space."
  },
  {
    "objectID": "spatial_data/what_is_spatial_data.html#example-1-election-geography",
    "href": "spatial_data/what_is_spatial_data.html#example-1-election-geography",
    "title": "What is spatial data?",
    "section": "Example 1: Election geography",
    "text": "Example 1: Election geography\nWhere in a country does a certain political party have the most electoral support? Where is the party the weakest? Here’s a map of election results displayed by county from the 2020 US presidential elections. Such a map could help us answer these questions in the context for the US elections.\n\n\n\nFigure 1: County-level map of 2020 US elections. (source)"
  },
  {
    "objectID": "spatial_data/what_is_spatial_data.html#example-2-aid-allocation",
    "href": "spatial_data/what_is_spatial_data.html#example-2-aid-allocation",
    "title": "What is spatial data?",
    "section": "Example 2: Aid allocation",
    "text": "Example 2: Aid allocation\nAnother question could be: Where do the poorest people in a country live? And do poverty alleviation projects actually reach this part of the population? The figure below might help to answer this question. It shows (in red) the locations of World Bank aid projects. Did those projects actually reach the people who needed the help most? You’ll have to read the paper (Briggs 2018) to find out.\n\n\n\nFigure 2: The geo-location of World Bank aid projects (source: Briggs 2018)"
  },
  {
    "objectID": "spatial_data/what_is_spatial_data.html#example-3-civil-war-intensity",
    "href": "spatial_data/what_is_spatial_data.html#example-3-civil-war-intensity",
    "title": "What is spatial data?",
    "section": "Example 3: Civil war intensity",
    "text": "Example 3: Civil war intensity\nA final example for a research question that involves spatial information might be: Where in a country does violent conflict occur? And which parts of a country are less affected by violence? Consider the map below, which shows the spatial distribution of violent events during the Bosnia civil war. The map shows that some areas where extremely affected (the dark areas) and some weren’t (the white ones). This is important information, if we want to, for example, study the strategy of why conflict actors target some areas but not others.\n\n\n\nFigure 3: Violence in Bosnia (source: Weidmann 2023)\n\n\nAll these examples represent important political science questions, from different subfields. To answer them we need some form of spatial information for the data we collect on these phenomena. In short, we need “spatial” data."
  },
  {
    "objectID": "spatial_data/what_is_spatial_data.html#footnotes",
    "href": "spatial_data/what_is_spatial_data.html#footnotes",
    "title": "What is spatial data?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe earth is actually not precisely a sphere, but that fact is usually not important for the types of spatial data we deal with as social scientists.↩︎"
  },
  {
    "objectID": "Visualization/data_viz_models.html",
    "href": "Visualization/data_viz_models.html",
    "title": "Visualizing regression models",
    "section": "",
    "text": "Visualizations are not just useful for presenting your data. Visualizations can also be more effective than the traditional regression tables in communicating the results of your statistical analysis!"
  },
  {
    "objectID": "Visualization/data_viz_models.html#coefficient-plots-for-a-single-regression-model",
    "href": "Visualization/data_viz_models.html#coefficient-plots-for-a-single-regression-model",
    "title": "Visualizing regression models",
    "section": "Coefficient plots for a single regression model",
    "text": "Coefficient plots for a single regression model\nBinder (2018) is interested in the factors that influence legislators willingness to bend the rules to strengthen legislative majorities. She investigates the decision of US Senators in 2017 to sign letter committing to save the “filibuster” procedure in the US Senate (which she interprets as an unwillingness to bend the rules). She hypothesizes that Republicans (the majority party) were less likely to sign the letter, but that republicans running for reelection in 2018 would be more likely to sign than other republicans, and that ideological outliers would be less likely to sign the letter. She controls for “other forces that may shape lawmakers’ procedural views including senators’ seniority in the chamber and whether they have ever served in the Senate minority party” (Binder 2018, 1460).\nShe estimates a single logistic regression model with the decision to sign the letter as the dependent variable. Instead of a regression table, she reports the coefficients and 95% confidence intervals in a coefficient plot (Binder 2018, 1461). We recreate this plot using broom and ggplot2\n\ndodgingrules &lt;- read_dta(\"../data/dodgingrules.dta\")\n\n1binder_model &lt;- glm(filisave ~ running18 + gop +  wings +  leader +\n                      stdrank + servedmin +  running18:gop,\n                    data = dodgingrules,\n                    family = binomial(link = \"logit\"))\n\n\n2tidy(binder_model) %&gt;%\n3  mutate(clustered_standard_error = sqrt(diag(vcovHAC(binder_model,\n                                                      cluster = dodgingrules$stateid))),\n4         upper = estimate + 1.96 * clustered_standard_error,\n         lower = estimate - 1.96 * clustered_standard_error,\n5         term = case_when(term == \"wings\" ~ \"Ideologue\",\n                          term == \"running18:gop\" ~ \"GOP running in 2018\",\n                          term == \"leader\" ~ \"Party leader\",\n                          term == \"gop\" ~ \"Republican\",\n                          term == \"stdrank\" ~ \"Chamber rank\",\n                          term == \"servedmin\" ~ \"Served in minority\",\n                          term == \"running18\" ~ \"Running in 2018\"),\n6         term = factor(term, levels = c(\"Running in 2018\",\"Served in minority\",\n                                        \"Chamber rank\",\"Republican\",\"Party leader\",\n                                        \"GOP running in 2018\", \"Ideologue\"  ))) %&gt;%\n7  drop_na(term) %&gt;%\n8  ggplot(aes(y = term,\n             x = estimate,\n             xmin = lower,\n             xmax = upper))+\n9  geom_vline(xintercept = 0, color = \"grey\", linetype = \"longdash\")+\n10  geom_hline(aes(yintercept = term), color = \"grey\")+\n11  geom_point(size = 2)+\n12  geom_errorbarh(height = 0, size = 1)+\n13  xlim(-10, 5)+\n14  theme_classic()+\n15  theme(panel.border = element_rect(color = \"black\", fill = NA))+\n16  ylab(\"\")+\n  xlab(\"\")\n\n\n1\n\nWe reestimate her logistic regression using glm(), which is the maximum likelihood equvalent of stan_glm() from rstanarm.\n\n2\n\nUsing tidy() from broom we can get all the regression table as a nice data.frame that is easy to work with.\n\n3\n\nBinder (2018) clustered her standard errors by state. We can cluster the variance covariance matrix with vcovHAC() from the sandwich package. The arguments we supply are a regression model (binder_model) and a variable to cluster on dodgingrules$stateid. We take the square root (sqrt()) of the diagonal (diag()) of this matrix to get the standard errors which we save as variable to the data.frame().\n\n4\n\nWe create the upper/lower limits of the confidence intervals by taking each coefficient and adding/subtracting 1.96 times the standard error.\n\n5\n\nWe recode the term column which has the variable for each coefficient to have the same labels as the one reported by Binder (2018).\n\n6\n\nBy making factor variable with levels we can ensure that the coefficients will have the same order in the figure as the order used by Binder (2018).\n\n7\n\nWe didn’t recode the term for the intercept so this row now has NA on term. Binder (2018) didn’t report the intercept in her figure, so we will just omit this row.\n\n8\n\nWe start making our plot with ggplot() we map the coefficient names to the y axis and the size of each coefficient to the x axis. In addition we now specify xmin and xmax which are going to provide the lower and upper end of each confidence interval.\n\n9\n\nAdding a vertical line at 0 makes it easy to see which confidence interval cross zero (indicating that the coefficient is not is significant at the 0.05-level) and which don’t (indicating that the coefficient is statistically significant). Binder (2018) added a grey dashed line, so we do the same.\n\n10\n\nBinder (2018) also added some grey horizontal lines for each coefficient. These don’t serve a clear purpose, but adding them is easy enough and illustrates how we can also map the yintercept to variable in the dataset using aes().\n\n11\n\nWe add points for each coefficient and make them slightly larger then the default size using size = 2.\n\n12\n\nWe use geom_errorbarh() to create horizontal error bars for the confidence intervals. By default they have quite pronounced edges that we can remove by setting height = 0.\n\n13\n\nWe adjust the length of the x-axis using xlim().\n\n14\n\nWe want to remove most of the background noise so we use theme_classic().\n\n15\n\nThe exception is that Binder (2018) had a black border around her figure, so adjust the theme() to add such a border. We set fill = NA so that nothing is added inside the border.\n\n16\n\nWe remove the axis labels using xlab() and ylab().\n\n\n\n\n\n\n\nFigure 1: Replication of Binder (2018)’s Figure 2. Logit coefficients and 95%-confidence intervals\n\n\n\n\nThe above example illustrates how you can make coefficient plots using the regular ggplot2 functionality. There are also packages that automatize more of this process for you. For instance, the modelSummary package that we used to create regression tables has a modelplot() function that produces coefficient plots.\nLet’s see if we can recreate the figure using modelplot()!\n\n1modelplot(binder_model,\n2          vcov = vcovHAC(binder_model, cluster = dodgingrules$stateid),\n3          coef_map = list(\"running18\" = \"Running in 2018\",\n                              \"servedmin\" = \"Served in minority\",\n                          \"stdrank\" = \"Chamber rank\",\n                         \"gop\" = \"Republican\",\n                         \"leader\" = \"Party leader\",\n                       \"running18:gop\" = \"GOP running in 2018\",\n                       \"wings\" = \"Ideologue\"),\n4          conf_level = 0.95)\n\n\n1\n\nWe supply the model to modelplot() from modelsummary.\n\n2\n\nwe can change how the standard errors are calculated using the vcov argument. Note that we just supply a new variance-covariance matrix so we don’t need to extract the square root of the diagonal. modelplot() knows how to that for us.\n\n3\n\nJust as for the tables we made using modelsummary(), we can use coef_map to rename, reorder, and omit coefficients!\n\n4\n\nWe decide whatever confidence interval we want here.\n\n\n\n\n\n\n\nFigure 2: Replication of Binder (2018)’s Figure 2 using modelplot()\n\n\n\n\nUsing modelplot() was arguably much easier, so many thanks to Arel-Bundock (2022)!\nBut the fun doesn’t stop. Since modelplot() produces a ggplot2 object, we can use all the `ggplot2 functions to further customize the figure! E.g.:\n\nmodelplot(binder_model, \n          vcov = vcovHAC(binder_model, cluster = dodgingrules$stateid), \n          coef_map = list(\"running18\" = \"Running in 2018\", \n                              \"servedmin\" = \"Served in minority\",\n                          \"stdrank\" = \"Chamber rank\",\n                         \"gop\" = \"Republican\", \n                         \"leader\" = \"Party leader\", \n                       \"running18:gop\" = \"GOP running in 2018\",\n                       \"wings\" = \"Ideologue\"), \n          conf_level = 0.95) +\n1  theme_classic() +\n2 geom_vline(xintercept = 0, color = \"grey\", linetype = \"longdash\")\n\n\n1\n\nWe can change the theme.\n\n2\n\nWe can add the dashed grey line to clarify which confidence intervals overlap with zero.\n\n\n\n\n\n\n\nFigure 3: Replication of Binder (2018)’s Figure 2 using modelplot() with some additional adjustments."
  },
  {
    "objectID": "Visualization/data_viz_models.html#coefficient-plots-with-multiple-regression-models",
    "href": "Visualization/data_viz_models.html#coefficient-plots-with-multiple-regression-models",
    "title": "Visualizing regression models",
    "section": "Coefficient plots with multiple regression models",
    "text": "Coefficient plots with multiple regression models\nUsing coefficient plots arguably also makes it easier to compare coefficients and uncertainty estimates from multiple regression models. To distinguish between different models in the coefficient plot, we can for instance use different colors, different symbols for the coefficient points, different line types for the coefficient estimates or some combination.1 Alternatively, We can have different models on one the axis and use colors and symbols to distinguish between different coefficients.\nFor instance, Shen-Bayh (2022) argues that authoritarian regimes are more likely to use judicial means of repressing insider threats (and to use extrajudicial strategies towards outsider threats). Using a dataset on failed coup plots in Anglophone sub-Saharan Africa, she estimates three logistic regression with a binary indicator for whether judicial strategies were used to deal with the plotters (or not) as the dependent variable and the challengers were “regime insiders” the main independent variable (Shen-Bayh 2022, 341). The models differ in the control variables that are included. While Shen-Bayh (2022) report her results in a table, we will illustrate how they could also have been reported in a coefficient plot:\n\njudicial_repression &lt;- read_delim(\"../data/sb_judicial_repression_wp_2018.csv\", delim = \",\") \njudicial_repression &lt;- judicial_repression %&gt;% \n  mutate(insider_threat = ifelse(challenger_type == \"Insiders\",1,0))\n\njudicial_repression_logit1 &lt;-  glm(trial ~ insider_threat, \n                                   data = judicial_repression, \n                                   family = binomial(link=\"logit\"))\njudicial_repression_logit2 &lt;- glm(trial ~ insider_threat + exec_mil, \n                                  data = judicial_repression, \n                                  family = binomial(link=\"logit\"))\njudicial_repression_logit3  &lt;- glm(trial ~ insider_threat + exec_mil +\n                                     as.factor(country) + as.factor(year), \n                                   data = judicial_repression, \n                                   family = binomial(link=\"logit\"))\n\nmodelplot(list(\"Model 1: no controls\" = judicial_repression_logit1,\n               \"Model 2: Controlling for military regime\" =  judicial_repression_logit2,\n               \"Model 3: Controlling for military regime +\\nyear and country fixed effects\" = judicial_repression_logit3), \n          coef_map = list(\"insider_threat\" = \"Insider threat\", \n                          \"exec_mil\" = \"Military regime\", \n                          \"(Intercept)\" = \"Intercept\"))+\n  geom_vline(xintercept = 0, color = \"grey\")\n\n\n\n\nFigure 4: Replication of Table 1 in Shen-Bayh (2022) as a coefficient plot.\n\n\n\n\nFigure Figure 4 makes it easy to compare the magnitude and uncertainty for the “Insider threat” coefficients across the three different specifications."
  },
  {
    "objectID": "Visualization/data_viz_models.html#footnotes",
    "href": "Visualization/data_viz_models.html#footnotes",
    "title": "Visualizing regression models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlthough coefficient plots make it easier to compare multiple models, it can be hard to distinguish a very large number of models based on colors and symbols.↩︎"
  },
  {
    "objectID": "Visualization/data_viz_examples.html",
    "href": "Visualization/data_viz_examples.html",
    "title": "Examples of different types of data visualizations",
    "section": "",
    "text": "Visualizations are useful for summarizing information about our different variables and the relationships between them. The type of visualizations that are useful will depend on what kinds of data you have (are your variables continuous or categorical, cross sectional or temporal, etc.?) and on the questions you ask.\nOn this page, we will show examples of different types graphs often used in political science. Indeed, all the examples are reproductions of figures from recent political science articles published in top journals. The code chunks below show both the ggplot2 code needed to create the visualization and the dplyr and tidyr wrangling code needed to get the data in the right format format for ggplot. Throughout we will thus make use of the following packages:\n\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(haven)\n\nThe examples below are meant to illustrate how to make various forms of frequently used graphs and to show the type of messages you can communicate with the different types of visualizations. Remember that with ggplot2 you are not stuck with any of these graphs or with the aesthetic design that we have chosen here. You may (and should) adapt your visualizations to your own needs!\n\nBar charts\nBar charts are great for visualizing the count or proportion of observations that belong to different categories on a categorical variable.\nFor instance, Chen and Xu (2023) collect data on litigation by multinational corporations in Chinese courts. To visualize the amount of litigation from companies from eight important economies they use a bar chart (Chen and Xu 2023, 162), which we can recreate using ggplot2:\n\nload(\"../data/chen_xu.RData\")\ndat %&gt;% \n1  mutate(across(.cols = c(plahq, defhq, plaregister, defregister), .fns = as.character),\n2                country = case_when(plahq == \"日本\" | defhq == \"日本\" | plaregister == \"日本\" | defregister == \"日本\" ~ \"Japan\",\n                                    plahq == \"韩国\" | defhq == \"韩国\" | plaregister == \"韩国\" | defregister == \"韩国\" ~ \"South Korea\",\n                                    plahq == \"美国\" | defhq == \"美国\" | plaregister == \"美国\" | defregister == \"美国\" ~ \"USA\",\n                                    plahq == \"德国\" | defhq == \"德国\" | plaregister == \"德国\" | defregister == \"德国\" ~ \"Germany\",\n                                    plahq == \"法国\" | defhq == \"法国\" | plaregister == \"法国\" | defregister == \"法国\" ~ \"France\",\n                                    plahq == \"新加坡\" | defhq == \"新加坡\" | plaregister == \"新加坡\" | defregister == \"新加坡\" ~ \"Singapore\",\n                                    plahq == \"英国\" | defhq == \"英国\" | plaregister == \"英国\" | defregister == \"英国\" ~ \"UK\",\n                                    plahq == \"澳大利亚\" | defhq == \"澳大利亚\" | plaregister == \"澳大利亚\" |\n                                      defregister == \"澳大利亚\" ~ \"Australia\")) %&gt;%\n3  drop_na(country) %&gt;%\n4  ggplot(aes(x = country))+\n5  geom_bar(stat = \"count\", width = 0.9, fill = \"darkgrey\", color = \"black\")+\n6  ylab(\"Number of cases\")+\n  xlab(\"\")+\n7  scale_y_continuous(breaks = seq(0, 1400, by = 200), limits = c(0,1600))+\n8  theme_classic()+\n9  theme(axis.line.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.text.x = element_text(angle = 45))+\n10    rotate_y_text()\n\n\n1\n\nTheir country variable in the bar chart is based on these four variables. They were all coded as “factors” so we recode them to characters.\n\n2\n\nThe dataset is in Chinese, but with great help from the replication materials we can create the country variable using case_when().\n\n3\n\nWe omit observations that have NA on the country variable we just created, which just means that they didn’t involve a company from any of the eight countries included in the graph.\n\n4\n\nWe start making our graph with ggplot(). We only need one aesthetic mapping (x). On the y-axis we will have a count of the observations with each value on the x-axis, but geom_bar() will take care of the counting.\n\n5\n\nWe use geom_bar() to get bars! stat = \"count\" means that it should count the observations for each value on the x-axis on let this count determine the height of the bars. We adjust the width of the bars, the color inside the bars (using fill) and the color of the border surrounding each bar (using color) to make it match the design of the original figure (which was not created using ggplot2).\n\n6\n\nUsing ylab and xlab we can change the axes labels.\n\n7\n\nUsing scale_y_continuous() we can change aspects relating the “scale” on the y-axis. Here we change which values to have ticks and labels for using breaks and the length of the y-axis using limits.\n\n8\n\ntheme_classic() roughly matches the appearance of the original graph which was created in the base R.\n\n9\n\nWe make some adjustments to the theme to match the original graph: We remove the line and tick marks from the x-axis and tilt change the labels so that they are printed in a 90 degrees angle.\n\n10\n\nrotate_y_text() from the `ggpubr package is useful to rotate the labels on the y-axis to match the original graph by Chen and Xu (2023). But truth to be told: we think the graph would look better if we hadn’t rotated the labels on the y-axis.\n\n\n\n\n\n\n\nYou can also use bar charts to show how the amount on one variables vary depending on the value on some categorical variable or combinations of categorical variables. For instance, Carey and Gohdes (2021) make the observation that most journalists are killed in democracies. To illustrate this claim, they count the number of journalists killings that occurred in democracies and the autocracies (using the binary regime classification from Boix, Miller, and Rosato (2013)). They further distinguish between killings known to have been committed by the state and killings with an unknown perpetrator and display the number of killings in each category in a bar chart. Let’s recreate their bar chart:\n\njournalist_killings &lt;- read_csv(\"../data/journalist_killings.csv\")\n1journalist_killings %&gt;%\n2  filter(major != 1, is.na(bmr_democracy)== FALSE) %&gt;%\n3  mutate(regime = ifelse(bmr_democracy == 1, \"Democracy\", \"Autocracy\")) %&gt;%\n4  group_by(regime) %&gt;%\n  summarize(state = sum(state), \n5            unknown = sum(unknown)) %&gt;%\n6  pivot_longer(cols = c(state, unknown),\n               values_to = \"Killings\",\n               names_to = \"Perpetrator\") %&gt;%\n7  ggplot(aes(x = regime,\n             y = Killings,\n             fill = Perpetrator)) +\n8    geom_bar(position = \"dodge\",\n9            stat = \"identity\",\n10            width = .3,\n11            color = \"black\") +\n12  scale_fill_manual(labels = c(\"state\", \"unconfirmed\"),\n                     values = c(\"grey\", \"white\"))+\n13   xlab(\"\")+\n  ylab(\"Journalists killed\") +\n14  theme(legend.position = \"bottom\",\n        panel.background = element_blank(),\n                    panel.border = element_rect(colour = \"black\", fill=NA),\n                     text = element_text(size=16)) +\n15  coord_flip()\n\n\n1\n\nSome simple wrangling is needed for this figure. We will wrangle the data as needed and supply the wrangled data directly to ggplot() using pipes (%&gt;%)\n\n2\n\nWe start by filtering out killings that occurred during major conflicts and observations from countries with missing on the regime variable.\n\n3\n\nWe recode the regime variable to get a categorical variable with nice labels for the figure.\n\n4\n\nWe group_by() the regime variable to summarize the data within each regime type\n\n5\n\nUsing summarize() we count the number of killings by the state and killings with an unknown perpetrator in the dataset\n\n6\n\nIn the dataset, there are different columns for killings depending on whether the perpetrator was the state or if the perpetrator was unknown. ggplot() prefers to have a longer dataset with just one “Killings” variable and additional variable measuring the type of “Perpetrator”. We thus pivot_longer().\n\n7\n\nFinally, we can start on recreating the graph, using ggplot(). Notice the we supply both x and y since we have now already counted the y for each category on x. Notice also that we use fill to let the colors vary group. While color would determine the color of the borders of the bars. fill determines the color of the area of the bars.\n\n8\n\nWe use geom_bar() to get bars in our graph. We want the bars side by side rather than on top of each other, so we specify position = \"dodge\".\n\n9\n\nWe have already counted the number of killings for each category, so we don’t need ggplot to do that for us. We therefore use stat = \"identity.\n\n10\n\nCarey and Gohdes (2021)’s bars are slimmer than the default, so we will also adjust their width accordingly.\n\n11\n\nHere we set color to decide the color of the border for each bar. color is not supplied to aes() so it will not vary by variable. Instead all the bars will get a black border.\n\n12\n\nUsing scale_fill_manual() we can make adjustments to the scale. Here we change the labels and the colors used.\n\n13\n\nWe change the labels.\n\n14\n\nWe adjust the theme(). Specifically we remove the background, create black frame around the graph, and change the position of the legend.\n\n15\n\nLike Carey and Gohdes (2021), we will flip the graph around so that the x-axis becomes the y-axis and vice versa.\n\n\n\n\n\n\n\n\n\nHistograms\nThe histogram is a type of bar chart that is useful for visualizing the distribution on continuous variables. Values on the continuous variable are binned in into different categories and displayed on the x-axis and the count or share of observations in each category are displayed on the y-axis.\nFor instance, Seabra and Mesquita (2022) introduces a dataset on the sponsorship of draft resolutions in the United Nations General Assembly. To illustrate their data, they report histograms both of the number of sponsors per draft resolution and of the number of drafts each country has sponsored (Seabra and Mesquita 2022, 7). Let’s replicate both histograms:\n\ndrafts &lt;- read_delim(\"../data/drafts abs 2009-2020 (S64-S74).csv\", delim = \",\")\n\ndrafts &lt;- drafts %&gt;%\n1  filter(session&gt;63 & session&lt;74)\n\n\n2per_draft &lt;- ggplot(drafts,\n3                    aes(x = n_sponsors_final))+\n4  geom_histogram(binwidth = 10)+\n5  labs(x = \"Total number of sponsors\",\n       y =\"Draft count\",\n       title = \"Number of sponsors per draft\")\n\n6per_country &lt;- drafts %&gt;%\n7  pivot_longer(cols = c(29:222),\n               names_to = \"country\",\n               values_to = \"sponsorship\") %&gt;%\n8  mutate(sponsorship = replace_na(sponsorship, 0),\n9         sponsorship = ifelse(sponsorship &gt; 0, 1,sponsorship)) %&gt;%\n10  group_by(country) %&gt;%\n  summarize(sponsorships = sum(sponsorship)) %&gt;%\n11  ggplot(aes(x = sponsorships))+\n12  geom_histogram()+\n13  labs(x=\"Number of sponsored drafts\", y=\"Country count\", title=\"Sponsored drafts per country\")\n\n14ggarrange(per_draft, per_country, ncol = 2, nrow = 1)\n\n\n1\n\nThe article is based on a smaller subset of General Assembly sessions than are available in their data. We subset to have the same data as in the article.\n\n2\n\nWe start making the first histogram with ggplot() since we will combine two graphs in this example, we assign the graph to an object (per_draft).\n\n3\n\nWe just need to supply a mapping for the x-axis\n\n4\n\ngeom_histogram() makes an histogram. We can specify how large each bin on the x variable should be using bindwidth.\n\n5\n\nWe change the labels for the x- and y- axes and add a title using labs. Seabra and Mesquita (2022) just used the default theme, so we will do the same.\n\n6\n\nFor the second histogram, we need to wrangle the data so that we have one row per country and a variable measuring how many drafts each country sponsored.\n\n7\n\nThere are a bunch of variables for each country with codes measuring their sponsorship status. We use pivot_longer() to get one row per country and a variable measuring the sponsorship status for each country on each draft. We use the numbering rather than the names of the columns for selection.\n\n8\n\nThe sponsorship variable has NA when countries did not sponsor the draft. We replace with 0s.\n\n9\n\nWe don’t want to distinguish between types of sponsorship so we replace all codes other than 0 with 1.\n\n10\n\nWe group_by() country and then use summarize() to count the number of sponsorships per country.\n\n11\n\nWe map the sponsorships variable to the x-axis using aes().\n\n12\n\nFor this histogram, we don’t change the binwidth or the bins for geom_histogram(). We then get the default number of bins, which is 30.\n\n13\n\nWe add appropriate labels using labs().\n\n14\n\nUsing ggarrange() from the ggpubrpackage, we can combine different “ggplots” in the same figure. We want them in the same row but in different columns, so we set the number of columns to be 2 and the number of rows to be 1.\n\n\n\n\n\n\n\n\n\n\n\nLine graphs\nWe often want to show the development of one or more variables over time. Line graphs are great for this purpose. For instance, Nyrup, Yamagishi, and Bramwell (2023) use the WhoGov data (which we discussed here) to show how the shares of female ministers in democracies and autocracies have developed over time (Nyrup, Yamagishi, and Bramwell 2023, 9). We can recreate the upper panel of their figure, like this:\n\nconsolidating_progress &lt;- read_csv(\"../data/df_consolidatingprogress_V1.csv\")\n\nconsolidating_progress %&gt;% \n1  drop_na(democracy_bmr) %&gt;%\n2  mutate(democracy_bmr = ifelse(democracy_bmr == 1, \"Democracy\", \"Autocracy\")) %&gt;%\n3  group_by(year, democracy_bmr) %&gt;%\n4  summarize(percentage_female = mean(share_female, na.rm = TRUE)) %&gt;%\n5  ggplot(aes(x = year, y = percentage_female, color = democracy_bmr))+\n6  geom_line(size = 1.1)+\n7  scale_x_continuous(breaks = seq(1970, 2021, 10)) +\n8  scale_y_continuous(breaks = seq(0, 30, by = 10), label = paste(seq(0, 30, by = 10), \"%\"), limits = c(0,30))+\n9  scale_color_manual(values=c(\"#CB2314\",\"#273046\"), guide = FALSE)+\n10  annotate(\"text\", x = 2018, y = 29, label = \"Democracies\",size = 3) +\n  annotate(\"text\", x = 2019, y = 18, label = \"Autocracies\",size = 3) +\n11  labs(title = \"% women in cabinet\", x = \"\", y = \"\") +\n  theme_classic()\n\n\n1\n\nSince we want to group observations by whether they are democracies or autocracies according to Boix, Miller, and Rosato (2013), we drop observations with NA on this variable.\n\n2\n\nWe recode the democracy variable to be categorical.\n\n3\n\nWe group_by() year and democracy variable.\n\n4\n\nThen we use summarize() to calculate the mean percentage within each group (note that even if the variable is called share_female it measures percentages, so we don’t need to multiply by 100).\n\n5\n\nIt is time to start visualizing using gpplot(). We need three aesthetic mappings. The x- and y- axes, but also a mapping to distinguish between autocracies and autocracies. We use color to get two different lines in different colors.\n\n6\n\nUsing geom_line() we produce our two lines and by increasing the size we get lines that are slightly thicker than the default.\n\n7\n\nUsing scale_x_continuous() we can adjust the values shown on the x-axis.\n\n8\n\nUsing scale_y_continuous() we similarly change the values shown on the y-axis. Here we also use paste() to add a “%” symbol after each value and limits to change the default height of the axis.\n\n9\n\nUsing scale_color_manual() we can change the colors used in the plot. Rather than using one of the named colors, Nyrup, Yamagishi, and Bramwell (2023) supplied their preferred colors using hexadecimal notation. Setting guide = FALSE removes the legend. 10. In place of the legend, we can add text to the plot using annotate(). We need to supply the coordinates for the text using x and y and the label to be included.\n\n10\n\nUsing labs() we can both change the labels for the x- and y- axes (i.e. instead of using ylab() and xlab()) and add a title on top of the graph.\n\n11\n\ntheme_classic() removes the background and grid, producing a graph that looks like the original graph (Nyrup, Yamagishi, and Bramwell 2023, 9).\n\n\n\n\n\n\n\n\n\nSmoothed time trends\nWe have already seen how to create scatter plots and add smoothed curves to illustrate the correlation between two variables. Points and smoothed curves can also be to illustrate developments over time. Such a strategy can help illuminate just what the overall time trend is, but also how much variation there is around the general trend.\nFor instance, Clark et al. (2023) ask “Are police racially biased in the decision to shoot?” To answer this question (sadly, the answer is “yes”), they collect data on all officer-involved shootings in eight jurisdictions in the United States. To illustrate their data they construct group the data by jurisdiction and month and calculate the natural logarithm of the number of shootings in each jurisdiction-month. They plot this number against the month for each jurisdiction and add a local regression curve to capture potential time trends (Clark et al. 2023, 835). Let’s recreate their graph!\n\n1library(lubridate)\n2load(\"../data/ois_data_for_models.RData\")\nois %&gt;% \n3  filter(is.na(civilian_race_factor) == FALSE,\n4  city_clean != \"San Antonio\") %&gt;%\n5  group_by(month = floor_date(Date, \"month\"), city_clean) %&gt;%\n6  count() %&gt;%\n7  ggplot(aes(x = month, y = log(n+1))) +\n8  geom_point() +\n9  geom_smooth(method = \"loess\") +\n10  facet_wrap(vars(city_clean), as.table = FALSE) +\n11  xlab(\"Month-Year\") +\n  ylab(\"Number of OIS\") +\n12  scale_y_continuous(breaks = c(log(1),log(11),log(21),\n                              log(101), log(201),log(401)),\n                     labels = c(\"0\", \"10\",\"20\", \"100\", \"200\",\"400\")) +\n13  theme_bw()\n\n\n1\n\nThe lubridate package is useful for worked in dates!\n\n2\n\nThe dataset is an “events dataset” with one row per officer-related shooting.\n\n3\n\nUltimately, they are interested in how racial bias influence the decision to shoot so they omit observations with NA on the racial identity of the civilian.\n\n4\n\nAs described in footnote 9 of the paper, they omit San Antonio because there is too little data available from this jurisdiction.\n\n5\n\nThey group their data by month (which they extract from a date variable using floor_date() from lubridate) and jurisdiction.\n\n6\n\nUsing count() we can count the number of rows for each group. The resulting variable will by default we be called n\n\n7\n\nWe start making the graph by declaring the aesthetic mapping. We want month on the x-axis and log(n) on the y-axis. This will compress the scale. However, we want to avoid taking the logarithm og zero, so instead we use log(n+1).\n\n8\n\nWe can add points using geom_point().\n\n9\n\nAnd we can add a fitted line or curve to the point using geom_smooth(). We set the method to be loess to get a local regression.\n\n10\n\nUsing facet_wrap() and selecting the variable capturing the jurisdiction in which the shooting occurred (city_clean), we get one facet per jurisdiction.\n\n11\n\nUsing ylab() and xlab(), we can change the axes labels.\n\n12\n\nWe can change aspects relating to the “scale” on the y-axis, using scale_y_continuous(). Clark et al. (2023) have a logarithmic scale on the axis but labels on the original scale. When setting the breaks we therefore take the logarithm of the values we want labels and tick marks for. In the labels argument we add the labels that should be displayed in the graph.\n\n13\n\nFinally, theme_bw() is the theme Clark et al. (2023) used.\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nBoix, Carles, Michael Miller, and Sebastian Rosato. 2013. “A Complete Data Set of Political Regimes, 1800–2007.” Comparative Political Studies 46 (12): 1523–54.\n\n\nCarey, Sabine C, and Anita R Gohdes. 2021. “Understanding Journalist Killings.” The Journal of Politics 83 (4): 1216–28.\n\n\nChen, Frederick R, and Jian Xu. 2023. “Partners with Benefits: When Multinational Corporations Succeed in Authoritarian Courts.” International Organization 77 (1): 144–78.\n\n\nClark, Tom S, Elisha Cohen, Adam N Glynn, Michael Leo Owens, Anna Gunderson, and Kaylyn Jackson Schiff. 2023. “Are Police Racially Biased in the Decision to Shoot?” The Journal of Politics 85 (3): 000–000.\n\n\nNyrup, Jacob, Hikaru Yamagishi, and Stuart Bramwell. 2023. “Consolidating Progress: The Selection of Female Ministers in Autocracies and Democracies.” American Political Science Review, 1–20.\n\n\nSeabra, Pedro, and Rafael Mesquita. 2022. “Beyond Roll-Call Voting: Sponsorship Dynamics at the UN General Assembly.” International Studies Quarterly 66 (2): sqac008."
  },
  {
    "objectID": "text_data/preprocessing.html",
    "href": "text_data/preprocessing.html",
    "title": "Preprocessing",
    "section": "",
    "text": "In this section we will look at preprocessing of text, i.e. how we can go from text to numbers and what choices/assumptions we want to make along the way. We will review the most basic assumption we make in quantitative analysis of large text data: bag of words.\nIt is very important to remember that all texts are unique! It does not take a large amount of words before one text begins to differ from another, even if the theme, form, aim and meaning are identical. Even if the same author was to write about exactly the same thing at two different times, the two texts would very likely differ. Therefore, we often take steps to reduce or standardize the number of elements in our texts, before we do analyses. This is what we understand here as preprocessing.\nAnd preprocessing is quite important to how analysis results end up looking."
  },
  {
    "objectID": "text_data/preprocessing.html#counting-words",
    "href": "text_data/preprocessing.html#counting-words",
    "title": "Preprocessing",
    "section": "Counting words",
    "text": "Counting words\n\nlibrary(tidytext)\nlibrary(janeaustenr)\n\nausten_count &lt;- austen_books() %&gt;%\n  unnest_tokens(token, text) %&gt;%\n  count(book, token, sort = TRUE) %&gt;%\n  arrange(desc(n))\n\nausten_count\n\n# A tibble: 40,379 × 3\n   book              token     n\n   &lt;fct&gt;             &lt;chr&gt; &lt;int&gt;\n 1 Mansfield Park    the    6206\n 2 Mansfield Park    to     5475\n 3 Mansfield Park    and    5438\n 4 Emma              to     5239\n 5 Emma              the    5201\n 6 Emma              and    4896\n 7 Mansfield Park    of     4778\n 8 Pride & Prejudice the    4331\n 9 Emma              of     4291\n10 Pride & Prejudice to     4162\n# ℹ 40,369 more rows"
  },
  {
    "objectID": "text_data/preprocessing.html#tf-idf",
    "href": "text_data/preprocessing.html#tf-idf",
    "title": "Preprocessing",
    "section": "TF-IDF",
    "text": "TF-IDF\nTF-IDF (term frequency-inverse document frequency) gives us the frequency of a word per document, weighted by how often the word appears in the corpus. The main reason for using TF-IDF instead of word counts is that it highlights words that are distinctive and meaningful within a corpus.\nThe formula for TF-IDF is quite simple if spelled out simply:\n\\[ tf-idf = tf \\times idf\\]\nwhere: \\[ tf = \\frac{token\\ count\\ in\\ document}{total\\ tokens\\ in\\ the\\ document} \\]\nand: \\[ idf = log(\\frac{total\\ amount\\ of\\ documents}{number\\ of\\ documents\\ that\\ contains\\ the\\ token}) \\]\nWe can calcultate TF-IDF with the bind_tf_idf() function:\n\nausten_tfidf &lt;- austen_books() %&gt;%\n  unnest_tokens(token, text) %&gt;%\n  count(book, token, sort = TRUE) %&gt;%\n  bind_tf_idf(term = token, document = book, n = n) %&gt;%\n  arrange(desc(tf_idf))\n\nausten_tfidf\n\n# A tibble: 40,379 × 6\n   book                token         n      tf   idf  tf_idf\n   &lt;fct&gt;               &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Sense & Sensibility elinor      623 0.00519  1.79 0.00931\n 2 Sense & Sensibility marianne    492 0.00410  1.79 0.00735\n 3 Mansfield Park      crawford    493 0.00307  1.79 0.00551\n 4 Pride & Prejudice   darcy       373 0.00305  1.79 0.00547\n 5 Persuasion          elliot      254 0.00304  1.79 0.00544\n 6 Emma                emma        786 0.00488  1.10 0.00536\n 7 Northanger Abbey    tilney      196 0.00252  1.79 0.00452\n 8 Emma                weston      389 0.00242  1.79 0.00433\n 9 Pride & Prejudice   bennet      294 0.00241  1.79 0.00431\n10 Persuasion          wentworth   191 0.00228  1.79 0.00409\n# ℹ 40,369 more rows"
  },
  {
    "objectID": "text_data/preprocessing.html#punctuation-numbers-and-more",
    "href": "text_data/preprocessing.html#punctuation-numbers-and-more",
    "title": "Preprocessing",
    "section": "Punctuation, numbers, and more",
    "text": "Punctuation, numbers, and more\nIt is not uncommon to remove punctuation and numbers in text analysis. Punctuation is usually removed, because it does not give us any particular information in a standard bag of words model. Nevertheless, punctuation can be relevant information if you want to divide texts into, for example, sentences. It may also be relevant to take care of things such as the paragraph sign (§) if you are working with legal texts. Think carefully about which features you remove before you remove them.\nIn the unnest_tokens() function from tidytext, punctuation is automatically removed (but not all):"
  },
  {
    "objectID": "text_data/preprocessing.html#stopwords",
    "href": "text_data/preprocessing.html#stopwords",
    "title": "Preprocessing",
    "section": "Stopwords",
    "text": "Stopwords\nStopwords are a subset of function words (and in some contexts other words) which are considered to be of little importance for the meaning of a text in text analysis. Most stopwords are function words, but it is not uncommon to remove features based on the context of a corpus; the word “honourable” would be of little importance in a corpus of speeches from the UK House of Commons, but possibly very important in a corpus of moral philosophy books.\nIn R, there are a couple of alternatives for identifying and removing stopwords. The stopwords package has the stopwords() function, which includes eight different stopword dictionaries with varying language support:\n\nlibrary(stopwords)\n\nsrc &lt;- stopwords_getsources()\nsrc\n\n[1] \"snowball\"      \"stopwords-iso\" \"misc\"          \"smart\"        \n[5] \"marimo\"        \"ancient\"       \"nltk\"          \"perseus\"      \n\nlangs &lt;- lapply(src, stopwords_getlanguages)\nnames(langs) &lt;- src\n\nlangs\n\n$snowball\n [1] \"da\" \"de\" \"en\" \"es\" \"fi\" \"fr\" \"hu\" \"ir\" \"it\" \"nl\" \"no\" \"pt\" \"ro\" \"ru\" \"sv\"\n\n$`stopwords-iso`\n [1] \"af\" \"ar\" \"hy\" \"eu\" \"bn\" \"br\" \"bg\" \"ca\" \"zh\" \"hr\" \"cs\" \"da\" \"nl\" \"en\" \"eo\"\n[16] \"et\" \"fi\" \"fr\" \"gl\" \"de\" \"el\" \"ha\" \"he\" \"hi\" \"hu\" \"id\" \"ga\" \"it\" \"ja\" \"ko\"\n[31] \"ku\" \"la\" \"lt\" \"lv\" \"ms\" \"mr\" \"no\" \"fa\" \"pl\" \"pt\" \"ro\" \"ru\" \"sk\" \"sl\" \"so\"\n[46] \"st\" \"es\" \"sw\" \"sv\" \"th\" \"tl\" \"tr\" \"uk\" \"ur\" \"vi\" \"yo\" \"zu\"\n\n$misc\n[1] \"ar\" \"ca\" \"el\" \"gu\" \"zh\"\n\n$smart\n[1] \"en\"\n\n$marimo\n[1] \"en\"    \"de\"    \"ru\"    \"ar\"    \"he\"    \"zh_tw\" \"zh_cn\" \"ko\"    \"ja\"   \n\n$ancient\n[1] \"grc\" \"la\" \n\n$nltk\n [1] \"ar\" \"az\" \"da\" \"nl\" \"en\" \"fi\" \"fr\" \"de\" \"el\" \"hu\" \"id\" \"it\" \"kk\" \"ne\" \"no\"\n[16] \"pt\" \"ro\" \"ru\" \"sl\" \"es\" \"sv\" \"tg\" \"tr\"\n\n$perseus\n[1] \"grc\" \"la\" \n\n\nThe most common source used in social science applications is probably the stopword list from SnowballC. Although, at least for Norwegian, this usage is probably a product of path dependency rather than quality:\n\nstopwords(\"no\", source = \"snowball\")\n\n  [1] \"og\"        \"i\"         \"jeg\"       \"det\"       \"at\"        \"en\"       \n  [7] \"et\"        \"den\"       \"til\"       \"er\"        \"som\"       \"på\"       \n [13] \"de\"        \"med\"       \"han\"       \"av\"        \"ikke\"      \"ikkje\"    \n [19] \"der\"       \"så\"        \"var\"       \"meg\"       \"seg\"       \"men\"      \n [25] \"ett\"       \"har\"       \"om\"        \"vi\"        \"min\"       \"mitt\"     \n [31] \"ha\"        \"hadde\"     \"hun\"       \"nå\"        \"over\"      \"da\"       \n [37] \"ved\"       \"fra\"       \"du\"        \"ut\"        \"sin\"       \"dem\"      \n [43] \"oss\"       \"opp\"       \"man\"       \"kan\"       \"hans\"      \"hvor\"     \n [49] \"eller\"     \"hva\"       \"skal\"      \"selv\"      \"sjøl\"      \"her\"      \n [55] \"alle\"      \"vil\"       \"bli\"       \"ble\"       \"blei\"      \"blitt\"    \n [61] \"kunne\"     \"inn\"       \"når\"       \"være\"      \"kom\"       \"noen\"     \n [67] \"noe\"       \"ville\"     \"dere\"      \"som\"       \"deres\"     \"kun\"      \n [73] \"ja\"        \"etter\"     \"ned\"       \"skulle\"    \"denne\"     \"for\"      \n [79] \"deg\"       \"si\"        \"sine\"      \"sitt\"      \"mot\"       \"å\"        \n [85] \"meget\"     \"hvorfor\"   \"dette\"     \"disse\"     \"uten\"      \"hvordan\"  \n [91] \"ingen\"     \"din\"       \"ditt\"      \"blir\"      \"samme\"     \"hvilken\"  \n [97] \"hvilke\"    \"sånn\"      \"inni\"      \"mellom\"    \"vår\"       \"hver\"     \n[103] \"hvem\"      \"vors\"      \"hvis\"      \"både\"      \"bare\"      \"enn\"      \n[109] \"fordi\"     \"før\"       \"mange\"     \"også\"      \"slik\"      \"vært\"     \n[115] \"være\"      \"båe\"       \"begge\"     \"siden\"     \"dykk\"      \"dykkar\"   \n[121] \"dei\"       \"deira\"     \"deires\"    \"deim\"      \"di\"        \"då\"       \n[127] \"eg\"        \"ein\"       \"eit\"       \"eitt\"      \"elles\"     \"honom\"    \n[133] \"hjå\"       \"ho\"        \"hoe\"       \"henne\"     \"hennar\"    \"hennes\"   \n[139] \"hoss\"      \"hossen\"    \"ikkje\"     \"ingi\"      \"inkje\"     \"korleis\"  \n[145] \"korso\"     \"kva\"       \"kvar\"      \"kvarhelst\" \"kven\"      \"kvi\"      \n[151] \"kvifor\"    \"me\"        \"medan\"     \"mi\"        \"mine\"      \"mykje\"    \n[157] \"no\"        \"nokon\"     \"noka\"      \"nokor\"     \"noko\"      \"nokre\"    \n[163] \"si\"        \"sia\"       \"sidan\"     \"so\"        \"somt\"      \"somme\"    \n[169] \"um\"        \"upp\"       \"vere\"      \"vore\"      \"verte\"     \"vort\"     \n[175] \"varte\"     \"vart\"     \n\n\nYou can remove stopwords quite easily with the tidytext package. Consider the following example:\n\n1persuasion %&gt;%\n2  tibble(text = .,\n         line = 1:length(.)) %&gt;%\n3  unnest_tokens(token, text) %&gt;%\n4  count(token) %&gt;%\n5  arrange(desc(n))\n\npersuasion %&gt;% \n  tibble(text = .,\n         line = 1:length(.)) %&gt;% \n  unnest_tokens(token, text,\n6                stopwords = stopwords(\"en\", source = \"nltk\")) %&gt;%\n  count(token) %&gt;% \n  arrange(desc(n))\n\n\n1\n\nUsing the book “Persuasion” by Jane Austen (janeaustenr package)\n\n2\n\nMaking a tibble of the text and line number\n\n3\n\nTokenizing the text (spliting into words)\n\n4\n\nCounting the occurence of each token\n\n5\n\nSorting by number of tokens\n\n6\n\nSame as 1-5, but now with removing stopwords\n\n\n\n\n# A tibble: 5,858 × 2\n   token     n\n   &lt;chr&gt; &lt;int&gt;\n 1 the    3329\n 2 to     2808\n 3 and    2800\n 4 of     2570\n 5 a      1594\n 6 in     1389\n 7 was    1337\n 8 her    1203\n 9 had    1187\n10 she    1146\n# ℹ 5,848 more rows\n# A tibble: 5,732 × 2\n   token       n\n   &lt;chr&gt;   &lt;int&gt;\n 1 &lt;NA&gt;     1144\n 2 could     451\n 3 anne      447\n 4 would     355\n 5 captain   303\n 6 mrs       291\n 7 mr        256\n 8 elliot    254\n 9 one       231\n10 must      228\n# ℹ 5,722 more rows\n\n\nHow many stopwords were removed? Does the amount differ between different stopword dictionaries?\n\nIDF as stopwords\nA different and more practical approach to removing stopwords is to let the corpus itself generate the stopword list by using the inverse document frequency (IDF), discussed above. The intuition being that the lower IDF, the less important the word is as a differentiator. This method is particularly useful if we want to analyze the difference between texts.\n\nidf_order &lt;- austen_tfidf %&gt;% \n  arrange(idf)\n\npersuasion %&gt;% \n  tibble(text = .,\n         line = 1:length(.)) %&gt;% \n  unnest_tokens(token, text,\n                stopwords = idf_order$token[which(idf_order$idf == 0)]) %&gt;%\n  count(token) %&gt;% \n  arrange(desc(n))\n\n# A tibble: 3,275 × 2\n   token         n\n   &lt;chr&gt;     &lt;int&gt;\n 1 &lt;NA&gt;       3222\n 2 anne        447\n 3 captain     303\n 4 elliot      254\n 5 wentworth   191\n 6 charles     155\n 7 walter      123\n 8 mary        121\n 9 russell     118\n10 musgrove    111\n# ℹ 3,265 more rows"
  },
  {
    "objectID": "text_data/preprocessing.html#visualize",
    "href": "text_data/preprocessing.html#visualize",
    "title": "Preprocessing",
    "section": "Visualize",
    "text": "Visualize\n\nlibrary(ggplot2)\n\ntibble(text = pp) %&gt;% \n  unnest_tokens(output = token, \n                input = text,\n                token = \"ngrams\",\n                n = 2) %&gt;% \n  count(token) %&gt;% \n  arrange(desc(n)) %&gt;% \n  mutate(rank = 1:nrow(.)) %&gt;% \n  filter(n &gt; 1) %&gt;% \n  ggplot(aes(x = log(rank), y = log(n))) +\n  geom_point() +\n  geom_path(aes(group = 1)) +\n  ggrepel::geom_label_repel(aes(label = token),\n                            max.overlaps = 5)"
  },
  {
    "objectID": "text_data/preprocessing.html#footnotes",
    "href": "text_data/preprocessing.html#footnotes",
    "title": "Preprocessing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nConsider, for instance, “I’ll do it as quickly as possible”: if we remove “as”, the sentence would be a lot more confusing.↩︎\n elver is defined as “a young eel, especially when undergoing mass migration upriver from the sea”↩︎"
  },
  {
    "objectID": "text_data/regex.html",
    "href": "text_data/regex.html",
    "title": "Regular expressions",
    "section": "",
    "text": "Regular expressions – commonly referred to as regex – is a pattern matching language for text. Regex provides a way to define patterns in language to detect, extract, remove, and replace segments of text. We will cover more practical uses of regex in the extracting data from texts section. But in this section, we will demonstrate the basics of regex and how to use it in R. We will use the stringr package for matching strings with patterns as it is a vital part of the tidyverse."
  },
  {
    "objectID": "text_data/regex.html#initial-example",
    "href": "text_data/regex.html#initial-example",
    "title": "Regular expressions",
    "section": "Initial example",
    "text": "Initial example\nAs a simple initial example, consider the following sentence:\n\nGro Harlem Brundtland (84) was the Prime Minister of Norway in 1981, 1986 to 1989, and 1990 to 1996.\n\nIf we want to extract all years mentioned in the sentence, we can use regex:\n\nlibrary(stringr)\n\nex_text &lt;- \"Gro Harlem Brundtland (84) was the Prime Minister of Norway in 1981,\n            1986 to 1989, and 1990 to 1996.\"\n\n1str_extract(ex_text, \"[0-9]\")\n\n2str_extract_all(ex_text, \"[0-9]\")\n\n3str_extract_all(ex_text, \"[0-9]+\")\n\n4str_extract_all(ex_text, \"[0-9]{4}\")\n\n\n1\n\nUsing str_extract() will only return the first match; here, that is any number between 0 and 9 – “[0-9]”\n\n2\n\nUsing str_extract_all() will extract all numbers; here, that is any number between 0 and 9 – “[0-9]”\n\n3\n\nAdding a “+” behind “[0-9]” indicates that we want this pattern – any number between 0 and 9 to match one or more times\n\n4\n\nAdding a “{4}” behind “[0-9]” indicates that we want this pattern – any number between 0 and 9 – to match exactly 4 times\n\n\n\n\n[1] \"8\"\n[[1]]\n [1] \"8\" \"4\" \"1\" \"9\" \"8\" \"1\" \"1\" \"9\" \"8\" \"6\" \"1\" \"9\" \"8\" \"9\" \"1\" \"9\" \"9\" \"0\" \"1\"\n[20] \"9\" \"9\" \"6\"\n\n[[1]]\n[1] \"84\"   \"1981\" \"1986\" \"1989\" \"1990\" \"1996\"\n\n[[1]]\n[1] \"1981\" \"1986\" \"1989\" \"1990\" \"1996\"\n\n\nNotice how the lines 1 through 3 does not extract the years correctly or just the years; regex is extremely precise in that it will give you exactly what you ask for."
  },
  {
    "objectID": "text_data/regex.html#usage",
    "href": "text_data/regex.html#usage",
    "title": "Regular expressions",
    "section": "Usage",
    "text": "Usage\nThe four most common regex usages and their corresponding stringr functions, with examples are listed in the table below. Remember that all of these functions also have a *_all version, for matching multiple times.\n\n\n\n\n\n\n\n\n\n\n\nstringr\nDescription\nExample\nExample return\n\n\n\n\nDetect\nstr_detect(string, pattern)\nChecks if a pattern exists in a string.\nstr_detect(\"Hello world\", \"world\")\nTRUE\n\n\nExtract\nstr_extract(string, pattern)\nExtracts the first match of a pattern from a string.\nstr_extract(\"Age: 25\", \"\\\\d+\")\n25\n\n\nRemove\nstr_remove(string, pattern)\nRemoves the first occurrence of a pattern from a string.\nstr_remove(\"Hello, world\", \"\\\\W\")\nHello world\n\n\nReplace\nstr_replace(string, pattern, replacement)\nReplaces the first match of a pattern with a replacement.\nstr_replace(\"Colors: red blue\", \"\\\\w+\", \"green\")\ngreen: red blue"
  },
  {
    "objectID": "text_data/regex.html#special-characters",
    "href": "text_data/regex.html#special-characters",
    "title": "Regular expressions",
    "section": "Special characters",
    "text": "Special characters\nThere are a lot of special characters in regex, and often we can extract the same text with completely different regex (e.g \\\\d and [0-9]). There are, however, some common ones that we use more often than othere. Here is a list of 10 commonly used regex special characters:\n\n\n\n\n\n\n\n\nSpecial character\nDescription\nstringr example\n\n\n\n\n.\nMatches any single character except a newline.\nstr_detect(\"a.b\", \"a.b\")\n\n\n*\nMatches zero or more occurrences of the preceding character.\nstr_detect(\"abbbc\", \"ab*c\")\n\n\n+\nMatches one or more occurrences of the preceding character.\nstr_detect(\"abbbc\", \"ab+c\")\n\n\n?\nMatches zero or one occurrence of the preceding character.\nstr_detect(\"color\", \"colou?r\")\n\n\n[ ]\nDefines a character class, allowing you to specify a range of characters.\nstr_detect(\"apple\", \"[aeiou]\")\n\n\n( )\nDefines a capturing group for capturing and extracting matched text.\nstr_extract(\"abab\", \"(ab)+\")\n\n\n\\\nEscapes a special character, treating it as a regular character.\nstr_detect(\"www.example.com\", \"\\\\.\")\n\n\n^\nMatches the beginning of a line or string.\nstr_detect(\"Start of line\", \"^Start\")\n\n\n$\nMatches the end of a line or string.\nstr_detect(\"end of line\", \"end$\")\n\n\n\\d\nMatches any digit (0-9).\nstr_extract(\"123\", \"\\\\d{3}\")\n\n\n\nTry running the stringr examples in R!"
  },
  {
    "objectID": "text_data/extracting_data_from_texts.html",
    "href": "text_data/extracting_data_from_texts.html",
    "title": "Extracting data from texts",
    "section": "",
    "text": "When we are working with uncleaned data, but sometimes also cleaned data, we often want to extract data from text that come with the data. This might be data such as dates, names, numbers, standardized sequences of text, and so on. In the following, we will illustrate some ways of extracting data from texts. We will use State of the Union speeches (SOTU) as an example. We will show the various extractions with one of the speeches as the running example. The structuring of the entire corpus is shown at the end. But, first lets do a short recap on how to structure HTML files (but see the webscraping section for a detailed run through):\nCode\nlibrary(rvest)\nlibrary(tidyverse)\n1# download.file(\"https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/annual-messages-congress-the-state-the-union\",\n#              destfile = \"../data/text_extract/sotu_table.html\")\n\n2sotu_tab_page &lt;- read_html(\"../data/text_extract/sotu_table.html\")\n\n3links &lt;- sotu_tab_page %&gt;%\n  html_nodes(\"a\") %&gt;%\n  html_attr(\"href\")\n\nlinks &lt;- links[which(str_detect(links, \"pid\\\\=[0-9]{4}\"))]\n  \n4pbapply::pblapply(links, function(x) {\n  \n  download.file(x, destfile = str_c(\"../data/text_extract/sotu/\",\n                                    str_extract(x, \"[0-9]+$\"),\n                                    \".html\"), quiet = TRUE)\n  \n5  Sys.sleep(2 + abs(rnorm(1)))\n\n})\n\n\n\n1\n\nDownloading the page containing the table\n\n2\n\nReading the .html file\n\n3\n\nExtracting the correct table and subsetting the links ending with four numbers.\n\n4\n\nDownloading the links (there are over 200 in total, so it will take a while)\n\n5\n\nAdding random sleep between each call\n1sotu &lt;- read_html(\"../data/text_extract/sotu/104596.html\")\n\ntext &lt;- sotu %&gt;% \n2  html_nodes(\"div[class='field-docs-content']\") %&gt;%\n3  html_text()\n\n4str_sub(text, 1, 50)\n\n\n1\n\nReading one .html file for testing\n\n2\n\nExtracting the speech content field of the speech\n\n3\n\nConverting to text\n\n4\n\nViewing the start of the string\n\n\n\n\n[1] \"\\n    The President. Mr. Speaker, Mr. Vice Presiden\""
  },
  {
    "objectID": "text_data/extracting_data_from_texts.html#extract-reactions",
    "href": "text_data/extracting_data_from_texts.html#extract-reactions",
    "title": "Extracting data from texts",
    "section": "Extract reactions",
    "text": "Extract reactions\nIn the SOTU speeches, there are often recorded reactions from the audience. For instance, in Trump’s speech from 2017:\n\n[Laughter] And they wanted me to ride one, and I said, “No, thank you.” [Laughter]\n\nAssuming that all reactions from the audience comes in square brackets (“[]”), we can extract all of these:\n\nreactions &lt;- text %&gt;% \n1  str_extract_all(\"\\\\[(.*?)\\\\]\") %&gt;%\n2  unlist() %&gt;%\n3  str_remove_all(\"\\\\[|\\\\]\")\n\nreactions\n\n\n1\n\nExtracting segments starting with “[” and ending with ”]”, but also everything between the two.\n\n2\n\nUnlisting into a character vector\n\n3\n\nRemoving “[” and ”]” from the string\n\n\n\n\n [1] \"Applause\" \"applause\" \"Applause\" \"Applause\" \"Laughter\" \"Laughter\"\n [7] \"Laughter\" \"Laughter\" \"Laughter\" \"Laughter\" \"Applause\" \"Laughter\"\n[13] \"Laughter\"\n\n\nSeeing as there are both laughs and applauses, we might want to separate these:\n\n1n_laugh &lt;- table(reactions)[\"Laughter\"]\nn_applause &lt;- table(reactions)[\"Applause\"]\n\nc(n_laugh, n_applause)\n\n\n1\n\nCounting number of recorded laughs and applauses in the audience\n\n\n\n\nLaughter Applause \n       8        4 \n\n\nNotice how this does not count the “applause” with lower case “a”. If we want to also count this, we can use regex to summarize different spelling:\n\nn_laugh &lt;- sum(str_detect(reactions, \"^[Ll]\"))\nn_applause &lt;- sum(str_detect(reactions, \"^[Aa]\"))\n\nc(n_laugh, n_applause)\n\n[1] 8 5"
  },
  {
    "objectID": "text_data/extracting_data_from_texts.html#dates",
    "href": "text_data/extracting_data_from_texts.html#dates",
    "title": "Extracting data from texts",
    "section": "Dates",
    "text": "Dates\nNext, looking at the SOTU speeches, there is a standardized segment for the date the speech was held. We can extract this node and convert it to a date. Because the dates contain months in letters, we have to set the correct locale in order to convert the dates correctly, as we discussed in the section on locales and encoding.\n\n1Sys.setlocale(\"LC_TIME\", \"en_US.UTF-8\")\n\ndate &lt;- sotu %&gt;% \n2  html_nodes(\"div[class='field-docs-start-date-time']\") %&gt;%\n3  html_text() %&gt;%\n4  str_trim() %&gt;%\n5  as.Date(\"%B %d, %Y\")\n\n6Sys.setlocale(\"LC_TIME\", \"nb_NO.UTF-8\")\n\n\n1\n\nSetting time locale to US\n\n2\n\nExtracting the date html node\n\n3\n\nConverting to text\n\n4\n\nStripping leading and trailing whitespace\n\n5\n\nConverting the vector to date (%B is month in letters, %d is day in two numbers, and %Y is years in four numbers)\n\n6\n\nReverting to Norwegian locale\n\n\n\n\n[1] \"en_US.UTF-8\"\n[1] \"nb_NO.UTF-8\"\n\n\nWe have now converted the date to a a “Date” class.\n\ndate ; class(date)\n\n[1] \"2014-01-28\"\n\n\n[1] \"Date\""
  },
  {
    "objectID": "text_data/extracting_data_from_texts.html#person",
    "href": "text_data/extracting_data_from_texts.html#person",
    "title": "Extracting data from texts",
    "section": "Person",
    "text": "Person\nIn the SOTU HTML files, there is also a div node containing the name of the President holding the speech. The node does, however, contain some other information as well:\n\nsotu %&gt;% \n  html_nodes(\"div[class='field-docs-person']\") %&gt;%                         \n  html_text()                                                          \n\n[1] \"\\n    \\n\\n  \\n  \\n  \\n    \\n\\n  \\n    Barack Obama  \\n\\n  \\n    \\n  \\n    44th President of the United States: 2009 ‐ 2017\\n  \\n  \\n\\n  \\n    Address Before a Joint Session of the Congress on the State of the Union\\n  \\n\\n\\n  \"\n\n\nWe can extract the segment – the name of the president – we want by using regex:\n\npres &lt;- sotu %&gt;% \n1  html_nodes(\"div[class='field-docs-person']\") %&gt;%\n2  html_text() %&gt;%\n3  str_trim() %&gt;%\n4  str_extract(\"^([A-Za-z]+\\\\s{0,1}\\\\.*)+\") %&gt;%\n5  str_trim()\n\npres_number &lt;- sotu %&gt;% \n6  html_nodes(\"div[class='field-docs-person']\") %&gt;%\n7  html_text() %&gt;%\n8  str_trim() %&gt;%\n9  str_extract(\"[0-9]+(th|st)\")\n\n\n1\n\nExtracting the president (“person”) node\n\n2\n\nConverting to text\n\n3\n\nTrimming leading/trailing whitespace\n\n4\n\nExtracting the start of the string (“^”), and one or more segments of any upper case or lower case character, followed by 0 or 1 whitespace, followed by 0 or more dots (“.”).\n\n5\n\nTrimming whitespace\n\n6\n\nExtracting the president (“person”) node\n\n7\n\nConverting to text\n\n8\n\nTrimming leading/trailing whitespace\n\n9\n\nExtracting the first segment of any number one or more times, followed by either “th” or “st”."
  },
  {
    "objectID": "text_data/extracting_data_from_texts.html#collect-everything",
    "href": "text_data/extracting_data_from_texts.html#collect-everything",
    "title": "Extracting data from texts",
    "section": "Collect everything",
    "text": "Collect everything\nNow we can insert everything into a tibble:\n\n1sotu_df &lt;- tibble(pres, pres_number,\n                  date,\n                  reactions = str_c(reactions, collapse = \"/\"),\n                  n_laugh, n_applause)\n\n\n2sotu_df\n\n\n1\n\nInserting all variables into a tibble\n\n2\n\nViewing the tibble\n\n\n\n\n# A tibble: 1 × 6\n  pres         pres_number date       reactions               n_laugh n_applause\n  &lt;chr&gt;        &lt;chr&gt;       &lt;date&gt;     &lt;chr&gt;                     &lt;int&gt;      &lt;int&gt;\n1 Barack Obama 44th        2014-01-28 Applause/applause/Appl…       8          5\n\n\nOf course, we want to standardize this for all speeches:\n\n\nCode\nall_sotu &lt;- list.files(\"../data/text_extract/sotu\", full.names = TRUE)\n\nsotu_data &lt;- lapply(all_sotu, function(x) {\n  \n  if(file.size(x) == 0) return(NULL)\n  \n  sotu &lt;- read_html(x)\n  \n  text &lt;- sotu %&gt;% \n    html_nodes(\"div[class='field-docs-content']\") %&gt;% \n    html_text()\n  \n  reactions &lt;- text %&gt;% \n    str_extract_all(\"\\\\[(.*?)\\\\]\") %&gt;% \n    unlist() %&gt;% \n    str_remove_all(\"\\\\[|\\\\]\") \n  \n  n_laugh &lt;- length(which(str_detect(reactions, \"[Ll]augh\")))\n  n_applause &lt;- length(which(str_detect(reactions, \"[Aa]pplau\")))\n  \n  Sys.setlocale(\"LC_TIME\", \"en_US.UTF-8\")\n  \n  date &lt;- sotu %&gt;% \n    html_nodes(\"div[class='field-docs-start-date-time']\") %&gt;% \n    html_text() %&gt;% \n    str_trim() %&gt;% \n    as.Date(\"%B %d, %Y\", lo)\n  \n  Sys.setlocale(\"LC_TIME\", \"nb_NO.UTF-8\")\n  \n  pres &lt;- sotu %&gt;% \n    html_nodes(\"div[class='field-docs-person']\") %&gt;% \n    html_text() %&gt;% \n    str_trim() %&gt;% \n    str_extract(\"^([A-Za-z]+\\\\s{0,1}\\\\.*)+\") %&gt;%\n    str_trim() \n  \n  pres_number &lt;- sotu %&gt;% \n    html_nodes(\"div[class='field-docs-person']\") %&gt;% \n    html_text() %&gt;% \n    str_trim() %&gt;% \n    str_extract(\"[0-9]+(th|st)\")\n  \n  \n  sotu_df &lt;- tibble(pres, pres_number,\n                    date,\n                    reactions = str_c(reactions, collapse = \"/\"), \n                    n_laugh, n_applause)\n  \n  \n  return(sotu_df)\n  \n})\n\nsotu_data &lt;- bind_rows(sotu_data)\n\nsotu_data\n\n\n# A tibble: 241 × 6\n   pres         pres_number date       reactions              n_laugh n_applause\n   &lt;chr&gt;        &lt;chr&gt;       &lt;date&gt;     &lt;chr&gt;                    &lt;int&gt;      &lt;int&gt;\n 1 Dwight D.    34th        1954-01-07 \"\"                           0          0\n 2 Barack Obama 44th        2013-02-12 \"Laughter/Applause/Ap…       1          4\n 3 Dwight D.    34th        1955-01-06 \"\"                           0          0\n 4 Barack Obama 44th        2014-01-28 \"Applause/applause/Ap…       8          5\n 5 Dwight D.    34th        1956-01-05 \"Read before a joint …       0          0\n 6 Dwight D.    34th        1956-01-05 \"Recorded on film and…       0          0\n 7 Barack Obama 44th        2015-01-20 \"Laughter/Laughter/la…       9          5\n 8 Dwight D.    34th        1957-01-10 \"\"                           0          0\n 9 Barack Obama 44th        2016-01-12 \"Laughter/Laughter/la…      11          5\n10 Dwight D.    34th        1958-01-09 \"\"                           0          0\n# ℹ 231 more rows\n\n\nTake note of any errors that might occur because of differences between the speeches. For instance, what do you think might have happened to the pres_number for the rows with NA?"
  },
  {
    "objectID": "text_data/extracting_data_from_texts.html#visualize",
    "href": "text_data/extracting_data_from_texts.html#visualize",
    "title": "Extracting data from texts",
    "section": "Visualize",
    "text": "Visualize\n\nlibrary(ggplot2)\n\nsotu_data %&gt;% \n  ggplot(aes(x = n_laugh, y = n_applause)) +\n  geom_point() +\n  geom_jitter() +\n  geom_smooth(method = \"lm\") +\n  ggrepel::geom_label_repel(\n    aes(label = ifelse(str_detect(pres, \"Obama\"), \"Obama\", NA)),\n    min.segment.length = 0\n    ) +\n  labs(x = \"# laugh\", y = \"# applause\") +\n  theme_classic()\n\n\n\n\nWhat do you think is a probable driver for the reactions based on this figure? How would you improve this figure? Hint: dates!\n\n\nCode\nsotu_data %&gt;% \n  filter(date &gt; as.Date(\"1970-01-01\")) %&gt;% \n  ggplot(aes(x = date, y = n_applause + n_laugh)) +\n  geom_point() +\n  geom_smooth() +\n  ggrepel::geom_label_repel(\n    aes(label = ifelse(str_detect(pres, \"Obama\"), \"Obama\", NA)),\n    min.segment.length = 0\n    ) +\n  labs(x = \"# reactions\", y = \"# applause\") +\n  theme_classic()"
  },
  {
    "objectID": "text_data/text_intro.html",
    "href": "text_data/text_intro.html",
    "title": "Political Text as Data",
    "section": "",
    "text": "We have experienced how powerful models of language can be over the last year\nwith the popularization of large language models (LLM). In this section, we will discuss how to work with text in R both in terms of reading text data into R, matching text patterns with regex, extracting data from text, and how do preprocess text.\nWe will utilize the stringr package and Regular Expressions extensively. The stringr package is a core component of the tidyverse and has a wide range of functions for working with text. Most of these start with str_*, in the sense that it says “string [something]” (str_detect() means “string detect”):\n\n\n [1] \"str_c\"             \"str_conv\"          \"str_count\"        \n [4] \"str_detect\"        \"str_dup\"           \"str_ends\"         \n [7] \"str_equal\"         \"str_escape\"        \"str_extract\"      \n[10] \"str_extract_all\"   \"str_flatten\"       \"str_flatten_comma\"\n[13] \"str_glue\"          \"str_glue_data\"     \"str_interp\"       \n[16] \"str_length\"        \"str_like\"          \"str_locate\"       \n[19] \"str_locate_all\"    \"str_match\"         \"str_match_all\"    \n[22] \"str_order\"         \"str_pad\"           \"str_rank\"         \n[25] \"str_remove\"        \"str_remove_all\"    \"str_replace\"      \n[28] \"str_replace_all\"   \"str_replace_na\"    \"str_sort\"         \n[31] \"str_split\"         \"str_split_1\"       \"str_split_fixed\"  \n[34] \"str_split_i\"       \"str_squish\"        \"str_starts\"       \n[37] \"str_sub\"           \"str_sub_all\"       \"str_sub&lt;-\"        \n[40] \"str_subset\"        \"str_to_lower\"      \"str_to_sentence\"  \n[43] \"str_to_title\"      \"str_to_upper\"      \"str_trim\"         \n[46] \"str_trunc\"         \"str_unique\"        \"str_view\"         \n[49] \"str_view_all\"      \"str_which\"         \"str_width\"        \n[52] \"str_wrap\"         \n\n\nSome of these have base equivalent functions. Some examples are:\n\n\n\nstringr\nbase\n\n\n\n\nstr_c()\npaste()\n\n\nstr_detect()\ngrepl()\n\n\nstr_sub()\nsubstr()\n\n\nstr_replace()\ngsub()\n\n\nstr_split()\nstrsplit()\n\n\nstr_conv()\niconv()\n\n\n…\n…"
  },
  {
    "objectID": "Quarto_documents/producing_tables.html",
    "href": "Quarto_documents/producing_tables.html",
    "title": "Producing Tables from R",
    "section": "",
    "text": "When including tables that include regression model, descriptive statistics, or something else that you produce in R, you would want to format the table in R and export it directly to your Quarto document from an R chunk in your .qmd-file. You do not want to manually copy-paste results from R and add them to a table and you don’t want to spend time manually formatting the tables to make them look nice. You want to automatize this process so that you don’t have to redo it every time something changes in your analysis!\nThere are lots of different R packages developed exactly for this purpose. Here we provide some illustrations, using the modelsummary package developed by Arel-Bundock (2022) and the gt package from the Posit team. Both packages have extensive and excellent documentation. The modelsummary documentation is available here and The gt documentation is available here."
  },
  {
    "objectID": "Quarto_documents/producing_tables.html#omitting-fixed-effects-coefficients",
    "href": "Quarto_documents/producing_tables.html#omitting-fixed-effects-coefficients",
    "title": "Producing Tables from R",
    "section": "Omitting fixed effects coefficients",
    "text": "Omitting fixed effects coefficients\nFirst, we don’t want to include all the governorate fixed effects coefficients in the table. We can omit some of the coefficients using the coef_omit argument. If we set coef_omit = \"as.factor\" all coefficients with names that start with “as.factor” will be omitted from the table:\n\nmodelsummary(models = list(grewal_model_1, grewal_model_2 ), \n             coef_omit = \"as.factor\")\n\n\n\n\n\n (1)\n  (2)\n\n\n\n\n(Intercept)\n0.340\n0.340\n\n\n\n(0.116)\n(0.116)\n\n\nnonvio2\n0.098\n0.087\n\n\n\n(0.028)\n(0.028)\n\n\nprime_frat\n−0.023\n−0.075\n\n\n\n(0.035)\n(0.040)\n\n\nprime_future\n−0.073\n−0.109\n\n\n\n(0.036)\n(0.040)\n\n\nprime_RU\n−0.066\n−0.066\n\n\n\n(0.037)\n(0.037)\n\n\nprime_UN\n−0.017\n−0.018\n\n\n\n(0.035)\n(0.035)\n\n\noppose92coup2\n0.042\n0.044\n\n\n\n(0.021)\n(0.022)\n\n\ncivwar\n0.014\n0.011\n\n\n\n(0.026)\n(0.026)\n\n\nislamist\n0.019\n0.018\n\n\n\n(0.030)\n(0.030)\n\n\nsupp_sharia\n0.158\n0.162\n\n\n\n(0.036)\n(0.036)\n\n\nactive\n−0.003\n0.000\n\n\n\n(0.024)\n(0.024)\n\n\nconscript\n−0.016\n−0.042\n\n\n\n(0.023)\n(0.025)\n\n\nsoldier\n0.013\n−0.006\n\n\n\n(0.024)\n(0.026)\n\n\njunoff\n0.033\n0.030\n\n\n\n(0.030)\n(0.030)\n\n\nsenoff\n0.110\n−0.117\n\n\n\n(0.061)\n(0.137)\n\n\nbranch2Land\n0.019\n0.018\n\n\n\n(0.028)\n(0.027)\n\n\ntrain_west\n−0.096\n−0.093\n\n\n\n(0.058)\n(0.058)\n\n\ntrain_russia\n−0.019\n−0.024\n\n\n\n(0.034)\n(0.034)\n\n\ntrain_china\n0.035\n0.044\n\n\n\n(0.105)\n(0.105)\n\n\nyoung\n0.133\n0.145\n\n\n\n(0.042)\n(0.049)\n\n\nsex\n0.020\n0.020\n\n\n\n(0.054)\n(0.054)\n\n\nedu\n−0.014\n−0.013\n\n\n\n(0.012)\n(0.012)\n\n\nurban_area\n0.006\n0.009\n\n\n\n(0.022)\n(0.021)\n\n\nemployed\n0.061\n0.063\n\n\n\n(0.024)\n(0.024)\n\n\nstudent\n−0.017\n−0.010\n\n\n\n(0.045)\n(0.045)\n\n\narabic\n0.094\n0.093\n\n\n\n(0.029)\n(0.029)\n\n\necon1\n−0.041\n−0.042\n\n\n\n(0.048)\n(0.048)\n\n\ncorr1\n0.002\n0.013\n\n\n\n(0.048)\n(0.048)\n\n\ndem\n0.127\n0.129\n\n\n\n(0.036)\n(0.036)\n\n\nsupp_opp_parties\n−0.055\n−0.054\n\n\n\n(0.034)\n(0.034)\n\n\ncontinue\n−0.033\n−0.037\n\n\n\n(0.022)\n(0.022)\n\n\nprotested\n0.090\n0.091\n\n\n\n(0.022)\n(0.022)\n\n\npreboutef\n−0.038\n−0.039\n\n\n\n(0.050)\n(0.049)\n\n\npost_exp\n−0.011\n−0.017\n\n\n\n(0.053)\n(0.053)\n\n\nmon\n−0.017\n−0.017\n\n\n\n(0.007)\n(0.007)\n\n\nnonvio2 × senoff\n\n0.272\n\n\n\n\n(0.146)\n\n\nprime_frat × conscript\n\n0.168\n\n\n\n\n(0.057)\n\n\nprime_future × soldier\n\n0.118\n\n\n\n\n(0.058)\n\n\noppose92coup2 × young\n\n−0.037\n\n\n\n\n(0.063)\n\n\nNum.Obs.\n1969\n1969\n\n\nR2\n0.110\n0.118\n\n\nR2 Adj.\n0.072\n0.078\n\n\nAIC\n2288.1\n2279.3\n\n\nBIC\n2757.2\n2770.8\n\n\nLog.Lik.\n−1060.025\n−1051.647\n\n\nF\n2.851\n2.923\n\n\nRMSE\n0.41\n0.41"
  },
  {
    "objectID": "Quarto_documents/producing_tables.html#rearranging-renaming-and-omitting-coefficients",
    "href": "Quarto_documents/producing_tables.html#rearranging-renaming-and-omitting-coefficients",
    "title": "Producing Tables from R",
    "section": "Rearranging, renaming, and omitting coefficients",
    "text": "Rearranging, renaming, and omitting coefficients\nSecond, we would to change the variable labels in the tables so that readers can understand what the different variables are. We might also want to change the order of the variables so that the interaction terms don’t end up at the bottom of the table. We can do both these things using the coef_map argument (coef_map can also be used to omit coefficients. We omit any coefficients not mentioned in coef_map so we then no longer need the coef_omit argument). The coef_map argument should be supplied a name list of the structure list(\"name_of_coefficient1\" = \"label 1\", \"name_of_coefficient2\" = \"label 2\"):\n\nmodelsummary(models = list(grewal_model_1, grewal_model_2 ), \n             #coef_omit = \"as.factor\",  ## We no longer need this because we can omit them in coef_map instead\n             coef_map = list(\"nonvio2\" = \"Nonviolent\", \n                              \"nonvio2:senoff\" = \"Nonviolent × Senior officer\",\n                            \"prime_frat\" =  \"Prime-Fraternization\", \n                             \"prime_frat:conscript\" = \"Prime-Fraternization × Conscript\", \n                             \"prime_future\" = \"Prime-Civilian Control\", \n                              \"prime_future:soldier\" = \"Prime-Civilian Control × Soldier\", \n                             \"prime_RU\" = \"Prime-Russian Support\", \n                                \"prime_RU:soldier\" = \"Prime-Russian Support × Soldier\", \n                             \"prime_UN\" = \"Prime-United Nations\", \n                             \"oppose92coup2\" = \"Oppose 1992 coup\",\n                               \"oppose92coup2:young\" = \"Oppose 1992 coup × Born after 1995\",\n                             \"active\" = \"Active-Duty\", \n                             \"conscript\" =  \"Conscript\", \n                             \"soldier\" = \"Soldier\",\n                             \"junoff\" = \"Junior officer\", \n                              \"senoff\" = \"Senior officer\", \n                             \"branch2Land\" = \"Army/Gendarmerie\",\n                             \"train_west\" = \"Trained in the West\", \n                             \"train_russia\" = \"Trained in Russia\", \n                             \"train_china\" = \"Trained in China\", \n                             \"civwar\" = \"Fought in the 1990s\", \n                             \"islamist\" = \"Islamist\", \n                             \"supp_sharia\" = \"Support Sharia\", \n                             \"econ1\" = \"Economy good\", \n                             \"corr1\" = \"Corruption high\", \n                             \"dem\" = \"Support democracy\", \n                             \"supp_opp_parties\" = \"Support opposition parties\", \n                             \"continue\" = \"Want Hirak to continue\", \n                             \"protested\" = \"Protested\", \n                             \"young\" = \"Born after 1995\", \n                             \"sex\" = \"Female\", \n                             \"edu\" = \"Education\", \n                            \"urban_area\" = \"Urban area\", \n                            \"employed\" = \"Employed\",\n                            \"student\" = \"Student\", \n                            \"arabic\" = \"Arab\", \n                           \"preboutef\" =  \"Pre-Bouteflika Ouster\", \n                           \"post_exp\" = \"Post-experiment\", \n                           \"mon\" = \"Month\", \n                           \"(Intercept)\" = \"Constant\"))\n\n\n\n\n\n (1)\n  (2)\n\n\n\n\nNonviolent\n0.098\n0.087\n\n\n\n(0.028)\n(0.028)\n\n\nNonviolent × Senior officer\n\n0.272\n\n\n\n\n(0.146)\n\n\nPrime-Fraternization\n−0.023\n−0.075\n\n\n\n(0.035)\n(0.040)\n\n\nPrime-Fraternization × Conscript\n\n0.168\n\n\n\n\n(0.057)\n\n\nPrime-Civilian Control\n−0.073\n−0.109\n\n\n\n(0.036)\n(0.040)\n\n\nPrime-Civilian Control × Soldier\n\n0.118\n\n\n\n\n(0.058)\n\n\nPrime-Russian Support\n−0.066\n−0.066\n\n\n\n(0.037)\n(0.037)\n\n\nPrime-United Nations\n−0.017\n−0.018\n\n\n\n(0.035)\n(0.035)\n\n\nOppose 1992 coup\n0.042\n0.044\n\n\n\n(0.021)\n(0.022)\n\n\nOppose 1992 coup × Born after 1995\n\n−0.037\n\n\n\n\n(0.063)\n\n\nActive-Duty\n−0.003\n0.000\n\n\n\n(0.024)\n(0.024)\n\n\nConscript\n−0.016\n−0.042\n\n\n\n(0.023)\n(0.025)\n\n\nSoldier\n0.013\n−0.006\n\n\n\n(0.024)\n(0.026)\n\n\nJunior officer\n0.033\n0.030\n\n\n\n(0.030)\n(0.030)\n\n\nSenior officer\n0.110\n−0.117\n\n\n\n(0.061)\n(0.137)\n\n\nArmy/Gendarmerie\n0.019\n0.018\n\n\n\n(0.028)\n(0.027)\n\n\nTrained in the West\n−0.096\n−0.093\n\n\n\n(0.058)\n(0.058)\n\n\nTrained in Russia\n−0.019\n−0.024\n\n\n\n(0.034)\n(0.034)\n\n\nTrained in China\n0.035\n0.044\n\n\n\n(0.105)\n(0.105)\n\n\nFought in the 1990s\n0.014\n0.011\n\n\n\n(0.026)\n(0.026)\n\n\nIslamist\n0.019\n0.018\n\n\n\n(0.030)\n(0.030)\n\n\nSupport Sharia\n0.158\n0.162\n\n\n\n(0.036)\n(0.036)\n\n\nEconomy good\n−0.041\n−0.042\n\n\n\n(0.048)\n(0.048)\n\n\nCorruption high\n0.002\n0.013\n\n\n\n(0.048)\n(0.048)\n\n\nSupport democracy\n0.127\n0.129\n\n\n\n(0.036)\n(0.036)\n\n\nSupport opposition parties\n−0.055\n−0.054\n\n\n\n(0.034)\n(0.034)\n\n\nWant Hirak to continue\n−0.033\n−0.037\n\n\n\n(0.022)\n(0.022)\n\n\nProtested\n0.090\n0.091\n\n\n\n(0.022)\n(0.022)\n\n\nBorn after 1995\n0.133\n0.145\n\n\n\n(0.042)\n(0.049)\n\n\nFemale\n0.020\n0.020\n\n\n\n(0.054)\n(0.054)\n\n\nEducation\n−0.014\n−0.013\n\n\n\n(0.012)\n(0.012)\n\n\nUrban area\n0.006\n0.009\n\n\n\n(0.022)\n(0.021)\n\n\nEmployed\n0.061\n0.063\n\n\n\n(0.024)\n(0.024)\n\n\nStudent\n−0.017\n−0.010\n\n\n\n(0.045)\n(0.045)\n\n\nArab\n0.094\n0.093\n\n\n\n(0.029)\n(0.029)\n\n\nPre-Bouteflika Ouster\n−0.038\n−0.039\n\n\n\n(0.050)\n(0.049)\n\n\nPost-experiment\n−0.011\n−0.017\n\n\n\n(0.053)\n(0.053)\n\n\nMonth\n−0.017\n−0.017\n\n\n\n(0.007)\n(0.007)\n\n\nConstant\n0.340\n0.340\n\n\n\n(0.116)\n(0.116)\n\n\nNum.Obs.\n1969\n1969\n\n\nR2\n0.110\n0.118\n\n\nR2 Adj.\n0.072\n0.078\n\n\nAIC\n2288.1\n2279.3\n\n\nBIC\n2757.2\n2770.8\n\n\nLog.Lik.\n−1060.025\n−1051.647\n\n\nF\n2.851\n2.923\n\n\nRMSE\n0.41\n0.41\n\n\n\n\n\n\n\nIf we are using the same coef_map for multiple tables, we don’t want to copy-paste it from table to table. We can instead assign the list to an object in our environment. That way, we will only have to correct it twice if for instance we discover a typo:\n\ngrewal_coef_map &lt;- list(\"nonvio2\" = \"Nonviolent\", \n                             \"nonvio2:senoff\" = \"Nonviolent × Senior officer\",\n                            \"prime_frat\" =  \"Prime-Fraternization\", \n                             \"prime_frat:conscript\" = \"Prime-Fraternization × Conscript\", \n                             \"prime_future\" = \"Prime-Civilian Control\", \n                              \"prime_future:soldier\" = \"Prime-Civilian Control × Soldier\", \n                             \"prime_RU\" = \"Prime-Russian Support\", \n                                \"prime_RU:soldier\" = \"Prime-Russian Support × Soldier\", \n                             \"prime_UN\" = \"Prime-United Nations\", \n                             \"oppose92coup2\" = \"Oppose 1992 coup\",\n                               \"oppose92coup2:young\" = \"Oppose 1992 coup × Born after 1995\",\n                             \"active\" = \"Active-Duty\", \n                             \"conscript\" =  \"Conscript\", \n                             \"soldier\" = \"Soldier\",\n                             \"junoff\" = \"Junior officer\", \n                              \"senoff\" = \"Senior officer\", \n                             \"branch2Land\" = \"Army/Gendarmerie\",\n                             \"train_west\" = \"Trained in the West\", \n                             \"train_russia\" = \"Trained in Russia\", \n                             \"train_china\" = \"Trained in China\", \n                             \"civwar\" = \"Fought in the 1990s\", \n                             \"islamist\" = \"Islamist\", \n                             \"supp_sharia\" = \"Support Sharia\", \n                             \"econ1\" = \"Economy good\", \n                             \"corr1\" = \"Corruption high\", \n                             \"dem\" = \"Support democracy\", \n                             \"supp_opp_parties\" = \"Support opposition parties\", \n                             \"continue\" = \"Want Hirak to continue\", \n                             \"protested\" = \"Protested\", \n                             \"young\" = \"Born after 1995\", \n                             \"sex\" = \"Female\", \n                             \"edu\" = \"Education\", \n                            \"urban_area\" = \"Urban area\", \n                            \"employed\" = \"Employed\",\n                            \"student\" = \"Student\", \n                            \"arabic\" = \"Arab\", \n                           \"preboutef\" =  \"Pre-Bouteflika Ouster\", \n                           \"post_exp\" = \"Post-experiment\", \n                           \"mon\" = \"Month\", \n                           \"(Intercept)\" = \"Constant\")\n\nmodelsummary(models = list(grewal_model_1, grewal_model_2 ), \n             coef_map = grewal_coef_map)\n\n\n\n\n\n (1)\n  (2)\n\n\n\n\nNonviolent\n0.098\n0.087\n\n\n\n(0.028)\n(0.028)\n\n\nNonviolent × Senior officer\n\n0.272\n\n\n\n\n(0.146)\n\n\nPrime-Fraternization\n−0.023\n−0.075\n\n\n\n(0.035)\n(0.040)\n\n\nPrime-Fraternization × Conscript\n\n0.168\n\n\n\n\n(0.057)\n\n\nPrime-Civilian Control\n−0.073\n−0.109\n\n\n\n(0.036)\n(0.040)\n\n\nPrime-Civilian Control × Soldier\n\n0.118\n\n\n\n\n(0.058)\n\n\nPrime-Russian Support\n−0.066\n−0.066\n\n\n\n(0.037)\n(0.037)\n\n\nPrime-United Nations\n−0.017\n−0.018\n\n\n\n(0.035)\n(0.035)\n\n\nOppose 1992 coup\n0.042\n0.044\n\n\n\n(0.021)\n(0.022)\n\n\nOppose 1992 coup × Born after 1995\n\n−0.037\n\n\n\n\n(0.063)\n\n\nActive-Duty\n−0.003\n0.000\n\n\n\n(0.024)\n(0.024)\n\n\nConscript\n−0.016\n−0.042\n\n\n\n(0.023)\n(0.025)\n\n\nSoldier\n0.013\n−0.006\n\n\n\n(0.024)\n(0.026)\n\n\nJunior officer\n0.033\n0.030\n\n\n\n(0.030)\n(0.030)\n\n\nSenior officer\n0.110\n−0.117\n\n\n\n(0.061)\n(0.137)\n\n\nArmy/Gendarmerie\n0.019\n0.018\n\n\n\n(0.028)\n(0.027)\n\n\nTrained in the West\n−0.096\n−0.093\n\n\n\n(0.058)\n(0.058)\n\n\nTrained in Russia\n−0.019\n−0.024\n\n\n\n(0.034)\n(0.034)\n\n\nTrained in China\n0.035\n0.044\n\n\n\n(0.105)\n(0.105)\n\n\nFought in the 1990s\n0.014\n0.011\n\n\n\n(0.026)\n(0.026)\n\n\nIslamist\n0.019\n0.018\n\n\n\n(0.030)\n(0.030)\n\n\nSupport Sharia\n0.158\n0.162\n\n\n\n(0.036)\n(0.036)\n\n\nEconomy good\n−0.041\n−0.042\n\n\n\n(0.048)\n(0.048)\n\n\nCorruption high\n0.002\n0.013\n\n\n\n(0.048)\n(0.048)\n\n\nSupport democracy\n0.127\n0.129\n\n\n\n(0.036)\n(0.036)\n\n\nSupport opposition parties\n−0.055\n−0.054\n\n\n\n(0.034)\n(0.034)\n\n\nWant Hirak to continue\n−0.033\n−0.037\n\n\n\n(0.022)\n(0.022)\n\n\nProtested\n0.090\n0.091\n\n\n\n(0.022)\n(0.022)\n\n\nBorn after 1995\n0.133\n0.145\n\n\n\n(0.042)\n(0.049)\n\n\nFemale\n0.020\n0.020\n\n\n\n(0.054)\n(0.054)\n\n\nEducation\n−0.014\n−0.013\n\n\n\n(0.012)\n(0.012)\n\n\nUrban area\n0.006\n0.009\n\n\n\n(0.022)\n(0.021)\n\n\nEmployed\n0.061\n0.063\n\n\n\n(0.024)\n(0.024)\n\n\nStudent\n−0.017\n−0.010\n\n\n\n(0.045)\n(0.045)\n\n\nArab\n0.094\n0.093\n\n\n\n(0.029)\n(0.029)\n\n\nPre-Bouteflika Ouster\n−0.038\n−0.039\n\n\n\n(0.050)\n(0.049)\n\n\nPost-experiment\n−0.011\n−0.017\n\n\n\n(0.053)\n(0.053)\n\n\nMonth\n−0.017\n−0.017\n\n\n\n(0.007)\n(0.007)\n\n\nConstant\n0.340\n0.340\n\n\n\n(0.116)\n(0.116)\n\n\nNum.Obs.\n1969\n1969\n\n\nR2\n0.110\n0.118\n\n\nR2 Adj.\n0.072\n0.078\n\n\nAIC\n2288.1\n2279.3\n\n\nBIC\n2757.2\n2770.8\n\n\nLog.Lik.\n−1060.025\n−1051.647\n\n\nF\n2.851\n2.923\n\n\nRMSE\n0.41\n0.41"
  },
  {
    "objectID": "Quarto_documents/producing_tables.html#changing-the-shape-of-the-table",
    "href": "Quarto_documents/producing_tables.html#changing-the-shape-of-the-table",
    "title": "Producing Tables from R",
    "section": "Changing the shape of the table",
    "text": "Changing the shape of the table\nThird, this table is very long (many coefficients) but not too wide (only two models). We may save some space in our document by changing the layout so that the standard errors are printed to the right of each coefficient rather than just below it. Indeed, this is how this table is formatted in the published article.\nWe can use the shape argument to change the shape of the table. shape takes a formula, with what should be in different rows before the ~ and what should be in different columns after the ~. We can thus get a more compact table like this:\n\nmodelsummary(models = list(grewal_model_1, grewal_model_2 ), \n             coef_map = grewal_coef_map, \n             shape = term ~ model + statistic)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\nEst.\nS.E.\nEst.\nS.E.\n\n\n\n\nNonviolent\n0.098\n0.028\n0.087\n0.028\n\n\nNonviolent × Senior officer\n\n\n0.272\n0.146\n\n\nPrime-Fraternization\n−0.023\n0.035\n−0.075\n0.040\n\n\nPrime-Fraternization × Conscript\n\n\n0.168\n0.057\n\n\nPrime-Civilian Control\n−0.073\n0.036\n−0.109\n0.040\n\n\nPrime-Civilian Control × Soldier\n\n\n0.118\n0.058\n\n\nPrime-Russian Support\n−0.066\n0.037\n−0.066\n0.037\n\n\nPrime-United Nations\n−0.017\n0.035\n−0.018\n0.035\n\n\nOppose 1992 coup\n0.042\n0.021\n0.044\n0.022\n\n\nOppose 1992 coup × Born after 1995\n\n\n−0.037\n0.063\n\n\nActive-Duty\n−0.003\n0.024\n0.000\n0.024\n\n\nConscript\n−0.016\n0.023\n−0.042\n0.025\n\n\nSoldier\n0.013\n0.024\n−0.006\n0.026\n\n\nJunior officer\n0.033\n0.030\n0.030\n0.030\n\n\nSenior officer\n0.110\n0.061\n−0.117\n0.137\n\n\nArmy/Gendarmerie\n0.019\n0.028\n0.018\n0.027\n\n\nTrained in the West\n−0.096\n0.058\n−0.093\n0.058\n\n\nTrained in Russia\n−0.019\n0.034\n−0.024\n0.034\n\n\nTrained in China\n0.035\n0.105\n0.044\n0.105\n\n\nFought in the 1990s\n0.014\n0.026\n0.011\n0.026\n\n\nIslamist\n0.019\n0.030\n0.018\n0.030\n\n\nSupport Sharia\n0.158\n0.036\n0.162\n0.036\n\n\nEconomy good\n−0.041\n0.048\n−0.042\n0.048\n\n\nCorruption high\n0.002\n0.048\n0.013\n0.048\n\n\nSupport democracy\n0.127\n0.036\n0.129\n0.036\n\n\nSupport opposition parties\n−0.055\n0.034\n−0.054\n0.034\n\n\nWant Hirak to continue\n−0.033\n0.022\n−0.037\n0.022\n\n\nProtested\n0.090\n0.022\n0.091\n0.022\n\n\nBorn after 1995\n0.133\n0.042\n0.145\n0.049\n\n\nFemale\n0.020\n0.054\n0.020\n0.054\n\n\nEducation\n−0.014\n0.012\n−0.013\n0.012\n\n\nUrban area\n0.006\n0.022\n0.009\n0.021\n\n\nEmployed\n0.061\n0.024\n0.063\n0.024\n\n\nStudent\n−0.017\n0.045\n−0.010\n0.045\n\n\nArab\n0.094\n0.029\n0.093\n0.029\n\n\nPre-Bouteflika Ouster\n−0.038\n0.050\n−0.039\n0.049\n\n\nPost-experiment\n−0.011\n0.053\n−0.017\n0.053\n\n\nMonth\n−0.017\n0.007\n−0.017\n0.007\n\n\nConstant\n0.340\n0.116\n0.340\n0.116"
  },
  {
    "objectID": "Quarto_documents/producing_tables.html#rerarranging-renaming-selecting-and-omitting-goodness-of-fit-statistics-and-other-model-information",
    "href": "Quarto_documents/producing_tables.html#rerarranging-renaming-selecting-and-omitting-goodness-of-fit-statistics-and-other-model-information",
    "title": "Producing Tables from R",
    "section": "Rerarranging, renaming, selecting, and omitting goodness-of-fit statistics and other model information",
    "text": "Rerarranging, renaming, selecting, and omitting goodness-of-fit statistics and other model information\nFourth, by default several different goodness-of-fit statistics and other model information is included. We may want to change their names, what information and the order of these different rows.\nIf so, we can use gof_map which works almost the same way as coef_map except that it deals with the model information rather than with the coefficients and that the supplied argument should be a data.frame rather than named list. The data.frame should have three columns:\n\nOne column named “raw” includes the names of the statistics you want in the format used by modelsummary. You type modelsummary::gof_map in your console, the default gof_map will be printed out and you can find out what the different “raw” statistics names are.\nOne column called “clean” where you write the names of the statistics as you want them printed in the table\nOne column called “fmt” where you tell R how many digits you want for each of the statistics:\n\nAs before, we will assign our preferences to an object in our environment so that it is easy to reuse it later:\n\ngrewal_gof_map &lt;- data.frame(raw = c(\"nobs\", \"r.squared\", \"adj.r.squared\"),\n                             clean  = c(\"No. of. obs\", \"R2\", \"Adj. R2\"), \n                             fmt = c(0,3,3))\n  \n\nmodelsummary(models = list(grewal_model_1, grewal_model_2 ), \n             coef_map = grewal_coef_map, \n             shape = term ~ model + statistic, \n             gof_map = grewal_gof_map)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\nEst.\nS.E.\nEst.\nS.E.\n\n\n\n\nNonviolent\n0.098\n0.028\n0.087\n0.028\n\n\nNonviolent × Senior officer\n\n\n0.272\n0.146\n\n\nPrime-Fraternization\n−0.023\n0.035\n−0.075\n0.040\n\n\nPrime-Fraternization × Conscript\n\n\n0.168\n0.057\n\n\nPrime-Civilian Control\n−0.073\n0.036\n−0.109\n0.040\n\n\nPrime-Civilian Control × Soldier\n\n\n0.118\n0.058\n\n\nPrime-Russian Support\n−0.066\n0.037\n−0.066\n0.037\n\n\nPrime-United Nations\n−0.017\n0.035\n−0.018\n0.035\n\n\nOppose 1992 coup\n0.042\n0.021\n0.044\n0.022\n\n\nOppose 1992 coup × Born after 1995\n\n\n−0.037\n0.063\n\n\nActive-Duty\n−0.003\n0.024\n0.000\n0.024\n\n\nConscript\n−0.016\n0.023\n−0.042\n0.025\n\n\nSoldier\n0.013\n0.024\n−0.006\n0.026\n\n\nJunior officer\n0.033\n0.030\n0.030\n0.030\n\n\nSenior officer\n0.110\n0.061\n−0.117\n0.137\n\n\nArmy/Gendarmerie\n0.019\n0.028\n0.018\n0.027\n\n\nTrained in the West\n−0.096\n0.058\n−0.093\n0.058\n\n\nTrained in Russia\n−0.019\n0.034\n−0.024\n0.034\n\n\nTrained in China\n0.035\n0.105\n0.044\n0.105\n\n\nFought in the 1990s\n0.014\n0.026\n0.011\n0.026\n\n\nIslamist\n0.019\n0.030\n0.018\n0.030\n\n\nSupport Sharia\n0.158\n0.036\n0.162\n0.036\n\n\nEconomy good\n−0.041\n0.048\n−0.042\n0.048\n\n\nCorruption high\n0.002\n0.048\n0.013\n0.048\n\n\nSupport democracy\n0.127\n0.036\n0.129\n0.036\n\n\nSupport opposition parties\n−0.055\n0.034\n−0.054\n0.034\n\n\nWant Hirak to continue\n−0.033\n0.022\n−0.037\n0.022\n\n\nProtested\n0.090\n0.022\n0.091\n0.022\n\n\nBorn after 1995\n0.133\n0.042\n0.145\n0.049\n\n\nFemale\n0.020\n0.054\n0.020\n0.054\n\n\nEducation\n−0.014\n0.012\n−0.013\n0.012\n\n\nUrban area\n0.006\n0.022\n0.009\n0.021\n\n\nEmployed\n0.061\n0.024\n0.063\n0.024\n\n\nStudent\n−0.017\n0.045\n−0.010\n0.045\n\n\nArab\n0.094\n0.029\n0.093\n0.029\n\n\nPre-Bouteflika Ouster\n−0.038\n0.050\n−0.039\n0.049\n\n\nPost-experiment\n−0.011\n0.053\n−0.017\n0.053\n\n\nMonth\n−0.017\n0.007\n−0.017\n0.007\n\n\nConstant\n0.340\n0.116\n0.340\n0.116"
  },
  {
    "objectID": "Quarto_documents/producing_tables.html#adding-significance-stars",
    "href": "Quarto_documents/producing_tables.html#adding-significance-stars",
    "title": "Producing Tables from R",
    "section": "Adding significance stars",
    "text": "Adding significance stars\nMany scholars like to add stars to their coefficients indicating that the coefficient has reached a particular level of statistical significance (for instance .05 or 0.01). Not everyone agrees that this practice is beneficial (e.g. Gill 2018), but if you do want to include stars in your regression tables, modelsummary() got you covered. You simply can set the argument stars to TRUE, like we do here to get a + sign added to coefficients significant at the .1 level, a star added to coefficients significant at the .05-level, two stars added to coefficients significant at the .01 level and three stars added to coefficients significant at the .001-level:\n\nmodelsummary(models = list(grewal_model_1, grewal_model_2 ), \n             coef_map = grewal_coef_map, \n             shape = term ~ model + statistic, \n             gof_map = grewal_gof_map, \n             stars = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\nEst.\nS.E.\nEst.\nS.E.\n\n\n\n\nNonviolent\n0.098***\n0.028\n0.087**\n0.028\n\n\nNonviolent × Senior officer\n\n\n0.272+\n0.146\n\n\nPrime-Fraternization\n−0.023\n0.035\n−0.075+\n0.040\n\n\nPrime-Fraternization × Conscript\n\n\n0.168**\n0.057\n\n\nPrime-Civilian Control\n−0.073*\n0.036\n−0.109**\n0.040\n\n\nPrime-Civilian Control × Soldier\n\n\n0.118*\n0.058\n\n\nPrime-Russian Support\n−0.066+\n0.037\n−0.066+\n0.037\n\n\nPrime-United Nations\n−0.017\n0.035\n−0.018\n0.035\n\n\nOppose 1992 coup\n0.042*\n0.021\n0.044*\n0.022\n\n\nOppose 1992 coup × Born after 1995\n\n\n−0.037\n0.063\n\n\nActive-Duty\n−0.003\n0.024\n0.000\n0.024\n\n\nConscript\n−0.016\n0.023\n−0.042+\n0.025\n\n\nSoldier\n0.013\n0.024\n−0.006\n0.026\n\n\nJunior officer\n0.033\n0.030\n0.030\n0.030\n\n\nSenior officer\n0.110+\n0.061\n−0.117\n0.137\n\n\nArmy/Gendarmerie\n0.019\n0.028\n0.018\n0.027\n\n\nTrained in the West\n−0.096+\n0.058\n−0.093\n0.058\n\n\nTrained in Russia\n−0.019\n0.034\n−0.024\n0.034\n\n\nTrained in China\n0.035\n0.105\n0.044\n0.105\n\n\nFought in the 1990s\n0.014\n0.026\n0.011\n0.026\n\n\nIslamist\n0.019\n0.030\n0.018\n0.030\n\n\nSupport Sharia\n0.158***\n0.036\n0.162***\n0.036\n\n\nEconomy good\n−0.041\n0.048\n−0.042\n0.048\n\n\nCorruption high\n0.002\n0.048\n0.013\n0.048\n\n\nSupport democracy\n0.127***\n0.036\n0.129***\n0.036\n\n\nSupport opposition parties\n−0.055\n0.034\n−0.054\n0.034\n\n\nWant Hirak to continue\n−0.033\n0.022\n−0.037+\n0.022\n\n\nProtested\n0.090***\n0.022\n0.091***\n0.022\n\n\nBorn after 1995\n0.133**\n0.042\n0.145**\n0.049\n\n\nFemale\n0.020\n0.054\n0.020\n0.054\n\n\nEducation\n−0.014\n0.012\n−0.013\n0.012\n\n\nUrban area\n0.006\n0.022\n0.009\n0.021\n\n\nEmployed\n0.061*\n0.024\n0.063*\n0.024\n\n\nStudent\n−0.017\n0.045\n−0.010\n0.045\n\n\nArab\n0.094**\n0.029\n0.093**\n0.029\n\n\nPre-Bouteflika Ouster\n−0.038\n0.050\n−0.039\n0.049\n\n\nPost-experiment\n−0.011\n0.053\n−0.017\n0.053\n\n\nMonth\n−0.017*\n0.007\n−0.017*\n0.007\n\n\nConstant\n0.340**\n0.116\n0.340**\n0.116\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\nYou can also specify the number of stars and the corresponding significance levels, by supplying a named numeric vector:\n\nmodelsummary(models = list(grewal_model_1, grewal_model_2 ), \n             coef_map = grewal_coef_map, \n             shape = term ~ model + statistic, \n             gof_map = grewal_gof_map, \n             stars = c(\"*\" = .05, \"**\" = .01,\"***\" = .01 ))\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\nEst.\nS.E.\nEst.\nS.E.\n\n\n\n\nNonviolent\n0.098***\n0.028\n0.087***\n0.028\n\n\nNonviolent × Senior officer\n\n\n0.272\n0.146\n\n\nPrime-Fraternization\n−0.023\n0.035\n−0.075\n0.040\n\n\nPrime-Fraternization × Conscript\n\n\n0.168***\n0.057\n\n\nPrime-Civilian Control\n−0.073*\n0.036\n−0.109***\n0.040\n\n\nPrime-Civilian Control × Soldier\n\n\n0.118*\n0.058\n\n\nPrime-Russian Support\n−0.066\n0.037\n−0.066\n0.037\n\n\nPrime-United Nations\n−0.017\n0.035\n−0.018\n0.035\n\n\nOppose 1992 coup\n0.042*\n0.021\n0.044*\n0.022\n\n\nOppose 1992 coup × Born after 1995\n\n\n−0.037\n0.063\n\n\nActive-Duty\n−0.003\n0.024\n0.000\n0.024\n\n\nConscript\n−0.016\n0.023\n−0.042\n0.025\n\n\nSoldier\n0.013\n0.024\n−0.006\n0.026\n\n\nJunior officer\n0.033\n0.030\n0.030\n0.030\n\n\nSenior officer\n0.110\n0.061\n−0.117\n0.137\n\n\nArmy/Gendarmerie\n0.019\n0.028\n0.018\n0.027\n\n\nTrained in the West\n−0.096\n0.058\n−0.093\n0.058\n\n\nTrained in Russia\n−0.019\n0.034\n−0.024\n0.034\n\n\nTrained in China\n0.035\n0.105\n0.044\n0.105\n\n\nFought in the 1990s\n0.014\n0.026\n0.011\n0.026\n\n\nIslamist\n0.019\n0.030\n0.018\n0.030\n\n\nSupport Sharia\n0.158***\n0.036\n0.162***\n0.036\n\n\nEconomy good\n−0.041\n0.048\n−0.042\n0.048\n\n\nCorruption high\n0.002\n0.048\n0.013\n0.048\n\n\nSupport democracy\n0.127***\n0.036\n0.129***\n0.036\n\n\nSupport opposition parties\n−0.055\n0.034\n−0.054\n0.034\n\n\nWant Hirak to continue\n−0.033\n0.022\n−0.037\n0.022\n\n\nProtested\n0.090***\n0.022\n0.091***\n0.022\n\n\nBorn after 1995\n0.133***\n0.042\n0.145***\n0.049\n\n\nFemale\n0.020\n0.054\n0.020\n0.054\n\n\nEducation\n−0.014\n0.012\n−0.013\n0.012\n\n\nUrban area\n0.006\n0.022\n0.009\n0.021\n\n\nEmployed\n0.061*\n0.024\n0.063*\n0.024\n\n\nStudent\n−0.017\n0.045\n−0.010\n0.045\n\n\nArab\n0.094***\n0.029\n0.093***\n0.029\n\n\nPre-Bouteflika Ouster\n−0.038\n0.050\n−0.039\n0.049\n\n\nPost-experiment\n−0.011\n0.053\n−0.017\n0.053\n\n\nMonth\n−0.017*\n0.007\n−0.017*\n0.007\n\n\nConstant\n0.340***\n0.116\n0.340***\n0.116\n\n\n\n * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.01"
  },
  {
    "objectID": "Quarto_documents/producing_tables.html#adding-rows-with-additional-information",
    "href": "Quarto_documents/producing_tables.html#adding-rows-with-additional-information",
    "title": "Producing Tables from R",
    "section": "Adding rows with additional information",
    "text": "Adding rows with additional information\nWhen removing the fixed effects coefficients, Grewal (2023) added a row indicating that the models do include governorate fixed effects. We can do the same using the the add_rows argument.\nThe add_rows argument needs a data.frame with the same number of columns as the table. In our case this means five columns since we have one column for the variable names, two columns with coefficients and two columns with standard errors. We add information the information we want to the rows:\n\nfixed_effects_info &lt;- data.frame(\"Governorate fixed effects\", \"Yes\",\"\", \"Yes\", \"\")\n\nNext, we need to to tell the position of that the additional row should get in our table. We will declare the position by assigning an attribute called “position” to our data.frame as illustrated below. We want it be included just below all the included coefficients. We are not in the business of manually counting things in this course, so we use length() to figure out how many coefficients we have in our coef_map and use this number as our position:\n\nattr(fixed_effects_info, \"position\") &lt;- length(grewal_coef_map)\n\nAnd now we just need to set the argument in the modelsummary() call:\n\nmodelsummary(models = list(grewal_model_1, grewal_model_2 ), \n             coef_map = grewal_coef_map, \n             shape = term ~ model + statistic, \n             gof_map = grewal_gof_map, \n             stars = c(\"*\" = .05, \"**\" = .01,\"***\" = .01 ), \n             add_rows = fixed_effects_info)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\nEst.\nS.E.\nEst.\nS.E.\n\n\n\n\nNonviolent\n0.098***\n0.028\n0.087***\n0.028\n\n\nNonviolent × Senior officer\n\n\n0.272\n0.146\n\n\nPrime-Fraternization\n−0.023\n0.035\n−0.075\n0.040\n\n\nPrime-Fraternization × Conscript\n\n\n0.168***\n0.057\n\n\nPrime-Civilian Control\n−0.073*\n0.036\n−0.109***\n0.040\n\n\nPrime-Civilian Control × Soldier\n\n\n0.118*\n0.058\n\n\nPrime-Russian Support\n−0.066\n0.037\n−0.066\n0.037\n\n\nPrime-United Nations\n−0.017\n0.035\n−0.018\n0.035\n\n\nOppose 1992 coup\n0.042*\n0.021\n0.044*\n0.022\n\n\nOppose 1992 coup × Born after 1995\n\n\n−0.037\n0.063\n\n\nActive-Duty\n−0.003\n0.024\n0.000\n0.024\n\n\nConscript\n−0.016\n0.023\n−0.042\n0.025\n\n\nSoldier\n0.013\n0.024\n−0.006\n0.026\n\n\nJunior officer\n0.033\n0.030\n0.030\n0.030\n\n\nSenior officer\n0.110\n0.061\n−0.117\n0.137\n\n\nArmy/Gendarmerie\n0.019\n0.028\n0.018\n0.027\n\n\nTrained in the West\n−0.096\n0.058\n−0.093\n0.058\n\n\nTrained in Russia\n−0.019\n0.034\n−0.024\n0.034\n\n\nTrained in China\n0.035\n0.105\n0.044\n0.105\n\n\nFought in the 1990s\n0.014\n0.026\n0.011\n0.026\n\n\nIslamist\n0.019\n0.030\n0.018\n0.030\n\n\nSupport Sharia\n0.158***\n0.036\n0.162***\n0.036\n\n\nEconomy good\n−0.041\n0.048\n−0.042\n0.048\n\n\nCorruption high\n0.002\n0.048\n0.013\n0.048\n\n\nSupport democracy\n0.127***\n0.036\n0.129***\n0.036\n\n\nSupport opposition parties\n−0.055\n0.034\n−0.054\n0.034\n\n\nWant Hirak to continue\n−0.033\n0.022\n−0.037\n0.022\n\n\nProtested\n0.090***\n0.022\n0.091***\n0.022\n\n\nBorn after 1995\n0.133***\n0.042\n0.145***\n0.049\n\n\nFemale\n0.020\n0.054\n0.020\n0.054\n\n\nEducation\n−0.014\n0.012\n−0.013\n0.012\n\n\nUrban area\n0.006\n0.022\n0.009\n0.021\n\n\nEmployed\n0.061*\n0.024\n0.063*\n0.024\n\n\nStudent\n−0.017\n0.045\n−0.010\n0.045\n\n\nArab\n0.094***\n0.029\n0.093***\n0.029\n\n\nPre-Bouteflika Ouster\n−0.038\n0.050\n−0.039\n0.049\n\n\nPost-experiment\n−0.011\n0.053\n−0.017\n0.053\n\n\nMonth\n−0.017*\n0.007\n−0.017*\n0.007\n\n\nGovernorate fixed effects\nYes\n\nYes\n\n\n\nNA\nNA\nNA\nNA\nNA\n\n\nConstant\n0.340***\n0.116\n0.340***\n0.116\n\n\n\n * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.01"
  },
  {
    "objectID": "Quarto_documents/producing_tables.html#changing-the-standard-errors",
    "href": "Quarto_documents/producing_tables.html#changing-the-standard-errors",
    "title": "Producing Tables from R",
    "section": "Changing the standard errors",
    "text": "Changing the standard errors\nWe often make various corrections to our standard errors (we use various robust, heteroskedasticity-consistent, clustered, bootstrapped, etc standard errors because we have reasons to believe that the classical standard errors are off in some way). If so, we can use the vcov argument to change the variance-covariance matrix from which the standard errors are calculated.\nThere are different ways of specifying this argument (so read the documentation), but the most robust way is arguably to supply a function telling R how to make the variance-covariance matrix.\nFor instance, although Grewal (2023) didn’t adjust his standard errors, it would arguably have made sense to use heteroskedasticity-consistent standard errors. We can get those using the vcovHC()function from sandwich package:\n\nlibrary(sandwich)\n\nmodelsummary(models = list(grewal_model_1, grewal_model_2 ), \n             coef_map = grewal_coef_map, \n             shape = term ~ model + statistic, \n             gof_map = grewal_gof_map, \n             stars = c(\"*\" = .05, \"**\" = .01,\"***\" = .01 ), \n             vcov = function(x){ vcovHC(x)})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\nEst.\nS.E.\nEst.\nS.E.\n\n\n\n\nNonviolent\n0.098***\n0.031\n0.087***\n0.032\n\n\nNonviolent × Senior officer\n\n\n0.272\n0.162\n\n\nPrime-Fraternization\n−0.023\n0.034\n−0.075\n0.038\n\n\nPrime-Fraternization × Conscript\n\n\n0.168***\n0.057\n\n\nPrime-Civilian Control\n−0.073*\n0.035\n−0.109***\n0.041\n\n\nPrime-Civilian Control × Soldier\n\n\n0.118*\n0.060\n\n\nPrime-Russian Support\n−0.066\n0.036\n−0.066\n0.036\n\n\nPrime-United Nations\n−0.017\n0.034\n−0.018\n0.034\n\n\nOppose 1992 coup\n0.042*\n0.020\n0.044*\n0.022\n\n\nOppose 1992 coup × Born after 1995\n\n\n−0.037\n0.058\n\n\nActive-Duty\n−0.003\n0.024\n0.000\n0.024\n\n\nConscript\n−0.016\n0.024\n−0.042\n0.025\n\n\nSoldier\n0.013\n0.025\n−0.006\n0.026\n\n\nJunior officer\n0.033\n0.031\n0.030\n0.031\n\n\nSenior officer\n0.110\n0.060\n−0.117\n0.158\n\n\nArmy/Gendarmerie\n0.019\n0.029\n0.018\n0.029\n\n\nTrained in the West\n−0.096\n0.065\n−0.093\n0.065\n\n\nTrained in Russia\n−0.019\n0.035\n−0.024\n0.035\n\n\nTrained in China\n0.035\n0.109\n0.044\n0.106\n\n\nFought in the 1990s\n0.014\n0.027\n0.011\n0.027\n\n\nIslamist\n0.019\n0.029\n0.018\n0.029\n\n\nSupport Sharia\n0.158***\n0.037\n0.162***\n0.037\n\n\nEconomy good\n−0.041\n0.050\n−0.042\n0.049\n\n\nCorruption high\n0.002\n0.050\n0.013\n0.050\n\n\nSupport democracy\n0.127***\n0.037\n0.129***\n0.037\n\n\nSupport opposition parties\n−0.055\n0.034\n−0.054\n0.034\n\n\nWant Hirak to continue\n−0.033\n0.022\n−0.037\n0.022\n\n\nProtested\n0.090***\n0.023\n0.091***\n0.023\n\n\nBorn after 1995\n0.133***\n0.040\n0.145***\n0.049\n\n\nFemale\n0.020\n0.051\n0.020\n0.051\n\n\nEducation\n−0.014\n0.012\n−0.013\n0.012\n\n\nUrban area\n0.006\n0.022\n0.009\n0.022\n\n\nEmployed\n0.061*\n0.025\n0.063*\n0.025\n\n\nStudent\n−0.017\n0.046\n−0.010\n0.046\n\n\nArab\n0.094***\n0.031\n0.093***\n0.030\n\n\nPre-Bouteflika Ouster\n−0.038\n0.054\n−0.039\n0.054\n\n\nPost-experiment\n−0.011\n0.054\n−0.017\n0.054\n\n\nMonth\n−0.017*\n0.007\n−0.017*\n0.007\n\n\nConstant\n0.340***\n0.126\n0.340***\n0.127\n\n\n\n * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.01"
  },
  {
    "objectID": "Quarto_documents/producing_tables.html#including-the-table-in-our-quarto-document",
    "href": "Quarto_documents/producing_tables.html#including-the-table-in-our-quarto-document",
    "title": "Producing Tables from R",
    "section": "Including the table in our Quarto document",
    "text": "Including the table in our Quarto document\nmodelsummary() also has arguments for tables and labels, etc. but it is better to declare this information in the execution options for the chunk that produces the table in our Quarto document. So for instance, we can add a table and label that we can use to cross-reference this table like this:\n\n```{r}\n#| tbl-cap: Our attempt at reproducing Grewal's table 2\n#| label: tbl-grewal_table\n\nmodelsummary(models = list(grewal_model_1, grewal_model_2 ), \n             coef_map = grewal_coef_map, \n             shape = term ~ model + statistic, \n             gof_map = grewal_gof_map, \n             stars = c(\"*\" = .05, \"**\" = .01,\"***\" = .01 ), \n             add_rows = fixed_effects_info)\n```\n\n\n\nTable 1: Our attempt at reproducing Grewal’s table 2\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\nEst.\nS.E.\nEst.\nS.E.\n\n\n\n\nNonviolent\n0.098***\n0.028\n0.087***\n0.028\n\n\nNonviolent × Senior officer\n\n\n0.272\n0.146\n\n\nPrime-Fraternization\n−0.023\n0.035\n−0.075\n0.040\n\n\nPrime-Fraternization × Conscript\n\n\n0.168***\n0.057\n\n\nPrime-Civilian Control\n−0.073*\n0.036\n−0.109***\n0.040\n\n\nPrime-Civilian Control × Soldier\n\n\n0.118*\n0.058\n\n\nPrime-Russian Support\n−0.066\n0.037\n−0.066\n0.037\n\n\nPrime-United Nations\n−0.017\n0.035\n−0.018\n0.035\n\n\nOppose 1992 coup\n0.042*\n0.021\n0.044*\n0.022\n\n\nOppose 1992 coup × Born after 1995\n\n\n−0.037\n0.063\n\n\nActive-Duty\n−0.003\n0.024\n0.000\n0.024\n\n\nConscript\n−0.016\n0.023\n−0.042\n0.025\n\n\nSoldier\n0.013\n0.024\n−0.006\n0.026\n\n\nJunior officer\n0.033\n0.030\n0.030\n0.030\n\n\nSenior officer\n0.110\n0.061\n−0.117\n0.137\n\n\nArmy/Gendarmerie\n0.019\n0.028\n0.018\n0.027\n\n\nTrained in the West\n−0.096\n0.058\n−0.093\n0.058\n\n\nTrained in Russia\n−0.019\n0.034\n−0.024\n0.034\n\n\nTrained in China\n0.035\n0.105\n0.044\n0.105\n\n\nFought in the 1990s\n0.014\n0.026\n0.011\n0.026\n\n\nIslamist\n0.019\n0.030\n0.018\n0.030\n\n\nSupport Sharia\n0.158***\n0.036\n0.162***\n0.036\n\n\nEconomy good\n−0.041\n0.048\n−0.042\n0.048\n\n\nCorruption high\n0.002\n0.048\n0.013\n0.048\n\n\nSupport democracy\n0.127***\n0.036\n0.129***\n0.036\n\n\nSupport opposition parties\n−0.055\n0.034\n−0.054\n0.034\n\n\nWant Hirak to continue\n−0.033\n0.022\n−0.037\n0.022\n\n\nProtested\n0.090***\n0.022\n0.091***\n0.022\n\n\nBorn after 1995\n0.133***\n0.042\n0.145***\n0.049\n\n\nFemale\n0.020\n0.054\n0.020\n0.054\n\n\nEducation\n−0.014\n0.012\n−0.013\n0.012\n\n\nUrban area\n0.006\n0.022\n0.009\n0.021\n\n\nEmployed\n0.061*\n0.024\n0.063*\n0.024\n\n\nStudent\n−0.017\n0.045\n−0.010\n0.045\n\n\nArab\n0.094***\n0.029\n0.093***\n0.029\n\n\nPre-Bouteflika Ouster\n−0.038\n0.050\n−0.039\n0.049\n\n\nPost-experiment\n−0.011\n0.053\n−0.017\n0.053\n\n\nMonth\n−0.017*\n0.007\n−0.017*\n0.007\n\n\nGovernorate fixed effects\nYes\n\nYes\n\n\n\nNA\nNA\nNA\nNA\nNA\n\n\nConstant\n0.340***\n0.116\n0.340***\n0.116\n\n\n\n * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince we declared a label, we can use it to cross reference Table 1 in our document."
  },
  {
    "objectID": "Quarto_documents/producing_tables.html#changing-the-variable-labels",
    "href": "Quarto_documents/producing_tables.html#changing-the-variable-labels",
    "title": "Producing Tables from R",
    "section": "Changing the variable labels",
    "text": "Changing the variable labels\nBefore we include the table in our document, we would want to change the variable labels. Changing the variable labels is a bit more cumbersome in datasummary() than in modelsummary(), but you can do by wrapping each a term in parentheses and use the = sign to map new variable labels to the variables names that appear in the dataset:\n\ndatasummary((Restraint = restraint) + (Nonviolent = nonvio2) + (`Prime-Fraternization` = prime_frat) + (`Prime-Civilian Control`=prime_future) + (`Prime-Russian Support` = prime_RU) + \n              (`Prime-United Nations` = prime_UN) + (`Oppose 1992 coup` = oppose92coup2) +\n              (`Fought in the 1990s` = civwar) + (Islamist = islamist) +  \n              (`Support Sharia` = supp_sharia) + (`Active-Duty`= active) +\n              (Conscript = conscript)  + (Soldier = soldier) + (`Junior officer` = junoff) +\n              (`Senior officer`= senoff) +(`Trained in the West` = train_west)  + \n              (`Trained in Russia` = train_russia) + \n              (`Trained in China` = train_china)  + (`Born after 1995` = young) + \n              (Female = sex) + (Education = edu) + \n              (Urban = urban_area) + (Employed = employed) + (Student = student) +\n              (Arab = arabic) + (`Economy is good` = econ1) + (`Corruption is high` = corr1) +\n              (`Support democracy` = dem) + (`Support opposition partie` = supp_opp_parties) +\n              (`Want Hirak to continue`= continue) + (Protested = protested) + \n              (`Pre-Bouteflika Ouster` = preboutef) + (`Post-experiment` = post_exp) + (Month = mon) ~ \n              Mean + Median + SD + Min + Max, \n            data = grewal_data)\n\n\n\n\n\nMean\nMedian\nSD\nMin\nMax\n\n\n\n\nRestraint\n0.73\n1.00\n0.44\n0.00\n1.00\n\n\nNonviolent\n0.85\n1.00\n0.36\n0.00\n1.00\n\n\nPrime-Fraternization\n0.16\n0.00\n0.37\n0.00\n1.00\n\n\nPrime-Civilian Control\n0.16\n0.00\n0.36\n0.00\n1.00\n\n\nPrime-Russian Support\n0.14\n0.00\n0.35\n0.00\n1.00\n\n\nPrime-United Nations\n0.16\n0.00\n0.37\n0.00\n1.00\n\n\nOppose 1992 coup\n0.45\n0.00\n0.50\n0.00\n1.00\n\n\nFought in the 1990s\n0.20\n0.00\n0.40\n0.00\n1.00\n\n\nIslamist\n0.14\n0.00\n0.35\n0.00\n1.00\n\n\nSupport Sharia\n0.71\n0.75\n0.29\n0.00\n1.00\n\n\nActive-Duty\n0.34\n0.00\n0.47\n0.00\n1.00\n\n\nConscript\n0.34\n0.00\n0.47\n0.00\n1.00\n\n\nSoldier\n0.30\n0.00\n0.46\n0.00\n1.00\n\n\nJunior officer\n0.18\n0.00\n0.39\n0.00\n1.00\n\n\nSenior officer\n0.03\n0.00\n0.17\n0.00\n1.00\n\n\nTrained in the West\n0.03\n0.00\n0.18\n0.00\n1.00\n\n\nTrained in Russia\n0.11\n0.00\n0.31\n0.00\n1.00\n\n\nTrained in China\n0.01\n0.00\n0.09\n0.00\n1.00\n\n\nBorn after 1995\n0.11\n0.00\n0.31\n0.00\n1.00\n\n\nFemale\n0.04\n0.00\n0.20\n0.00\n1.00\n\n\nEducation\n4.38\n4.00\n1.02\n1.00\n7.00\n\n\nUrban\n0.65\n1.00\n0.48\n0.00\n1.00\n\n\nEmployed\n0.55\n1.00\n0.50\n0.00\n1.00\n\n\nStudent\n0.11\n0.00\n0.31\n0.00\n1.00\n\n\nArab\n0.82\n1.00\n0.38\n0.00\n1.00\n\n\nEconomy is good\n0.15\n0.00\n0.24\n0.00\n1.00\n\n\nCorruption is high\n0.85\n1.00\n0.24\n0.00\n1.00\n\n\nSupport democracy\n0.63\n0.75\n0.28\n0.00\n1.00\n\n\nSupport opposition partie\n0.22\n0.00\n0.29\n0.00\n1.00\n\n\nWant Hirak to continue\n0.51\n1.00\n0.50\n0.00\n1.00\n\n\nProtested\n0.54\n1.00\n0.50\n0.00\n1.00\n\n\nPre-Bouteflika Ouster\n0.05\n0.00\n0.21\n0.00\n1.00\n\n\nPost-experiment\n0.24\n0.00\n0.43\n0.00\n1.00\n\n\nMonth\n3.88\n3.00\n2.96\n1.00\n11.00\n\n\n\n\n\n\n\nBecause this way of renaming the variable labels is somewhat convoluted, it might be better to instead subset the data.frame and rename the variables there before passing it to the datasummary() function. We will illustrate this approach when we create a correlation table using datasummary_correlation()."
  },
  {
    "objectID": "basics_refresh/encoding.html",
    "href": "basics_refresh/encoding.html",
    "title": "Locales and encoding",
    "section": "",
    "text": "Locale is a set of parameters that determine the linguistic conventions used for formatting and displaying information such as text. These conventions can vary based on geographical regions, languages, and cultural norms. Locales are essential for ensuring that software applications can adapt to the preferences and conventions of different users and regions.\nIn R, locales play a crucial role in working with many data types in R, such as text, dates, number formatting, and so on. The locale setting affects functions that handle text manipulation, collation, and formatting. It is particularly important when dealing with multilingual text analysis, as different languages have distinct rules for character sorting, case sensitivity, and formatting.\nWe can check your locale with the Sys.getlocale() and set the locale in R using the Sys.setlocale() function. For example, we can set the locale to “en_US.UTF-8” for American English conventions, “fr_FR.UTF-8” for French conventions, “nb_NO.UTF-8” for Norwegian conventions. Here is an example of how sorting is affected by locale:\n\nSys.getlocale(\"LC_COLLATE\")\n\n[1] \"en_US.UTF-8\"\n\nplaces_ex &lt;- c(\"Aarhus\", \"Ålgård\", \"Arendal\", \"Bergen\")\n\nsort(places_ex)\n\n[1] \"Aarhus\"  \"Ålgård\"  \"Arendal\" \"Bergen\" \n\nSys.setlocale(\"LC_COLLATE\", \"nb_NO.UTF-8\")\n\n[1] \"nb_NO.UTF-8\"\n\nsort(places_ex)\n\n[1] \"Arendal\" \"Bergen\"  \"Ålgård\"  \"Aarhus\" \n\n\n\n\n[1] \"en_US.UTF-8\"\n\n\nThere are several different types of locales we can change:\n\n\n\n\n\n\n\nLC Variant\nDescription\n\n\n\n\nLC_COLLATE\nControls string sorting and comparison.\n\n\nLC_CTYPE\nManages character encoding and case handling.\n\n\nLC_TIME\nDictates date and time formatting.\n\n\nLC_NUMERIC\nGoverns number formatting, including separators.\n\n\nLC_MONETARY\nHandles formatting of monetary values and currencies.\n\n\nLC_MESSAGES\nInfluences language and format of system messages and output.\n\n\nLC_ALL\nEverything.\n\n\n\nIf we want to set all locales to one type, then we can run Sys.setlocale(\"LC_ALL\", \"en_US.UTF-8\") (or the convention we want)."
  },
  {
    "objectID": "basics_refresh/encoding.html#r-scripts",
    "href": "basics_refresh/encoding.html#r-scripts",
    "title": "Locales and encoding",
    "section": "R-scripts",
    "text": "R-scripts\nThe problem most students will encounter when it comes to encoding is opening R-scripts with the wrong encoding. Typically, the Norwegian characters æ, ø, and å will often display as question marks, dots, or weird symbols in Rstudio if the script was made on one operating system and opened on a different one:\n\n\n\n\n\nUTF-8\n\n\n\n\n\n\nISO-8859-1\n\n\n\n\nThis can easily be fixed within RStudio by opening the “File”-menu and selecting “Reopen with encoding”, and then select the encoding you want to open the script with. For instance, a Windows user opening a R-script made in Linux will want to reopen the script in “UTF-8”."
  },
  {
    "objectID": "basics_refresh/encoding.html#text-strings",
    "href": "basics_refresh/encoding.html#text-strings",
    "title": "Locales and encoding",
    "section": "Text strings",
    "text": "Text strings\nWhen working with text in R, we often encounter strange behavior because of locale and encoding. It is important to be aware of these, and know how to fix potential issues.\nTake this example:\n\nexample &lt;- c(\"Norwegian\", \"Nørwegian\", \"النرويج\")\n\nEncoding(example)\n\n[1] \"unknown\" \"UTF-8\"   \"UTF-8\"  \n\nexample\n\n[1] \"Norwegian\" \"Nørwegian\" \"النرويج\"  \n\niconv(example, \"UTF-8\", \"latin1\")\n\n[1] \"Norwegian\" \"Nørwegian\" NA         \n\niconv(example, \"UTF-8\", \"ISO-8859-1\")\n\n[1] \"Norwegian\"    \"N\\xf8rwegian\" NA            \n\niconv(example, \"UTF-8\", \"ASCII\")\n\n[1] \"Norwegian\" NA          NA         \n\n\nThe first element will work on most common encodings because it only contains letters in the English alphabet. The second element, containing the letter “ø” will not work in ASCII encoding. Do notice that “xf8” is the hexadecimal for “ø”.1 Finally, the third element, which is Arabic, will only work in UTF-8 (of the examples shown here).\nIf we get into trouble with our text, we can convert the encoding with the iconv() function:\n\ntxt_ex &lt;- iconv(\"Øyvind Stiansen\", from = \"UTF-8\", to = \"ISO-8859-1\")\ntxt_ex\n\n[1] \"\\xd8yvind Stiansen\"\n\ntxt_ex &lt;- iconv(txt_ex, from = \"ISO-8859-1\", to = \"UTF-8\")\ntxt_ex\n\n[1] \"Øyvind Stiansen\"\n\n\nBe careful though! If the encoding is unable to handle the input, you can not recover the content:\n\ntxt_ex &lt;- iconv(\"Øyvind Stiansen\", from = \"UTF-8\", to = \"ASCII\")\ntxt_ex\n\n[1] NA\n\ntxt_ex &lt;- iconv(txt_ex, from = \"ASCII\", to = \"UTF-8\")\ntxt_ex\n\n[1] NA"
  },
  {
    "objectID": "basics_refresh/encoding.html#footnotes",
    "href": "basics_refresh/encoding.html#footnotes",
    "title": "Locales and encoding",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA list of these codes can be found here!. It can be useful for stuff like printing a snowman ☃ or a cat 🐈.↩︎"
  },
  {
    "objectID": "basics_refresh/Iteration.html",
    "href": "basics_refresh/Iteration.html",
    "title": "Iteration",
    "section": "",
    "text": "Motivation\nWe often need to apply the same functions to multiple values. Most of our time working in R, we will do this without giving a second thought as functions are vectorized: when we apply the function to a vector it will actually apply the function to each element of the vector.\nFor instance, let’s say we have this vector which consists of all integers starting at 1 and ending at 10000:\n\nnumbers &lt;- 1:10000\n\nLet’s say we want to take the natural logarithm of each of these numbers. We would simply run:\n\nlog_numbers &lt;- log(numbers)\n\nThe function log() is vectorized so it operates on each element of number and creates a new vector, which we decided to call log_numbers. This feature is an important strength of R as it makes our code fast to write and run and easy to both write and read.\nAlas, sometimes we still need to repeatedly run the same functions.\nConsider, for instance, the function read_dta() from the package haven. This function takes a single Stata (.dta) file and loads it to R as data.frame. But what if we have many .dta-files?\nIn the directory “..data/wealth_and_democracy” we have 60 .dta-files each containing data for one of the years in the period 1789 to 1848. Each file contains all countries included in the Historical Varieties of Democracy data (Knutsen et al. 2019) for that particular year, the country’s score on the polyarchy index in that year, and an estimate of its gross domestic product per capita. We can load the file for 1789 like this:\n\nlibrary(haven)\ncountries_1789 &lt;- read_dta(\"../data/wealth_and_democracy/wealth_and_democracy_in_1789.dta\")\nhead(countries_1789)\n\n# A tibble: 6 × 5\n  country_name country_id  year v2x_polyarchy e_gdppc\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n1 Mexico                3  1789         0.028    1.35\n2 Sweden                5  1789         0.188    1.69\n3 Japan                 9  1789         0.026    1.21\n4 Russia               11  1789         0.029    1.58\n5 Egypt                13  1789         0.033    1.12\n6 Poland               17  1789         0.199    1.00\n\n\nIt is also straightforward to get a vector of all the file names in the directory using the dir() function:\n\nfile_names &lt;- dir(\"../data/wealth_and_democracy/\")\nprint(file_names)\n\n [1] \"wealth_and_democracy_in_1789.dta\" \"wealth_and_democracy_in_1790.dta\"\n [3] \"wealth_and_democracy_in_1791.dta\" \"wealth_and_democracy_in_1792.dta\"\n [5] \"wealth_and_democracy_in_1793.dta\" \"wealth_and_democracy_in_1794.dta\"\n [7] \"wealth_and_democracy_in_1795.dta\" \"wealth_and_democracy_in_1796.dta\"\n [9] \"wealth_and_democracy_in_1797.dta\" \"wealth_and_democracy_in_1798.dta\"\n[11] \"wealth_and_democracy_in_1799.dta\" \"wealth_and_democracy_in_1800.dta\"\n[13] \"wealth_and_democracy_in_1801.dta\" \"wealth_and_democracy_in_1802.dta\"\n[15] \"wealth_and_democracy_in_1803.dta\" \"wealth_and_democracy_in_1804.dta\"\n[17] \"wealth_and_democracy_in_1805.dta\" \"wealth_and_democracy_in_1806.dta\"\n[19] \"wealth_and_democracy_in_1807.dta\" \"wealth_and_democracy_in_1808.dta\"\n[21] \"wealth_and_democracy_in_1809.dta\" \"wealth_and_democracy_in_1810.dta\"\n[23] \"wealth_and_democracy_in_1811.dta\" \"wealth_and_democracy_in_1812.dta\"\n[25] \"wealth_and_democracy_in_1813.dta\" \"wealth_and_democracy_in_1814.dta\"\n[27] \"wealth_and_democracy_in_1815.dta\" \"wealth_and_democracy_in_1816.dta\"\n[29] \"wealth_and_democracy_in_1817.dta\" \"wealth_and_democracy_in_1818.dta\"\n[31] \"wealth_and_democracy_in_1819.dta\" \"wealth_and_democracy_in_1820.dta\"\n[33] \"wealth_and_democracy_in_1821.dta\" \"wealth_and_democracy_in_1822.dta\"\n[35] \"wealth_and_democracy_in_1823.dta\" \"wealth_and_democracy_in_1824.dta\"\n[37] \"wealth_and_democracy_in_1825.dta\" \"wealth_and_democracy_in_1826.dta\"\n[39] \"wealth_and_democracy_in_1827.dta\" \"wealth_and_democracy_in_1828.dta\"\n[41] \"wealth_and_democracy_in_1829.dta\" \"wealth_and_democracy_in_1830.dta\"\n[43] \"wealth_and_democracy_in_1831.dta\" \"wealth_and_democracy_in_1832.dta\"\n[45] \"wealth_and_democracy_in_1833.dta\" \"wealth_and_democracy_in_1834.dta\"\n[47] \"wealth_and_democracy_in_1835.dta\" \"wealth_and_democracy_in_1836.dta\"\n[49] \"wealth_and_democracy_in_1837.dta\" \"wealth_and_democracy_in_1838.dta\"\n[51] \"wealth_and_democracy_in_1839.dta\" \"wealth_and_democracy_in_1840.dta\"\n[53] \"wealth_and_democracy_in_1841.dta\" \"wealth_and_democracy_in_1842.dta\"\n[55] \"wealth_and_democracy_in_1843.dta\" \"wealth_and_democracy_in_1844.dta\"\n[57] \"wealth_and_democracy_in_1845.dta\" \"wealth_and_democracy_in_1846.dta\"\n[59] \"wealth_and_democracy_in_1847.dta\" \"wealth_and_democracy_in_1848.dta\"\n\n\nHowever, loading all these files is not as straightforward. If we try to read multiple .dta-files at once using\n\n1paths &lt;- paste(\"../data/wealth_and_democracy/\", file_names, sep = \"\")\n2all_years &lt;- read_dta(paths)\n\n\n1\n\nHere we use paste() to create a vector with the paths to all the different .dta-files\n\n2\n\nWe attempt to load all the files in one go, but this fails!\n\n\n\n\nError in `read_dta()`:\n! This kind of input is not handled.\n\n\nHow then can we load all the different files? We could start with the code we used to load the file for 1789, copy-paste it 60 times, and edit the different file names. This process is, however, both cumbersome and error prone.\nA much better would be to use one of the different iteration tools in R.\n\n\nfor-loops\nA for-loop does the operation(s) you tell it to do for all the elements in a vector. For instance, we can write a for-loop to print all the integers between -5 and 5:\n\n1for(i in -5:5){\n2  print(i)\n3}\n\n\n1\n\nWe write that we want to something for all elements i in the vector -5:5. After the curly bracket ({), we can start defining what to do.\n\n2\n\nHere we define what to do. We simply want to print(i).\n\n3\n\nThe curly bracket closes the loop, meaning that we are done defining what should happen to each element i.\n\n\n\n\n[1] -5\n[1] -4\n[1] -3\n[1] -2\n[1] -1\n[1] 0\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\n\n\n\nYou don’t have loop over i\n\n\n\nThere is nothing magical about i, we could have just as well written:\n\nfor(number in -5:5){ \n  print(number) \n} \n\n[1] -5\n[1] -4\n[1] -3\n[1] -2\n[1] -1\n[1] 0\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nor whatever we find intuitive. Using i is just a convention.\n\n\nNow that we know the basic of for-loops, let’s see if we can write a loop that loads all our 60 .dta-files. To that we need a vector with the paths to all the files. We already have the paths vector, which we can use. A loop that only loads the files would then look like this:\n\n1for(i in paths){\n2  read_dta(i)\n3}\n\n\n1\n\nFor each element i in paths,\n\n2\n\nread the .dta-file for the element i.\n\n3\n\nThat’s it. Go back to the top and do the next element i if there are more of them.\n\n\n\n\nOf course, just loading all the files is not terribly useful unless we also assign them to objects. For that we we will need a vector of object names:\n\nobject_names &lt;- paste(\"countries\", 1789:1848, sep = \"_\")\nprint(object_names)\n\n [1] \"countries_1789\" \"countries_1790\" \"countries_1791\" \"countries_1792\"\n [5] \"countries_1793\" \"countries_1794\" \"countries_1795\" \"countries_1796\"\n [9] \"countries_1797\" \"countries_1798\" \"countries_1799\" \"countries_1800\"\n[13] \"countries_1801\" \"countries_1802\" \"countries_1803\" \"countries_1804\"\n[17] \"countries_1805\" \"countries_1806\" \"countries_1807\" \"countries_1808\"\n[21] \"countries_1809\" \"countries_1810\" \"countries_1811\" \"countries_1812\"\n[25] \"countries_1813\" \"countries_1814\" \"countries_1815\" \"countries_1816\"\n[29] \"countries_1817\" \"countries_1818\" \"countries_1819\" \"countries_1820\"\n[33] \"countries_1821\" \"countries_1822\" \"countries_1823\" \"countries_1824\"\n[37] \"countries_1825\" \"countries_1826\" \"countries_1827\" \"countries_1828\"\n[41] \"countries_1829\" \"countries_1830\" \"countries_1831\" \"countries_1832\"\n[45] \"countries_1833\" \"countries_1834\" \"countries_1835\" \"countries_1836\"\n[49] \"countries_1837\" \"countries_1838\" \"countries_1839\" \"countries_1840\"\n[53] \"countries_1841\" \"countries_1842\" \"countries_1843\" \"countries_1844\"\n[57] \"countries_1845\" \"countries_1846\" \"countries_1847\" \"countries_1848\"\n\n\nSince object_names is a character vector, we cannot use &lt;- to assign each file to the right object in the loop. Instead we will use the function assign() which takes an object name, x, in the form of a character string and a value. It will then assign whatever is in value to whatever name is in x in the global environment.\nSince we are now working with two vectors (paths and object_names) in the loop, we will loop over integer from 1 to their length() (which should be 60 for both vectors) and use indexing to grab the right element in each iteration of the loop:\n\nfor(i in 1:length(paths)){\n  assign(x = object_names[i], \n         value = read_dta(paths[i]))\n}\n\nWhen running the loop above, 60 different datasets are loaded and assigned to objects in our global environment. For instance, we will now have an object called countries_1802:\n\nhead(countries_1802)\n\n# A tibble: 6 × 5\n  country_name country_id  year v2x_polyarchy e_gdppc\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n1 Mexico                3  1802         0.028    1.32\n2 Sweden                5  1802         0.189    1.49\n3 Switzerland           6  1802         0.188    2.01\n4 Japan                 9  1802         0.028    1.26\n5 Russia               11  1802         0.02     1.58\n6 Egypt                13  1802         0.032    1.08\n\n\nWe probably didn’t load all these datasets just for the fun of it.\nPerhaps our reason for loading all these different .dta files was that we want to estimate linear regression model for the bivarate relationship between wealth (as captured by log(gross domestic product per capita)) and democracy (as captured by the polyarchy index)). We will consider wealth as the independent variable and democracy as the independent variable (although this relationship could arguably also work in the opposite direction). For each regression we want to save the coefficient for log(gross domestic product per capita) and its standard error.\nAgain, we don’t want to write out the code for a regression model 60 times only changing which dataset it is estimated on. It would be great if we also could estimate model and retrieve the coefficient of interest inside the loop. Can we?\nA good way to start is to write out the code for what we would like to do for a single dataset. Let’s do the year 1814:\n\n1library(broom)\n2dataset &lt;- read_dta(\"../data/wealth_and_democracy/wealth_and_democracy_in_1814.dta\")\n3regression_model &lt;- lm(v2x_polyarchy ~ log(e_gdppc), data = dataset)\n4tidy(regression_model)\n5tidy(regression_model)[2, c(\"estimate\", \"std.error\")]\n\n\n1\n\nThe broom package is great for converting the objects of regression models into data.frames that are easy to work with in R\n\n2\n\nWe load the data\n\n3\n\nWe estimate the model\n\n4\n\nJust to illustrate: tidy() from broom produces a nice little data.frame (or technically, a tibble, but that’s just a type of data.frame) based on our model. This data.frame is easy to work with.\n\n5\n\nWe can subset the data.frame produced by tidy() from broom to grab the coefficient and standard error we are interested in.\n\n\n\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    0.0433    0.0154      2.80 0.00875\n2 log(e_gdppc)   0.109     0.0309      3.52 0.00141\n# A tibble: 1 × 2\n  estimate std.error\n     &lt;dbl&gt;     &lt;dbl&gt;\n1    0.109    0.0309\n\n\nA next useful step is to see if we can rewrite this code just from the paths vector and save the result in a convenient way.\n\n1estimates &lt;- data.frame(coefficient = rep(NA, times = length(paths)),\n                        standard_error = rep(NA, times = length(paths)))\n2dataset &lt;- read_dta(paths[26])\n3regression_model &lt;- lm(v2x_polyarchy ~ log(e_gdppc), data = dataset)\n\n4estimates[26,] &lt;- tidy(regression_model)[2, c(\"estimate\", \"std.error\")]\n\n\n1\n\nWe create a data.frame to store the results in. We want one row per element in paths so we use length(path) to determine the number of rows. We want to variables coefficient and standard_error. We don’t know what values are yet (that’s what we are trying to figure out!), so we just fill the data.frame with NAs for now.\n\n2\n\nWe load paths[26], i.e. the 26th element of paths and assign it to dataset\n\n3\n\nWe estimate the regression model, using dataset as the data.\n\n4\n\nWe store the coefficient and standard error for log(e_gdppc) to the 26th row of estimates.\n\n\n\n\n\n1for(i in 1:nrow(estimates)){\n2  dataset &lt;- read_dta(paths[i])\n3  regression_model &lt;- lm(v2x_polyarchy ~ log(e_gdppc), data = dataset)\n4  estimates[i,] &lt;- tidy(regression_model)[2, c(\"estimate\", \"std.error\")]\n}\n\n\n1\n\nWe loop over all the numbers from 1 to the number of rows in estimates (we could also have used the length of paths).\n\n2\n\nWe load the element i of paths and save it to dataset. We did this for every iteration, so we will continuously overwrite dataset.\n\n3\n\nWe estimate the model using whatever data is currently stored as dataset and assign it regression_model. regression_model will also be overwritten for each iteration.\n\n4\n\nFinally we save the coefficient and standard error to the ith row of estimates. Because i is changing for each iteration. These results will not be overwritten. Instead they will be saved in estimates\n\n\n\n\nThe first rows of estimates look like this:\n\nhead(estimates)\n\n  coefficient standard_error\n1  0.07566119     0.04031438\n2  0.07796730     0.04090374\n3  0.07859954     0.04003433\n4  0.07971598     0.04083772\n5  0.08638019     0.03958585\n6  0.08455238     0.04014866\n\n\nWe can now use this data.frame to, let’s say, make a Barbie-themed (Jané 2023) coefficient plot displaying the coefficient and 95% confidence interval for each year from 1789 to 1848. We will cover how to make such plots later, so for now we have hidden the code (if you wish, you may, however, click on the “Code” button to display the code for the plot).\n\n\nCode\nlibrary(ggplot2)\nlibrary(ThemePark)\nestimates$year &lt;- 1789:1848\nestimates$lower &lt;- estimates$coefficient - 1.96 * estimates$standard_error\nestimates$upper &lt;- estimates$coefficient + 1.96 * estimates$standard_error\nggplot(estimates, \n       aes(x = coefficient, \n           xmin = lower, \n           xmax = upper, \n           y = year))+\n  geom_vline(xintercept = 0, linetype = \"dashed\")+\n  geom_errorbar()+\n  geom_point()+\n  xlab(\"Coefficient for log(gdp per capita)\")+\n  ylab(\"Year\")+\n  xlim(-0.10, 0.25)+\n  theme_barbie(barbie_font = TRUE) \n\n\n\n\n\n\n\nThe apply() family of functions\nfor-loops run relatively slowly in R. This doesn’t matter if you are only looping over a few element and each step doesn’t take a lot of time for your computer to complete, but for-loops don’t scale all that well.\nWhat is R is good at is applying functions to vectors. So, it will be useful to consider alternatives to for-loops that play on this strength:\nOne such alternative is the apply family of function, which is available in base R. The function lapply() will take a list or vector, apply some function to all elements in that list or vector, and return a list with the results.\nSo let’s say we again have the vector -5:5 consisting all integers from -5 to 5. We want to divide all of them by 2. Using lapply() we may:\n\nlapply(X = -5:5, FUN = function(x){x/2})\n\n[[1]]\n[1] -2.5\n\n[[2]]\n[1] -2\n\n[[3]]\n[1] -1.5\n\n[[4]]\n[1] -1\n\n[[5]]\n[1] -0.5\n\n[[6]]\n[1] 0\n\n[[7]]\n[1] 0.5\n\n[[8]]\n[1] 1\n\n[[9]]\n[1] 1.5\n\n[[10]]\n[1] 2\n\n[[11]]\n[1] 2.5\n\n\nThe result will be a list of the same length as -5:5 and each element will contain the corresponding element in the original vector divided by 2. On its own, this is not terribly impressive, we might as well have run -5:5/2. But just like for a for-loop the advantage is found in how we can extend the logic to other types of inputs.\nWe could for write a function that given a path, will load a .dta-file found in that path, estimate a regression model with v2x_polyarchy as the dependent variable and log(e_gdppc) and extract the coefficient and standard error for log(e_gdppc). We can lapply this function to all the elements of our vector paths:\n\n1coefficients_and_standard_errors &lt;- lapply(X = paths,\n2                                           FUN = function(x){\n3                                             dataset &lt;- read_dta(x)\n                                             regression_model &lt;- lm(v2x_polyarchy ~\n                                                                      log(e_gdppc), data = dataset)\n                                             tidy(regression_model)[2, c(\"estimate\", \"std.error\")]\n                                           })\n\n\n1\n\nWe need to specify the argument X (notice the upper case X), which should be a list or atomic vector to apply the function to.\n\n2\n\nWe need to specify the argument FUN we specify the function that should be applied to all elements in X. We can define our own function, which is what we use here (using the skills we already picked up here)\n\n3\n\nOur function, takes the argument x and for each x loads a dataset using read_dta(), estimates or regression model on that dataset using lm(), extracts the coefficient of interest using tidy() from broom and some basic subsetting.\n\n\n\n\nThe object coefficients_and_standard_errors is a list and each element of the list is a data.frame with the one coefficient and one standard error. For instance, we will find our coefficient and standard error for the year 1814 in the 26th element like before:\n\ncoefficients_and_standard_errors[26]\n\n[[1]]\n# A tibble: 1 × 2\n  estimate std.error\n     &lt;dbl&gt;     &lt;dbl&gt;\n1    0.109    0.0309\n\n\nUsing the functions do.call() and rbind(), we may combine all the elements of list together in one data.frame. do.call() takes two functions: a function and a list of elements to apply the function to. Here we will supply the function rbind that binds different data.frames together by the rows and our list (coefficients_and_standard_errors) of data.frames we would like to combine:\n\n1estimates2 &lt;- do.call(rbind,\n2                       coefficients_and_standard_errors)\n\nhead(estimates2)\n\n\n1\n\nFirst we specify what function should applied. Here we use rbind() which binds data.frames together. It is the base R equivalent of the dplyr::bind_rows which we will spend more time on later.\n\n2\n\nWe then specify the list of arguments to the function (here rbind). Since each element in the list is a data.frame, what we are doing here is to tell R to bind all these data.frames together.\n\n\n\n\n# A tibble: 6 × 2\n  estimate std.error\n     &lt;dbl&gt;     &lt;dbl&gt;\n1   0.0757    0.0403\n2   0.0780    0.0409\n3   0.0786    0.0400\n4   0.0797    0.0408\n5   0.0864    0.0396\n6   0.0846    0.0401\n\n\nThus, we managed to do exactly what we did in the for-loop above using lapply(). This is great news as using the apply functions can be much more efficient than writing for-loops in R.\nlapply() is not the only apply() function. apply will take a data.frame or matrix and apply some function across the rows or across the columns. This can be useful for instance when computing summary statistics. Let’s calculate the mean() for both v2x_polyarchy and e_gdppc in countries_1814\n\n1apply(countries_1814[,c(\"v2x_polyarchy\", \"e_gdppc\")],\n2      2,\n3      mean)\n\n\n1\n\ngive apply() a data.frame or matrix. Here we subset countries_1814 to only contain the variables v2x_polyarchy and e_gdppc.\n\n2\n\nWe specify 1 if we want the function to be done by the rows and 2 if it should be done by columns. Here we specify 2 because we want the mean() for each column\n\n3\n\nFinally we specify the function we want to apply(), here we use mean().\n\n\n\n\nv2x_polyarchy       e_gdppc \n     0.074000      1.447563 \n\n\nsapply() is like lapply() but will simplify the result to be an atomic vector or a matrix instead of a list. Thus, if we had written the code dividing all the integers in -5:5 by 2 using sapply() instead of lapply(), we would have produced a nice numeric vector instead of a list:\n\nsapply(X = -5:5, FUN = function(x){x/2})\n\n [1] -2.5 -2.0 -1.5 -1.0 -0.5  0.0  0.5  1.0  1.5  2.0  2.5\n\n\nIf our goal is to produce a variable for a data.frame, making a vector with sapply() is often better than making a list with lapply().\n\n\nThe purrr package\nThe purrr package provides an even more comprehensive set of tools for using functions to do various iterative tasks in R (and thus avoiding for-loops).\nThe workhorse function of purrr is the map()-function which works very similarly to the apply-functions in base R. map() takes the argument .x which should be provided a list or atomic vector and .f which should be function. The function will be applied to each element of .x and map() will retun a list with the results.\nLet’s try to take all the integers in -5:5 and divide by 2:\n\n1library(purrr)\n2map(.x = -5:5,\n3    .f = function(x){\n      x/2}\n)\n\n\n1\n\nRemember to load the purrr package!\n\n2\n\nSupply your vector to .x!\n\n3\n\nSupply your function to .f. You can define your own function as we do here or provide the name of a function.\n\n\n\n\n[[1]]\n[1] -2.5\n\n[[2]]\n[1] -2\n\n[[3]]\n[1] -1.5\n\n[[4]]\n[1] -1\n\n[[5]]\n[1] -0.5\n\n[[6]]\n[1] 0\n\n[[7]]\n[1] 0.5\n\n[[8]]\n[1] 1\n\n[[9]]\n[1] 1.5\n\n[[10]]\n[1] 2\n\n[[11]]\n[1] 2.5\n\n\nIt is sometimes inconvenient that map() returns a list. Often we want an atomic vector (like an numeric or character vector) instead. purrr also has a series of map_*-functions that return specific types of vectors. For instance, map_dbl will return a numeric vector (the numeric data type is also called “double”):\n\nmap_dbl(.x = -5:5, \n    .f = function(x){\n      x/2}\n)\n\n [1] -2.5 -2.0 -1.5 -1.0 -0.5  0.0  0.5  1.0  1.5  2.0  2.5\n\n\nSimilarly, map_chr will return a character vector (but purrr doesn’t like it when we implicitly coerce a numeric vector to a character vector and will spit out a warning to complain) :\n\nmap_chr(.x = -5:5, \n    .f = function(x){\n      x/2}\n)\n\nWarning: Automatic coercion from double to character was deprecated in purrr 1.0.0.\nℹ Please use an explicit call to `as.character()` within `map_chr()` instead.\n\n\n [1] \"-2.500000\" \"-2.000000\" \"-1.500000\" \"-1.000000\" \"-0.500000\" \"0.000000\" \n [7] \"0.500000\"  \"1.000000\"  \"1.500000\"  \"2.000000\"  \"2.500000\" \n\n\nThe function map_vec will work with any kind of vector and return a vector of the same data type as it was supplied.\n\nmap_vec(.x = -5:5, \n    .f = function(x){\n      x/2}\n)\n\n [1] -2.5 -2.0 -1.5 -1.0 -0.5  0.0  0.5  1.0  1.5  2.0  2.5\n\n\nSo can we load our datasets and estimate our regression models using purrr Of course we can! Using only map, we can create a list with all the coefficients and standard errors (just like we did with lapply())\n\n1coefficients_and_standard_errors_from_map &lt;- map(.x = paths,\n2                                        .f = function(x){\n                                             dataset &lt;- read_dta(x)\n                                             regression_model &lt;- lm(v2x_polyarchy ~\n                                                                      log(e_gdppc), data = dataset)\n                                             tidy(regression_model)[2, c(\"estimate\", \"std.error\")]\n                                           })\n\n\n1\n\nWe supply our paths vector to .x\n\n2\n\nWe supply our function for loading the data, estimating the model, and retrieving the output we care about to .f coefficients_and_standard_errors_from_map is a list with each element containing a data.frame the coefficient and standard error for one of the 60 models we just ran.\n\n\n\n\nThe purrr package also contains a set of functions for working with the lists that we will tend to produce when using map(). For instance, reduce() will combine all the elements of a list you provide (to its .x argument) using some function (that you supply to its .f argument). So, we can use reduce() together with rbind() to produce a single data.frame:\n\n1coefficients_and_standard_errors_from_map &lt;- reduce(.x = coefficients_and_standard_errors_from_map,\n2                                                    .f = rbind)\n\nhead(coefficients_and_standard_errors_from_map)\n\n\n1\n\nWe use reduce() to combine all the elements of a list into a single value. We supply the list to .x.\n\n2\n\nAnd we supply the function we will use to combine the elements to .f.\n\n\n\n\n# A tibble: 6 × 2\n  estimate std.error\n     &lt;dbl&gt;     &lt;dbl&gt;\n1   0.0757    0.0403\n2   0.0780    0.0409\n3   0.0786    0.0400\n4   0.0797    0.0408\n5   0.0864    0.0396\n6   0.0846    0.0401\n\n\n\n\n\n\n\nReferences\n\nJané, Matthew B. 2023. “Theme_park: Popular Culture ggplot Themes.” https://github.com/MatthewBJane/theme_park.\n\n\nKnutsen, Carl Henrik, Jan Teorell, Tore Wig, Agnes Cornell, John Gerring, Haakon Gjerløw, Svend-Erik Skaaning, et al. 2019. “Introducing the Historical Varieties of Democracy Dataset: Political Institutions in the Long 19th Century.” Journal of Peace Research 56 (3): 440–51."
  }
]